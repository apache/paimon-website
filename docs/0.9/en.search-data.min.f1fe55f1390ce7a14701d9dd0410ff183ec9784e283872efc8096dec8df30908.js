'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href','section'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/docs/0.9/concepts/','title':"Concepts",'section':"Apache Paimon",'content':""});index.add({'id':1,'href':'/docs/0.9/program-api/java-api/','title':"Java API",'section':"Program API",'content':"Java API #  We do not recommend using the Paimon API naked, unless you are a professional downstream ecosystem developer, and even if you do, there will be significant difficulties.\nIf you are only using Paimon, we strongly recommend using computing engines such as Flink SQL or Spark SQL.\nThe following documents are not detailed and are for reference only.\n Dependency #  Maven dependency:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.paimon\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;paimon-bundle\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.9.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Or download the jar file: Paimon Bundle. Paimon relies on Hadoop environment, you should add hadoop classpath or bundled jar.\nCreate Catalog #  Before coming into contact with the Table, you need to create a Catalog.\nimport org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.CatalogContext; import org.apache.paimon.catalog.CatalogFactory; import org.apache.paimon.fs.Path; import org.apache.paimon.options.Options; public class CreateCatalog { public static Catalog createFilesystemCatalog() { CatalogContext context = CatalogContext.create(new Path(\u0026#34;...\u0026#34;)); return CatalogFactory.createCatalog(context); } public static Catalog createHiveCatalog() { // Paimon Hive catalog relies on Hive jars  // You should add hive classpath or hive bundled jar.  Options options = new Options(); options.set(\u0026#34;warehouse\u0026#34;, \u0026#34;...\u0026#34;); options.set(\u0026#34;metastore\u0026#34;, \u0026#34;hive\u0026#34;); options.set(\u0026#34;uri\u0026#34;, \u0026#34;...\u0026#34;); options.set(\u0026#34;hive-conf-dir\u0026#34;, \u0026#34;...\u0026#34;); options.set(\u0026#34;hadoop-conf-dir\u0026#34;, \u0026#34;...\u0026#34;); CatalogContext context = CatalogContext.create(options); return CatalogFactory.createCatalog(context); } } Create Table #  You can use the catalog to create tables. The created tables are persistence in the file system. Next time you can directly obtain these tables.\nimport org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; import org.apache.paimon.schema.Schema; import org.apache.paimon.types.DataTypes; public class CreateTable { public static void main(String[] args) { Schema.Builder schemaBuilder = Schema.newBuilder(); schemaBuilder.primaryKey(\u0026#34;f0\u0026#34;, \u0026#34;f1\u0026#34;); schemaBuilder.partitionKeys(\u0026#34;f1\u0026#34;); schemaBuilder.column(\u0026#34;f0\u0026#34;, DataTypes.STRING()); schemaBuilder.column(\u0026#34;f1\u0026#34;, DataTypes.INT()); Schema schema = schemaBuilder.build(); Identifier identifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;my_table\u0026#34;); try { Catalog catalog = CreateCatalog.createFilesystemCatalog(); catalog.createTable(identifier, schema, false); } catch (Catalog.TableAlreadyExistException e) { // do something  } catch (Catalog.DatabaseNotExistException e) { // do something  } } } Get Table #  The Table interface provides access to the table metadata and tools to read and write table.\nimport org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; import org.apache.paimon.table.Table; public class GetTable { public static Table getTable() { Identifier identifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;my_table\u0026#34;); try { Catalog catalog = CreateCatalog.createFilesystemCatalog(); return catalog.getTable(identifier); } catch (Catalog.TableNotExistException e) { // do something  throw new RuntimeException(\u0026#34;table not exist\u0026#34;); } } } Batch Read #  For relatively small amounts of data, or for data that has undergone projection and filtering, you can directly use a standalone program to read the table data.\nBut if the data volume of the table is relatively large, you can distribute splits to different tasks for reading.\nThe reading is divided into two stages:\n Scan Plan: Generate plan splits in a global node (\u0026lsquo;Coordinator\u0026rsquo;, or named \u0026lsquo;Driver\u0026rsquo;). Read Split: Read split in distributed tasks.  import org.apache.paimon.data.InternalRow; import org.apache.paimon.predicate.Predicate; import org.apache.paimon.predicate.PredicateBuilder; import org.apache.paimon.reader.RecordReader; import org.apache.paimon.table.Table; import org.apache.paimon.table.source.ReadBuilder; import org.apache.paimon.table.source.Split; import org.apache.paimon.table.source.TableRead; import org.apache.paimon.types.DataTypes; import org.apache.paimon.types.RowType; import com.google.common.collect.Lists; import java.util.List; public class ReadTable { public static void main(String[] args) throws Exception { // 1. Create a ReadBuilder and push filter (`withFilter`)  // and projection (`withProjection`) if necessary  Table table = GetTable.getTable(); PredicateBuilder builder = new PredicateBuilder(RowType.of(DataTypes.STRING(), DataTypes.INT())); Predicate notNull = builder.isNotNull(0); Predicate greaterOrEqual = builder.greaterOrEqual(1, 12); int[] projection = new int[] {0, 1}; ReadBuilder readBuilder = table.newReadBuilder() .withProjection(projection) .withFilter(Lists.newArrayList(notNull, greaterOrEqual)); // 2. Plan splits in \u0026#39;Coordinator\u0026#39; (or named \u0026#39;Driver\u0026#39;)  List\u0026lt;Split\u0026gt; splits = readBuilder.newScan().plan().splits(); // 3. Distribute these splits to different tasks  // 4. Read a split in task  TableRead read = readBuilder.newRead(); RecordReader\u0026lt;InternalRow\u0026gt; reader = read.createReader(splits); reader.forEachRemaining(System.out::println); } } Batch Write #  The writing is divided into two stages:\n Write records: Write records in distributed tasks, generate commit messages. Commit/Abort: Collect all CommitMessages, commit them in a global node (\u0026lsquo;Coordinator\u0026rsquo;, or named \u0026lsquo;Driver\u0026rsquo;, or named \u0026lsquo;Committer\u0026rsquo;). When the commit fails for certain reason, abort unsuccessful commit via CommitMessages.  import org.apache.paimon.data.BinaryString; import org.apache.paimon.data.GenericRow; import org.apache.paimon.table.Table; import org.apache.paimon.table.sink.BatchTableCommit; import org.apache.paimon.table.sink.BatchTableWrite; import org.apache.paimon.table.sink.BatchWriteBuilder; import org.apache.paimon.table.sink.CommitMessage; import java.util.List; public class BatchWrite { public static void main(String[] args) throws Exception { // 1. Create a WriteBuilder (Serializable)  Table table = GetTable.getTable(); BatchWriteBuilder writeBuilder = table.newBatchWriteBuilder().withOverwrite(); // 2. Write records in distributed tasks  BatchTableWrite write = writeBuilder.newWrite(); GenericRow record1 = GenericRow.of(BinaryString.fromString(\u0026#34;Alice\u0026#34;), 12); GenericRow record2 = GenericRow.of(BinaryString.fromString(\u0026#34;Bob\u0026#34;), 5); GenericRow record3 = GenericRow.of(BinaryString.fromString(\u0026#34;Emily\u0026#34;), 18); // If this is a distributed write, you can use writeBuilder.newWriteSelector.  // WriteSelector determines to which logical downstream writers a record should be written to.  // If it returns empty, no data distribution is required.  write.write(record1); write.write(record2); write.write(record3); List\u0026lt;CommitMessage\u0026gt; messages = write.prepareCommit(); // 3. Collect all CommitMessages to a global node and commit  BatchTableCommit commit = writeBuilder.newCommit(); commit.commit(messages); // Abort unsuccessful commit to delete data files  // commit.abort(messages);  } } Stream Read #  The difference of Stream Read is that StreamTableScan can continuously scan and generate splits.\nStreamTableScan provides the ability to checkpoint and restore, which can let you save the correct state during stream reading.\nimport org.apache.paimon.data.InternalRow; import org.apache.paimon.predicate.Predicate; import org.apache.paimon.predicate.PredicateBuilder; import org.apache.paimon.reader.RecordReader; import org.apache.paimon.table.Table; import org.apache.paimon.table.source.ReadBuilder; import org.apache.paimon.table.source.Split; import org.apache.paimon.table.source.StreamTableScan; import org.apache.paimon.table.source.TableRead; import org.apache.paimon.types.DataTypes; import org.apache.paimon.types.RowType; import com.google.common.collect.Lists; import java.util.List; public class StreamReadTable { public static void main(String[] args) throws Exception { // 1. Create a ReadBuilder and push filter (`withFilter`)  // and projection (`withProjection`) if necessary  Table table = GetTable.getTable(); PredicateBuilder builder = new PredicateBuilder(RowType.of(DataTypes.STRING(), DataTypes.INT())); Predicate notNull = builder.isNotNull(0); Predicate greaterOrEqual = builder.greaterOrEqual(1, 12); int[] projection = new int[] {0, 1}; ReadBuilder readBuilder = table.newReadBuilder() .withProjection(projection) .withFilter(Lists.newArrayList(notNull, greaterOrEqual)); // 2. Plan splits in \u0026#39;Coordinator\u0026#39; (or named \u0026#39;Driver\u0026#39;)  StreamTableScan scan = readBuilder.newStreamScan(); while (true) { List\u0026lt;Split\u0026gt; splits = scan.plan().splits(); // Distribute these splits to different tasks  Long state = scan.checkpoint(); // can be restored in scan.restore(state) after fail over  // 3. Read a split in task  TableRead read = readBuilder.newRead(); RecordReader\u0026lt;InternalRow\u0026gt; reader = read.createReader(splits); reader.forEachRemaining(System.out::println); Thread.sleep(1000); } } } Stream Write #  The difference of Stream Write is that StreamTableCommit can continuously commit.\nKey points to achieve exactly-once consistency:\n CommitUser represents a user. A user can commit multiple times. In distributed processing, you are expected to use the same commitUser. Different applications need to use different commitUsers. The commitIdentifier of StreamTableWrite and StreamTableCommit needs to be consistent, and the id needs to be incremented for the next committing. When a failure occurs, if you still have uncommitted CommitMessages, please use StreamTableCommit#filterAndCommit to exclude the committed messages by commitIdentifier.  import org.apache.paimon.data.BinaryString; import org.apache.paimon.data.GenericRow; import org.apache.paimon.table.Table; import org.apache.paimon.table.sink.CommitMessage; import org.apache.paimon.table.sink.StreamTableCommit; import org.apache.paimon.table.sink.StreamTableWrite; import org.apache.paimon.table.sink.StreamWriteBuilder; import java.util.List; public class StreamWriteTable { public static void main(String[] args) throws Exception { // 1. Create a WriteBuilder (Serializable)  Table table = GetTable.getTable(); StreamWriteBuilder writeBuilder = table.newStreamWriteBuilder(); // 2. Write records in distributed tasks  StreamTableWrite write = writeBuilder.newWrite(); // commitIdentifier like Flink checkpointId  long commitIdentifier = 0; while (true) { GenericRow record1 = GenericRow.of(BinaryString.fromString(\u0026#34;Alice\u0026#34;), 12); GenericRow record2 = GenericRow.of(BinaryString.fromString(\u0026#34;Bob\u0026#34;), 5); GenericRow record3 = GenericRow.of(BinaryString.fromString(\u0026#34;Emily\u0026#34;), 18); // If this is a distributed write, you can use writeBuilder.newWriteSelector.  // WriteSelector determines to which logical downstream writers a record should be written to.  // If it returns empty, no data distribution is required.  write.write(record1); write.write(record2); write.write(record3); List\u0026lt;CommitMessage\u0026gt; messages = write.prepareCommit(false, commitIdentifier); commitIdentifier++; // 3. Collect all CommitMessages to a global node and commit  StreamTableCommit commit = writeBuilder.newCommit(); commit.commit(commitIdentifier, messages); // 4. When failure occurs and you\u0026#39;re not sure if the commit process is successful,  // you can use `filterAndCommit` to retry the commit process.  // Succeeded commits will be automatically skipped.  /* Map\u0026lt;Long, List\u0026lt;CommitMessage\u0026gt;\u0026gt; commitIdentifiersAndMessages = new HashMap\u0026lt;\u0026gt;(); commitIdentifiersAndMessages.put(commitIdentifier, messages); commit.filterAndCommit(commitIdentifiersAndMessages); */ Thread.sleep(1000); } } } Data Types #     Java Paimon     boolean boolean   byte byte   short short   int int   long long   float float   double double   string org.apache.paimon.data.BinaryString   decimal org.apache.paimon.data.Decimal   timestamp org.apache.paimon.data.Timestamp   byte[] byte[]   array org.apache.paimon.data.InternalArray   map org.apache.paimon.data.InternalMap   InternalRow org.apache.paimon.data.InternalRow    Predicate Types #     SQL Predicate Paimon Predicate     and org.apache.paimon.predicate.PredicateBuilder.And   or org.apache.paimon.predicate.PredicateBuilder.Or   is null org.apache.paimon.predicate.PredicateBuilder.IsNull   is not null org.apache.paimon.predicate.PredicateBuilder.IsNotNull   in org.apache.paimon.predicate.PredicateBuilder.In   not in org.apache.paimon.predicate.PredicateBuilder.NotIn   = org.apache.paimon.predicate.PredicateBuilder.Equal   \u0026lt;\u0026gt; org.apache.paimon.predicate.PredicateBuilder.NotEqual   \u0026lt; org.apache.paimon.predicate.PredicateBuilder.LessThan   \u0026lt;= org.apache.paimon.predicate.PredicateBuilder.LessOrEqual   \u0026gt; org.apache.paimon.predicate.PredicateBuilder.GreaterThan   \u0026gt;= org.apache.paimon.predicate.PredicateBuilder.GreaterOrEqual    Create Database #  You can use the catalog to create databases. The created databases are persistence in the file system.\nimport org.apache.paimon.catalog.Catalog; public class CreateDatabase { public static void main(String[] args) { try { Catalog catalog = CreateCatalog.createFilesystemCatalog(); catalog.createDatabase(\u0026#34;my_db\u0026#34;, false); } catch (Catalog.DatabaseAlreadyExistException e) { // do something  } } } Determine Whether Database Exists #  You can use the catalog to determine whether the database exists\nimport org.apache.paimon.catalog.Catalog; public class DatabaseExists { public static void main(String[] args) { Catalog catalog = CreateCatalog.createFilesystemCatalog(); boolean exists = catalog.databaseExists(\u0026#34;my_db\u0026#34;); } } List Databases #  You can use the catalog to list databases.\nimport org.apache.paimon.catalog.Catalog; import java.util.List; public class ListDatabases { public static void main(String[] args) { Catalog catalog = CreateCatalog.createFilesystemCatalog(); List\u0026lt;String\u0026gt; databases = catalog.listDatabases(); } } Drop Database #  You can use the catalog to drop databases.\nimport org.apache.paimon.catalog.Catalog; public class DropDatabase { public static void main(String[] args) { try { Catalog catalog = CreateCatalog.createFilesystemCatalog(); catalog.dropDatabase(\u0026#34;my_db\u0026#34;, false, true); } catch (Catalog.DatabaseNotEmptyException e) { // do something  } catch (Catalog.DatabaseNotExistException e) { // do something  } } } Determine Whether Table Exists #  You can use the catalog to determine whether the table exists\nimport org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; public class TableExists { public static void main(String[] args) { Identifier identifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;my_table\u0026#34;); Catalog catalog = CreateCatalog.createFilesystemCatalog(); boolean exists = catalog.tableExists(identifier); } } List Tables #  You can use the catalog to list tables.\nimport org.apache.paimon.catalog.Catalog; import java.util.List; public class ListTables { public static void main(String[] args) { try { Catalog catalog = CreateCatalog.createFilesystemCatalog(); List\u0026lt;String\u0026gt; tables = catalog.listTables(\u0026#34;my_db\u0026#34;); } catch (Catalog.DatabaseNotExistException e) { // do something  } } } Drop Table #  You can use the catalog to drop table.\nimport org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; public class DropTable { public static void main(String[] args) { Identifier identifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;my_table\u0026#34;); try { Catalog catalog = CreateCatalog.createFilesystemCatalog(); catalog.dropTable(identifier, false); } catch (Catalog.TableNotExistException e) { // do something  } } } Rename Table #  You can use the catalog to rename a table.\nimport org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; public class RenameTable { public static void main(String[] args) { Identifier fromTableIdentifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;my_table\u0026#34;); Identifier toTableIdentifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;test_table\u0026#34;); try { Catalog catalog = CreateCatalog.createFilesystemCatalog(); catalog.renameTable(fromTableIdentifier, toTableIdentifier, false); } catch (Catalog.TableAlreadyExistException e) { // do something  } catch (Catalog.TableNotExistException e) { // do something  } } } Alter Table #  You can use the catalog to alter a table, but you need to pay attention to the following points.\n Column %s cannot specify NOT NULL in the %s table. Cannot update partition column type in the table. Cannot change nullability of primary key. If the type of the column is nested row type, update the column type is not supported. Update column to nested row type is not supported.  import org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; import org.apache.paimon.schema.Schema; import org.apache.paimon.schema.SchemaChange; import org.apache.paimon.types.DataField; import org.apache.paimon.types.DataTypes; import com.google.common.collect.Lists; import java.util.Arrays; import java.util.HashMap; import java.util.Map; public class AlterTable { public static void main(String[] args) { Identifier identifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;my_table\u0026#34;); Map\u0026lt;String, String\u0026gt; options = new HashMap\u0026lt;\u0026gt;(); options.put(\u0026#34;bucket\u0026#34;, \u0026#34;4\u0026#34;); options.put(\u0026#34;compaction.max.file-num\u0026#34;, \u0026#34;40\u0026#34;); Catalog catalog = CreateCatalog.createFilesystemCatalog(); catalog.createDatabase(\u0026#34;my_db\u0026#34;, false); try { catalog.createTable( identifier, new Schema( Lists.newArrayList( new DataField(0, \u0026#34;col1\u0026#34;, DataTypes.STRING(), \u0026#34;field1\u0026#34;), new DataField(1, \u0026#34;col2\u0026#34;, DataTypes.STRING(), \u0026#34;field2\u0026#34;), new DataField(2, \u0026#34;col3\u0026#34;, DataTypes.STRING(), \u0026#34;field3\u0026#34;), new DataField(3, \u0026#34;col4\u0026#34;, DataTypes.BIGINT(), \u0026#34;field4\u0026#34;), new DataField( 4, \u0026#34;col5\u0026#34;, DataTypes.ROW( new DataField( 5, \u0026#34;f1\u0026#34;, DataTypes.STRING(), \u0026#34;f1\u0026#34;), new DataField( 6, \u0026#34;f2\u0026#34;, DataTypes.STRING(), \u0026#34;f2\u0026#34;), new DataField( 7, \u0026#34;f3\u0026#34;, DataTypes.STRING(), \u0026#34;f3\u0026#34;)), \u0026#34;field5\u0026#34;), new DataField(8, \u0026#34;col6\u0026#34;, DataTypes.STRING(), \u0026#34;field6\u0026#34;)), Lists.newArrayList(\u0026#34;col1\u0026#34;), // partition keys  Lists.newArrayList(\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;), // primary key  options, \u0026#34;table comment\u0026#34;), false); } catch (Catalog.TableAlreadyExistException e) { // do something  } catch (Catalog.DatabaseNotExistException e) { // do something  } // add option  SchemaChange addOption = SchemaChange.setOption(\u0026#34;snapshot.time-retained\u0026#34;, \u0026#34;2h\u0026#34;); // remove option  SchemaChange removeOption = SchemaChange.removeOption(\u0026#34;compaction.max.file-num\u0026#34;); // add column  SchemaChange addColumn = SchemaChange.addColumn(\u0026#34;col1_after\u0026#34;, DataTypes.STRING()); // add a column after col1  SchemaChange.Move after = SchemaChange.Move.after(\u0026#34;col1_after\u0026#34;, \u0026#34;col1\u0026#34;); SchemaChange addColumnAfterField = SchemaChange.addColumn(\u0026#34;col7\u0026#34;, DataTypes.STRING(), \u0026#34;\u0026#34;, after); // rename column  SchemaChange renameColumn = SchemaChange.renameColumn(\u0026#34;col3\u0026#34;, \u0026#34;col3_new_name\u0026#34;); // drop column  SchemaChange dropColumn = SchemaChange.dropColumn(\u0026#34;col6\u0026#34;); // update column comment  SchemaChange updateColumnComment = SchemaChange.updateColumnComment(new String[] {\u0026#34;col4\u0026#34;}, \u0026#34;col4 field\u0026#34;); // update nested column comment  SchemaChange updateNestedColumnComment = SchemaChange.updateColumnComment(new String[] {\u0026#34;col5\u0026#34;, \u0026#34;f1\u0026#34;}, \u0026#34;col5 f1 field\u0026#34;); // update column type  SchemaChange updateColumnType = SchemaChange.updateColumnType(\u0026#34;col4\u0026#34;, DataTypes.DOUBLE()); // update column position, you need to pass in a parameter of type Move  SchemaChange updateColumnPosition = SchemaChange.updateColumnPosition(SchemaChange.Move.first(\u0026#34;col4\u0026#34;)); // update column nullability  SchemaChange updateColumnNullability = SchemaChange.updateColumnNullability(new String[] {\u0026#34;col4\u0026#34;}, false); // update nested column nullability  SchemaChange updateNestedColumnNullability = SchemaChange.updateColumnNullability(new String[] {\u0026#34;col5\u0026#34;, \u0026#34;f2\u0026#34;}, false); SchemaChange[] schemaChanges = new SchemaChange[] { addOption, removeOption, addColumn, addColumnAfterField, renameColumn, dropColumn, updateColumnComment, updateNestedColumnComment, updateColumnType, updateColumnPosition, updateColumnNullability, updateNestedColumnNullability }; try { catalog.alterTable(identifier, Arrays.asList(schemaChanges), false); } catch (Catalog.TableNotExistException e) { // do something  } catch (Catalog.ColumnAlreadyExistException e) { // do something  } catch (Catalog.ColumnNotExistException e) { // do something  } } } Table metadata:\n name return a name string to identify this table. rowType return the current row type of this table containing a sequence of table\u0026rsquo;s fields. partitionKeys returns the partition keys of this table. parimaryKeys returns the primary keys of this table. options returns the configuration of this table in a map of key-value. comment returns the optional comment of this table. copy return a new table by applying dynamic options to this table.  "});index.add({'id':2,'href':'/docs/0.9/migration/migration-from-hive/','title':"Migration From Hive",'section':"Migration",'content':"Hive Table Migration #  Apache Hive supports ORC, Parquet file formats that could be migrated to Paimon. When migrating data to a paimon table, the origin table will be permanently disappeared. So please back up your data if you still need the original table. The migrated table will be append table.\nNow, we can use paimon hive catalog with Migrate Table Procedure and Migrate File Procedure to totally migrate a table from hive to paimon. At the same time, you can use paimon hive catalog with Migrate Database Procedure to fully synchronize all tables in the database to paimon.\n Migrate Table Procedure: Paimon table does not exist, use the procedure upgrade hive table to paimon table. Hive table will disappear after action done. Migrate Database Procedure: Paimon table does not exist, use the procedure upgrade all hive tables in database to paimon table. All hive tables will disappear after action done. Migrate File Procedure: Paimon table already exists, use the procedure to migrate files from hive table to paimon table. Notice that, Hive table will also disappear after action done.  These three actions now support file format of hive \u0026ldquo;orc\u0026rdquo; and \u0026ldquo;parquet\u0026rdquo; and \u0026ldquo;avro\u0026rdquo;.\nWe highly recommend to back up hive table data before migrating, because migrating action is not atomic. If been interrupted while migrating, you may lose your data. \nExample for Migration #  Migrate Hive Table\nCommand: CALL sys.migrate_table('hive', '\u0026lt;hive_database\u0026gt;.\u0026lt;hive_tablename\u0026gt;', '\u0026lt;paimon_tableconf\u0026gt;');\nExample\nCREATE CATALOG PAIMON WITH (\u0026#39;type\u0026#39;=\u0026#39;paimon\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;hive\u0026#39;, \u0026#39;uri\u0026#39; = \u0026#39;thrift://localhost:9083\u0026#39;, \u0026#39;warehouse\u0026#39;=\u0026#39;/path/to/warehouse/\u0026#39;); USE CATALOG PAIMON; CALL sys.migrate_table(\u0026#39;hive\u0026#39;, \u0026#39;default.hivetable\u0026#39;, \u0026#39;file.format=orc\u0026#39;); After invoke, \u0026ldquo;hivetable\u0026rdquo; will totally convert to paimon format. Writing and reading the table by old \u0026ldquo;hive way\u0026rdquo; will fail. We can add our table properties while importing by sys.migrate_table('.', \u0026lsquo;').  here should be separated by \u0026ldquo;,\u0026rdquo;. For example:\nCALL sys.migrate_table(\u0026#39;hive\u0026#39;, \u0026#39;my_db.wait_to_upgrate\u0026#39;, \u0026#39;file.format=orc,read.batch-size=2096,write-only=true\u0026#39;) If your flink version is below 1.17, you can use flink action to achieve this:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-0.9.0.jar \\ migrate_table --warehouse \u0026lt;warehouse-path\u0026gt; \\ --source_type hive \\ --table \u0026lt;database.table-name\u0026gt; \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\ [--options \u0026lt;paimon-table-conf [,paimon-table-conf ...]\u0026gt; ] Example:\n\u0026lt;FLINK_HOME\u0026gt;/flink run ./paimon-flink-action-0.9.0.jar migrate_table \\ --warehouse /path/to/warehouse \\ --catalog_conf uri=thrift://localhost:9083 \\ --catalog_conf metastore=hive \\ --source_type hive \\ --table default.hive_or_paimon \\ Migrate Hive Database\nCommand: CALL sys.migrate_database('hive', '\u0026lt;hive_database\u0026gt;', '\u0026lt;paimon_tableconf\u0026gt;');\nExample\nCREATE CATALOG PAIMON WITH (\u0026#39;type\u0026#39;=\u0026#39;paimon\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;hive\u0026#39;, \u0026#39;uri\u0026#39; = \u0026#39;thrift://localhost:9083\u0026#39;, \u0026#39;warehouse\u0026#39;=\u0026#39;/path/to/warehouse/\u0026#39;); USE CATALOG PAIMON; CALL sys.migrate_database(\u0026#39;hive\u0026#39;, \u0026#39;default\u0026#39;, \u0026#39;file.format=orc\u0026#39;); After invoke, all tables in \u0026ldquo;default\u0026rdquo; database will totally convert to paimon format. Writing and reading the table by old \u0026ldquo;hive way\u0026rdquo; will fail. We can add our table properties while importing by sys.migrate_database('\u0026rsquo;, \u0026lsquo;').  here should be separated by \u0026ldquo;,\u0026rdquo;. For example:\nCALL sys.migrate_database(\u0026#39;hive\u0026#39;, \u0026#39;my_db\u0026#39;, \u0026#39;file.format=orc,read.batch-size=2096,write-only=true\u0026#39;) If your flink version is below 1.17, you can use flink action to achieve this:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-0.9.0.jar \\ migrate_databse --warehouse \u0026lt;warehouse-path\u0026gt; \\ --source_type hive \\ --database \u0026lt;database\u0026gt; \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\ [--options \u0026lt;paimon-table-conf [,paimon-table-conf ...]\u0026gt; ] Example:\n\u0026lt;FLINK_HOME\u0026gt;/flink run ./paimon-flink-action-0.9.0.jar migrate_table \\ --warehouse /path/to/warehouse \\ --catalog_conf uri=thrift://localhost:9083 \\ --catalog_conf metastore=hive \\ --source_type hive \\ --database default \\ Migrate Hive File\nCommand: CALL sys.migrate_file('hive', '\u0026lt;hive_database\u0026gt;.\u0026lt;hive_table_name\u0026gt;', '\u0026lt;paimon_database\u0026gt;.\u0026lt;paimon_tablename\u0026gt;');\nExample\nCREATE CATALOG PAIMON WITH (\u0026#39;type\u0026#39;=\u0026#39;paimon\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;hive\u0026#39;, \u0026#39;uri\u0026#39; = \u0026#39;thrift://localhost:9083\u0026#39;, \u0026#39;warehouse\u0026#39;=\u0026#39;/path/to/warehouse/\u0026#39;); USE CATALOG PAIMON; CALL sys.migrate_file(\u0026#39;hive\u0026#39;, \u0026#39;default.hivetable\u0026#39;, \u0026#39;default.paimontable\u0026#39;); After invoke, \u0026ldquo;hivetable\u0026rdquo; will disappear. And all files will be moved and renamed to paimon directory. \u0026ldquo;paimontable\u0026rdquo; here must have the same partition keys with \u0026ldquo;hivetable\u0026rdquo;, and \u0026ldquo;paimontable\u0026rdquo; should be in unaware-bucket mode.\n"});index.add({'id':3,'href':'/docs/0.9/append-table/overview/','title':"Overview",'section':"Table w/o PK",'content':"Overview #  If a table does not have a primary key defined, it is an append table. Compared to the primary key table, it does not have the ability to directly receive changelogs. It cannot be directly updated with data through upsert. It can only receive incoming data from append data.\nFlink CREATE TABLE my_table ( product_id BIGINT, price DOUBLE, sales BIGINT ) WITH ( -- \u0026#39;target-file-size\u0026#39; = \u0026#39;256 MB\u0026#39;,  -- \u0026#39;file.format\u0026#39; = \u0026#39;parquet\u0026#39;,  -- \u0026#39;file.compression\u0026#39; = \u0026#39;zstd\u0026#39;,  -- \u0026#39;file.compression.zstd-level\u0026#39; = \u0026#39;3\u0026#39; );  Batch write and batch read in typical application scenarios, similar to a regular Hive partition table, but compared to the Hive table, it can bring:\n Object storage (S3, OSS) friendly Time Travel and Rollback DELETE / UPDATE with low cost Automatic small file merging in streaming sink Streaming read \u0026amp; write like a queue High performance query with order and index  "});index.add({'id':4,'href':'/docs/0.9/concepts/overview/','title':"Overview",'section':"Concepts",'content':"Overview #  Apache Paimon\u0026rsquo;s Architecture:\nAs shown in the architecture above:\nRead/Write: Paimon supports a versatile way to read/write data and perform OLAP queries.\n For reads, it supports consuming data  from historical snapshots (in batch mode), from the latest offset (in streaming mode), or reading incremental snapshots in a hybrid way.   For writes, it supports  streaming synchronization from the changelog of databases (CDC) batch insert/overwrite from offline data.    Ecosystem: In addition to Apache Flink, Paimon also supports read by other computation engines like Apache Hive, Apache Spark and Trino.\nInternal:\n Under the hood, Paimon stores the columnar files on the filesystem/object-store The metadata of the file is saved in the manifest file, providing large-scale storage and data skipping. For primary key table, uses the LSM tree structure to support a large volume of data updates and high-performance queries.  Unified Storage #  For streaming engines like Apache Flink, there are typically three types of connectors:\n Message queue, such as Apache Kafka, it is used in both source and intermediate stages in this pipeline, to guarantee the latency stay within seconds. OLAP system, such as ClickHouse, it receives processed data in streaming fashion and serving user’s ad-hoc queries. Batch storage, such as Apache Hive, it supports various operations of the traditional batch processing, including INSERT OVERWRITE.  Paimon provides table abstraction. It is used in a way that does not differ from the traditional database:\n In batch execution mode, it acts like a Hive table and supports various operations of Batch SQL. Query it to see the latest snapshot. In streaming execution mode, it acts like a message queue. Query it acts like querying a stream changelog from a message queue where historical data never expires.  "});index.add({'id':5,'href':'/docs/0.9/concepts/spec/overview/','title':"Overview",'section':"Specification",'content':"Spec Overview #  This is the specification for the Paimon table format, this document standardizes the underlying file structure and design of Paimon.\nTerms #   Schema: fields, primary keys definition, partition keys definition and options. Snapshot: the entrance to all data committed at some specific time point. Manifest list: includes several manifest files. Manifest: includes several data files or changelog files. Data File: contains incremental records. Changelog File: contains records produced by changelog-producer. Global Index: index for a bucket or partition. Data File Index: index for a data file.  Run Flink SQL with Paimon:\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;/your/path\u0026#39; ); USE CATALOG my_catalog; CREATE TABLE my_table ( k INT PRIMARY KEY NOT ENFORCED, f0 INT, f1 STRING ); INSERT INTO my_table VALUES (1, 11, \u0026#39;111\u0026#39;); Take a look to the disk:\nwarehouse └── default.db └── my_table ├── bucket-0 │ └── data-59f60cb9-44af-48cc-b5ad-59e85c663c8f-0.orc ├── index │ └── index-5625e6d9-dd44-403b-a738-2b6ea92e20f1-0 ├── manifest │ ├── index-manifest-5d670043-da25-4265-9a26-e31affc98039-0 │ ├── manifest-6758823b-2010-4d06-aef0-3b1b597723d6-0 │ ├── manifest-list-9f856d52-5b33-4c10-8933-a0eddfaa25bf-0 │ └── manifest-list-9f856d52-5b33-4c10-8933-a0eddfaa25bf-1 ├── schema │ └── schema-0 └── snapshot ├── EARLIEST ├── LATEST └── snapshot-1 "});index.add({'id':6,'href':'/docs/0.9/engines/overview/','title':"Overview",'section':"Engine Others",'content':"Overview #  Compatibility Matrix #     Engine Version Batch Read Batch Write Create Table Alter Table Streaming Write Streaming Read Batch Overwrite DELETE \u0026amp; UPDATE MERGE INTO Time Travel     Flink 1.15 - 1.20 ✅ ✅ ✅ ✅(1.17+) ✅ ✅ ✅ ✅(1.17+) ❌ ✅   Spark 3.1 - 3.5 ✅ ✅(3.2+) ✅ ✅ ✅(3.3+) ✅(3.3+) ✅(3.2+) ✅(3.2+) ✅(3.2+) ✅(3.3+)   Hive 2.1 - 3.1 ✅ ✅ ✅ ❌ ❌ ❌ ❌ ❌ ❌ ✅   Trino 420 - 439 ✅ ✅(427+) ✅(427+) ✅(427+) ❌ ❌ ❌ ❌ ❌ ✅   Presto 0.236 - 0.280 ✅ ❌ ✅ ✅ ❌ ❌ ❌ ❌ ❌ ❌   StarRocks 3.1+ ✅ ❌ ❌ ❌ ❌ ❌ ❌ ❌ ❌ ✅   Doris 2.0.6+ ✅ ❌ ❌ ❌ ❌ ❌ ❌ ❌ ❌ ✅    Streaming Engines #  Flink Streaming #  Flink is the most comprehensive streaming computing engine that is widely used for data CDC ingestion and the construction of streaming pipelines.\nRecommended version is Flink 1.17.2.\nSpark Streaming #  You can also use Spark Streaming to build a streaming pipeline. Spark\u0026rsquo;s schema evolution capability will be better implemented, but you must accept the mechanism of mini-batch.\nBatch Engines #  Spark Batch #  Spark Batch is the most widely used batch computing engine.\nRecommended version is Spark 3.4.3.\nFlink Batch #  Flink Batch is also available, which can make your pipeline more integrated with streaming and batch unified.\nOLAP Engines #  StarRocks #  StarRocks is the most recommended OLAP engine with the most advanced integration.\nRecommended version is StarRocks 3.2.6.\nOther OLAP #  You can also use Doris and Trino and Presto, or, you can just use Spark, Flink and Hive to query Paimon tables.\nDownload #  Download Link\n"});index.add({'id':7,'href':'/docs/0.9/filesystems/overview/','title':"Overview",'section':"Filesystems",'content':"Overview #  Apache Paimon utilizes the same pluggable file systems as Apache Flink. Users can follow the standard plugin mechanism to configure the plugin structure if using Flink as compute engine. However, for other engines like Spark or Hive, the provided opt jars (by Flink) may get conflicts and cannot be used directly. It is not convenient for users to fix class conflicts, thus Paimon provides the self-contained and engine-unified FileSystem pluggable jars for user to query tables from Spark/Hive side.\nSupported FileSystems #     FileSystem URI Scheme Pluggable Description     Local File System file:// N Built-in Support   HDFS hdfs:// N Built-in Support, ensure that the cluster is in the hadoop environment   Aliyun OSS oss:// Y    S3 s3:// Y     Dependency #  We recommend you to download the jar directly: Download Link.\nYou can also manually build bundled jar from the source code.\nTo build from source code, clone the git repository.\nBuild shaded jar with the following command.\nmvn clean install -DskipTests You can find the shaded jars under ./paimon-filesystems/paimon-${fs}/target/paimon-${fs}-0.9.0.jar.\n"});index.add({'id':8,'href':'/docs/0.9/flink/cdc-ingestion/overview/','title':"Overview",'section':"CDC Ingestion",'content':"Overview #  Paimon supports a variety of ways to ingest data into Paimon tables with schema evolution. This means that the added columns are synchronized to the Paimon table in real time and the synchronization job will not be restarted for this purpose.\nWe currently support the following sync ways:\n MySQL Synchronizing Table: synchronize one or multiple tables from MySQL into one Paimon table. MySQL Synchronizing Database: synchronize the whole MySQL database into one Paimon database. Program API Sync: synchronize your custom DataStream input into one Paimon table. Kafka Synchronizing Table: synchronize one Kafka topic\u0026rsquo;s table into one Paimon table. Kafka Synchronizing Database: synchronize one Kafka topic containing multiple tables or multiple topics containing one table each into one Paimon database. MongoDB Synchronizing Collection: synchronize one Collection from MongoDB into one Paimon table. MongoDB Synchronizing Database: synchronize the whole MongoDB database into one Paimon database. Pulsar Synchronizing Table: synchronize one Pulsar topic\u0026rsquo;s table into one Paimon table. Pulsar Synchronizing Database: synchronize one Pulsar topic containing multiple tables or multiple topics containing one table each into one Paimon database.  What is Schema Evolution #  Suppose we have a MySQL table named tableA, it has three fields: field_1, field_2, field_3. When we want to load this MySQL table to Paimon, we can do this in Flink SQL, or use MySqlSyncTableAction.\nFlink SQL:\nIn Flink SQL, if we change the table schema of the MySQL table after the ingestion, the table schema change will not be synchronized to Paimon.\nMySqlSyncTableAction:\nIn MySqlSyncTableAction, if we change the table schema of the MySQL table after the ingestion, the table schema change will be synchronized to Paimon, and the data of field_4 which is newly added will be synchronized to Paimon too.\nSchema Change Evolution #  Cdc Ingestion supports a limited number of schema changes. Currently, the framework can not rename table, drop columns, so the behaviors of RENAME TABLE and DROP COLUMN will be ignored, RENAME COLUMN will add a new column. Currently supported schema changes includes:\n  Adding columns.\n  Altering column types. More specifically,\n altering from a string type (char, varchar, text) to another string type with longer length, altering from a binary type (binary, varbinary, blob) to another binary type with longer length, altering from an integer type (tinyint, smallint, int, bigint) to another integer type with wider range, altering from a floating-point type (float, double) to another floating-point type with wider range,  are supported.\n  Computed Functions #  --computed_column are the definitions of computed columns. The argument field is from source table field name.\nTemporal Functions #  Temporal functions can convert date and epoch time to another form. A common use case is to generate partition values.\n  Function Description     year(temporal-column [, precision]) Extract year from the input. Output is an INT value represent the year.   month(temporal-column [, precision]) Extract month of year from the input. Output is an INT value represent the month of year.   day(temporal-column [, precision]) Extract day of month from the input. Output is an INT value represent the day of month.   hour(temporal-column [, precision]) Extract hour from the input. Output is an INT value represent the hour.   minute(temporal-column [, precision]) Extract minute from the input. Output is an INT value represent the minute.   second(temporal-column [, precision]) Extract second from the input. Output is an INT value represent the second.   date_format(temporal-column, format-string [, precision]) Convert the input to desired formatted string. Output type is STRING.    The data type of the temporal-column can be one of the following cases:\n DATE, DATETIME or TIMESTAMP. Any integer numeric type (such as INT and BIGINT). In this case, the data will be considered as epoch time of 1970-01-01 00:00:00. You should set precision of the value (default is 0). STRING. In this case, if you didn\u0026rsquo;t set the time unit, the data will be considered as formatted string of DATE, DATETIME or TIMESTAMP value. Otherwise, the data will be considered as string value of epoch time. So you must set time unit in the latter case.  The precision represents the unit of the epoch time. Currently, There are four valid precisions: 0 (for epoch seconds), 3 (for epoch milliseconds), 6(for epoch microseconds) and 9 (for epoch nanoseconds). Take the time point 1970-01-01 00:00:00.123456789 as an example, the epoch seconds are 0, the epoch milliseconds are 123, the epoch microseconds are 123456, and the epoch nanoseconds are 123456789. The precision should match the input values. You can set precision in this way: date_format(epoch_col, yyyy-MM-dd, 0).\ndate_format is a flexible function which is able to convert the temporal value to various formats with different format strings. A most common format string is yyyy-MM-dd HH:mm:ss.SSS. Another example is yyyy-ww which can extract the year and the week-of-the-year from the input. Note that the output is affected by the locale. For example, in some regions the first day of a week is Monday while in others is Sunday, so if you use date_format(date_col, yyyy-ww) and the input of date_col is 2024-01-07 (Sunday), the output maybe 2024-01 (if the first day of a week is Monday) or 2024-02 (if the first day of a week is Sunday).\nOther Functions #    Function Description     substring(column,beginInclusive) Get column.substring(beginInclusive). Output is a STRING.   substring(column,beginInclusive,endExclusive) Get column.substring(beginInclusive,endExclusive). Output is a STRING.   truncate(column,width) truncate column by width. Output type is same with column.If the column is a STRING, truncate(column,width) will truncate the string to width characters, namely `value.substring(0, width)`. If the column is an INT or LONG, truncate(column,width) will truncate the number with the algorithm `v - (((v % W) + W) % W)`. The `redundant` compute part is to keep the result always positive. If the column is a DECIMAL, truncate(column,width) will truncate the decimal with the algorithm: let `scaled_W = decimal(W, scale(v))`, then return `v - (v % scaled_W)`.   cast(value,dataType) Get a constant value. The output is an atomic type, such as STRING, INT, BOOLEAN, etc.    Special Data Type Mapping #   MySQL TINYINT(1) type will be mapped to Boolean by default. If you want to store number (-128~127) in it like MySQL, you can specify type mapping option tinyint1-not-bool (Use --type_mapping), then the column will be mapped to TINYINT in Paimon table. You can use type mapping option to-nullable (Use --type_mapping) to ignore all NOT NULL constraints (except primary keys). You can use type mapping option to-string (Use --type_mapping) to map all MySQL data type to STRING. You can use type mapping option char-to-string (Use --type_mapping) to map MySQL CHAR(length)/VARCHAR(length) types to STRING. You can use type mapping option longtext-to-bytes (Use --type_mapping) to map MySQL LONGTEXT types to BYTES. MySQL BIGINT UNSIGNED, BIGINT UNSIGNED ZEROFILL, SERIAL will be mapped to DECIMAL(20, 0) by default. You can use type mapping option bigint-unsigned-to-bigint (Use --type_mapping) to map these types to Paimon BIGINT, but there is potential data overflow because BIGINT UNSIGNED can store up to 20 digits integer value but Paimon BIGINT can only store up to 19 digits integer value. So you should ensure the overflow won\u0026rsquo;t occur when using this option. MySQL BIT(1) type will be mapped to Boolean. When using Hive catalog, MySQL TIME type will be mapped to STRING. MySQL BINARY will be mapped to Paimon VARBINARY. This is because the binary value is passed as bytes in binlog, so it should be mapped to byte type (BYTES or VARBINARY). We choose VARBINARY because it can retain the length information.  Custom Job Settings #  Checkpointing #  Use -Dexecution.checkpointing.interval=\u0026lt;interval\u0026gt; to enable checkpointing and set interval. For 0.7 and later versions, if you haven\u0026rsquo;t enabled checkpointing, Paimon will enable checkpointing by default and set checkpoint interval to 180 seconds.\nJob Name #  Use -Dpipeline.name=\u0026lt;job-name\u0026gt; to set custom synchronization job name.\ntable configuration #  You can use --table_conf to set table properties and some flink job properties (like sink.parallelism). If the table is created by the cdc job, the table\u0026rsquo;s properties will be equal to the given properties. Otherwise, the job will use the given properties to alter table\u0026rsquo;s properties. But note that immutable options (like merge-engine) and bucket number won\u0026rsquo;t be altered.\n"});index.add({'id':9,'href':'/docs/0.9/primary-key-table/merge-engine/overview/','title':"Overview",'section':"Merge Engine",'content':"Overview #  When Paimon sink receives two or more records with the same primary keys, it will merge them into one record to keep primary keys unique. By specifying the merge-engine table property, users can choose how records are merged together.\nAlways set table.exec.sink.upsert-materialize to NONE in Flink SQL TableConfig, sink upsert-materialize may result in strange behavior. When the input is out of order, we recommend that you use Sequence Field to correct disorder.  Deduplicate #  The deduplicate merge engine is the default merge engine. Paimon will only keep the latest record and throw away other records with the same primary keys.\nSpecifically, if the latest record is a DELETE record, all records with the same primary keys will be deleted. You can config ignore-delete to ignore it.\n"});index.add({'id':10,'href':'/docs/0.9/primary-key-table/overview/','title':"Overview",'section':"Table with PK",'content':"Overview #  If you define a table with primary key, you can insert, update or delete records in the table.\nPrimary keys consist of a set of columns that contain unique values for each record. Paimon enforces data ordering by sorting the primary key within each bucket, allowing users to achieve high performance by applying filtering conditions on the primary key. See CREATE TABLE.\nBucket #  Unpartitioned tables, or partitions in partitioned tables, are sub-divided into buckets, to provide extra structure to the data that may be used for more efficient querying.\nEach bucket directory contains an LSM tree and its changelog files.\nThe range for a bucket is determined by the hash value of one or more columns in the records. Users can specify bucketing columns by providing the bucket-key option. If no bucket-key option is specified, the primary key (if defined) or the complete record will be used as the bucket key.\nA bucket is the smallest storage unit for reads and writes, so the number of buckets limits the maximum processing parallelism. This number should not be too big, though, as it will result in lots of small files and low read performance. In general, the recommended data size in each bucket is about 200MB - 1GB.\nAlso, see rescale bucket if you want to adjust the number of buckets after a table is created.\nLSM Trees #  Paimon adapts the LSM tree (log-structured merge-tree) as the data structure for file storage. This documentation briefly introduces the concepts about LSM trees.\nSorted Runs #  LSM tree organizes files into several sorted runs. A sorted run consists of one or multiple data files and each data file belongs to exactly one sorted run.\nRecords within a data file are sorted by their primary keys. Within a sorted run, ranges of primary keys of data files never overlap.\nAs you can see, different sorted runs may have overlapping primary key ranges, and may even contain the same primary key. When querying the LSM tree, all sorted runs must be combined and all records with the same primary key must be merged according to the user-specified merge engine and the timestamp of each record.\nNew records written into the LSM tree will be first buffered in memory. When the memory buffer is full, all records in memory will be sorted and flushed to disk. A new sorted run is now created.\n"});index.add({'id':11,'href':'/docs/0.9/flink/quick-start/','title':"Quick Start",'section':"Engine Flink",'content':"Quick Start #  This documentation is a guide for using Paimon in Flink.\nJars #  Paimon currently supports Flink 1.20, 1.19, 1.18, 1.17, 1.16, 1.15. We recommend the latest Flink version for a better experience.\nDownload the jar file with corresponding version.\n Currently, paimon provides two types jar: one of which(the bundled jar) is used for read/write data, and the other(action jar) for operations such as manually compaction,    Version Type Jar     Flink 1.20 Bundled Jar paimon-flink-1.20-0.9.0.jar   Flink 1.19 Bundled Jar paimon-flink-1.19-0.9.0.jar   Flink 1.18 Bundled Jar paimon-flink-1.18-0.9.0.jar   Flink 1.17 Bundled Jar paimon-flink-1.17-0.9.0.jar   Flink 1.16 Bundled Jar paimon-flink-1.16-0.9.0.jar   Flink 1.15 Bundled Jar paimon-flink-1.15-0.9.0.jar   Flink Action Action Jar paimon-flink-action-0.9.0.jar     You can also manually build bundled jar from the source code.\nTo build from source code, clone the git repository.\nBuild bundled jar with the following command.\n mvn clean install -DskipTests  You can find the bundled jar in ./paimon-flink/paimon-flink-\u0026lt;flink-version\u0026gt;/target/paimon-flink-\u0026lt;flink-version\u0026gt;-0.9.0.jar, and the action jar in ./paimon-flink/paimon-flink-action/target/paimon-flink-action-0.9.0.jar.\nStart #  Step 1: Download Flink\nIf you haven\u0026rsquo;t downloaded Flink, you can download Flink, then extract the archive with the following command.\ntar -xzf flink-*.tgz Step 2: Copy Paimon Bundled Jar\nCopy paimon bundled jar to the lib directory of your Flink home.\ncp paimon-flink-*.jar \u0026lt;FLINK_HOME\u0026gt;/lib/ Step 3: Copy Hadoop Bundled Jar\nIf the machine is in a hadoop environment, please ensure the value of the environment variable HADOOP_CLASSPATH include path to the common Hadoop libraries, you do not need to use the following pre-bundled Hadoop jar.  Download Pre-bundled Hadoop jar and copy the jar file to the lib directory of your Flink home.\ncp flink-shaded-hadoop-2-uber-*.jar \u0026lt;FLINK_HOME\u0026gt;/lib/ Step 4: Start a Flink Local Cluster\nIn order to run multiple Flink jobs at the same time, you need to modify the cluster configuration in \u0026lt;FLINK_HOME\u0026gt;/conf/flink-conf.yaml.\ntaskmanager.numberOfTaskSlots:2To start a local cluster, run the bash script that comes with Flink:\n\u0026lt;FLINK_HOME\u0026gt;/bin/start-cluster.sh You should be able to navigate to the web UI at localhost:8081 to view the Flink dashboard and see that the cluster is up and running.\nYou can now start Flink SQL client to execute SQL scripts.\n\u0026lt;FLINK_HOME\u0026gt;/bin/sql-client.sh Step 5: Create a Catalog and a Table\nCatalog -- if you\u0026#39;re trying out Paimon in a distributed environment, -- the warehouse path should be set to a shared file system, such as HDFS or OSS CREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39;=\u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39;=\u0026#39;file:/tmp/paimon\u0026#39; ); USE CATALOG my_catalog; -- create a word count table CREATE TABLE word_count ( word STRING PRIMARY KEY NOT ENFORCED, cnt BIGINT ); Generic-Catalog Using FlinkGenericCatalog, you need to use Hive metastore. Then, you can use all the tables from Paimon, Hive, and Flink Generic Tables (Kafka and other tables)!\nIn this mode, you should use \u0026lsquo;connector\u0026rsquo; option for creating tables.\nPaimon will use hive.metastore.warehouse.dir in your hive-site.xml, please use path with scheme. For example, hdfs://.... Otherwise, Paimon will use the local path.  CREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39;=\u0026#39;paimon-generic\u0026#39;, \u0026#39;hive-conf-dir\u0026#39;=\u0026#39;...\u0026#39;, \u0026#39;hadoop-conf-dir\u0026#39;=\u0026#39;...\u0026#39; ); USE CATALOG my_catalog; -- create a word count table CREATE TABLE word_count ( word STRING PRIMARY KEY NOT ENFORCED, cnt BIGINT ) WITH ( \u0026#39;connector\u0026#39;=\u0026#39;paimon\u0026#39; );  Step 6: Write Data\n-- create a word data generator table CREATE TEMPORARY TABLE word_table ( word STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;, \u0026#39;fields.word.length\u0026#39; = \u0026#39;1\u0026#39; ); -- paimon requires checkpoint interval in streaming mode SET \u0026#39;execution.checkpointing.interval\u0026#39; = \u0026#39;10 s\u0026#39;; -- write streaming data to dynamic table INSERT INTO word_count SELECT word, COUNT(*) FROM word_table GROUP BY word; Step 7: OLAP Query\n-- use tableau result mode SET \u0026#39;sql-client.execution.result-mode\u0026#39; = \u0026#39;tableau\u0026#39;; -- switch to batch mode RESET \u0026#39;execution.checkpointing.interval\u0026#39;; SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;batch\u0026#39;; -- olap query the table SELECT * FROM word_count; You can execute the query multiple times and observe the changes in the results.\nStep 8: Streaming Query\n-- switch to streaming mode SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;streaming\u0026#39;; -- track the changes of table and calculate the count interval statistics SELECT `interval`, COUNT(*) AS interval_cnt FROM (SELECT cnt / 10000 AS `interval` FROM word_count) GROUP BY `interval`; Step 9: Exit\nCancel streaming job in localhost:8081, then execute the following SQL script to exit Flink SQL client.\n-- uncomment the following line if you want to drop the dynamic table and clear the files -- DROP TABLE word_count;  -- exit sql-client EXIT; Stop the Flink local cluster.\n./bin/stop-cluster.sh Use Flink Managed Memory #  Paimon tasks can create memory pools based on executor memory which will be managed by Flink executor, such as managed memory in Flink task manager. It will improve the stability and performance of sinks by managing writer buffers for multiple tasks through executor.\nThe following properties can be set if using Flink managed memory:\n   Option Default Description     sink.use-managed-memory-allocator false If true, flink sink will use managed memory for merge tree; otherwise, it will create an independent memory allocator, which means each task allocates and manages its own memory pool (heap memory), if there are too many tasks in one Executor, it may cause performance issues and even OOM.   sink.managed.writer-buffer-memory 256M Weight of writer buffer in managed memory, Flink will compute the memory size, for writer according to the weight, the actual memory used depends on the running environment. Now the memory size defined in this property are equals to the exact memory allocated to write buffer in runtime.    Use In SQL Users can set memory weight in SQL for Flink Managed Memory, then Flink sink operator will get the memory pool size and create allocator for Paimon writer.\nINSERT INTO paimon_table /*+ OPTIONS(\u0026#39;sink.use-managed-memory-allocator\u0026#39;=\u0026#39;true\u0026#39;, \u0026#39;sink.managed.writer-buffer-memory\u0026#39;=\u0026#39;256M\u0026#39;) */ SELECT * FROM ....; Setting dynamic options #  When interacting with the Paimon table, table options can be tuned without changing the options in the catalog. Paimon will extract job-level dynamic options and take effect in the current session. The dynamic option\u0026rsquo;s key format is paimon.${catalogName}.${dbName}.${tableName}.${config_key}. The catalogName/dbName/tableName can be *, which means matching all the specific parts.\nFor example:\n-- set scan.timestamp-millis=1697018249000 for the table mycatalog.default.T SET \u0026#39;paimon.mycatalog.default.T.scan.timestamp-millis\u0026#39; = \u0026#39;1697018249000\u0026#39;; SELECT * FROM T; -- set scan.timestamp-millis=1697018249000 for the table default.T in any catalog SET \u0026#39;paimon.*.default.T.scan.timestamp-millis\u0026#39; = \u0026#39;1697018249000\u0026#39;; SELECT * FROM T; "});index.add({'id':12,'href':'/docs/0.9/spark/quick-start/','title':"Quick Start",'section':"Engine Spark",'content':"Quick Start #  Preparation #  Paimon currently supports Spark 3.5, 3.4, 3.3, 3.2 and 3.1. We recommend the latest Spark version for a better experience.\nDownload the jar file with corresponding version.\n   Version Jar     Spark 3.5 paimon-spark-3.5-0.9.0.jar   Spark 3.4 paimon-spark-3.4-0.9.0.jar   Spark 3.3 paimon-spark-3.3-0.9.0.jar   Spark 3.2 paimon-spark-3.2-0.9.0.jar   Spark 3.1 paimon-spark-3.1-0.9.0.jar    You can also manually build bundled jar from the source code.\nTo build from source code, clone the git repository.\nBuild bundled jar with the following command.\nmvn clean install -DskipTests For Spark 3.3, you can find the bundled jar in ./paimon-spark/paimon-spark-3.3/target/paimon-spark-3.3-0.9.0.jar.\nSetup #  If you are using HDFS, make sure that the environment variable HADOOP_HOME or HADOOP_CONF_DIR is set.  Step 1: Specify Paimon Jar File\nAppend path to paimon jar file to the --jars argument when starting spark-sql.\nspark-sql ... --jars /path/to/paimon-spark-3.3-0.9.0.jar OR use the --packages option.\nspark-sql ... --packages org.apache.paimon:paimon-spark-3.3:0.9.0 Alternatively, you can copy paimon-spark-3.3-0.9.0.jar under spark/jars in your Spark installation directory.\nStep 2: Specify Paimon Catalog\nCatalog When starting spark-sql, use the following command to register Paimon’s Spark catalog with the name paimon. Table files of the warehouse is stored under /tmp/paimon.\nspark-sql ... \\  --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\  --conf spark.sql.catalog.paimon.warehouse=file:/tmp/paimon \\  --conf spark.sql.extensions=org.apache.paimon.spark.extensions.PaimonSparkSessionExtensions Catalogs are configured using properties under spark.sql.catalog.(catalog_name). In above case, \u0026lsquo;paimon\u0026rsquo; is the catalog name, you can change it to your own favorite catalog name.\nAfter spark-sql command line has started, run the following SQL to create and switch to database default.\nUSE paimon; USE default; After switching to the catalog ('USE paimon'), Spark\u0026rsquo;s existing tables will not be directly accessible, you can use the spark_catalog.${database_name}.${table_name} to access Spark tables.\nGeneric Catalog When starting spark-sql, use the following command to register Paimon’s Spark Generic catalog to replace Spark default catalog spark_catalog. (default warehouse is Spark spark.sql.warehouse.dir)\nCurrently, it is only recommended to use SparkGenericCatalog in the case of Hive metastore, Paimon will infer Hive conf from Spark session, you just need to configure Spark\u0026rsquo;s Hive conf.\nspark-sql ... \\  --conf spark.sql.catalog.spark_catalog=org.apache.paimon.spark.SparkGenericCatalog \\  --conf spark.sql.extensions=org.apache.paimon.spark.extensions.PaimonSparkSessionExtensions Using SparkGenericCatalog, you can use Paimon tables in this Catalog or non-Paimon tables such as Spark\u0026rsquo;s csv, parquet, Hive tables, etc.\n Create Table #  Catalog create table my_table ( k int, v string ) tblproperties ( \u0026#39;primary-key\u0026#39; = \u0026#39;k\u0026#39; ); Generic Catalog create table my_table ( k int, v string ) USING paimon tblproperties ( \u0026#39;primary-key\u0026#39; = \u0026#39;k\u0026#39; ) ;  Insert Table #  Paimon currently supports Spark 3.2+ for SQL write.  INSERT INTO my_table VALUES (1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;); Query Table #  SQL SELECT * FROM my_table; /* 1\tHi 2\tHello */ DataFrame val dataset = spark.read.format(\u0026#34;paimon\u0026#34;).load(\u0026#34;file:/tmp/paimon/default.db/my_table\u0026#34;) dataset.show() /* +---+------+ | k | v| +---+------+ | 1| Hi| | 2| Hello| +---+------+ */  Spark Type Conversion #  This section lists all supported type conversion between Spark and Paimon. All Spark\u0026rsquo;s data types are available in package org.apache.spark.sql.types.\n  Spark Data Type Paimon Data Type Atomic Type     StructType RowType false   MapType MapType false   ArrayType ArrayType false   BooleanType BooleanType true   ByteType TinyIntType true   ShortType SmallIntType true   IntegerType IntType true   LongType BigIntType true   FloatType FloatType true   DoubleType DoubleType true   StringType VarCharType(Integer.MAX_VALUE) true   VarCharType(length) VarCharType(length) true   CharType(length) CharType(length) true   DateType DateType true   TimestampType LocalZonedTimestamp true   TimestampNTZType(Spark3.4+) TimestampType true   DecimalType(precision, scale) DecimalType(precision, scale) true   BinaryType VarBinaryType, BinaryType true    Due to the previous design, in Spark3.3 and below, Paimon will map both Paimon\u0026rsquo;s TimestampType and LocalZonedTimestamp to Spark\u0026rsquo;s TimestampType, and only correctly handle with TimestampType.\nTherefore, when using Spark3.3 and below, reads Paimon table with LocalZonedTimestamp type written by other engines, such as Flink, the query result of LocalZonedTimestamp type will have time zone offset, which needs to be adjusted manually.\nWhen using Spark3.4 and above, all timestamp types can be parsed correctly.\n "});index.add({'id':13,'href':'/docs/0.9/project/roadmap/','title':"Roadmap",'section':"Project",'content':"Roadmap #  Native Format IO #  Integrate native Parquet \u0026amp; ORC reader \u0026amp; writer.\nDeletion Vectors (Merge On Write) #   Primary Key Table Deletion Vectors Mode supports async compaction. Append Table supports DELETE \u0026amp; UPDATE with Deletion Vectors Mode. (Now only Spark SQL) Optimize lookup performance for HDD disk.  Flink Lookup Join #  Support Flink Custom Data Distribution Lookup Join to reach large-scale data lookup join.\nProduce Iceberg snapshots #  Introduce a mode to produce Iceberg snapshots.\nBranch #  Branch production ready.\nChangelog life cycle decouple #  Changelog life cycle decouple supports none changelog-producer.\nPartition Mark Done #  Support partition mark done.\nDefault File Format #   Default compression is ZSTD with level 1. Parquet supports filter push down. Parquet supports arrow with row type element. Parquet becomes default file format.  Variant Type #  Support Variant Type with Spark 4.0 and Flink 2.0. Unlocking support for semi-structured data.\nBucketed Join #  Support Bucketed Join with Spark SQL to reduce shuffler in Join.\nFile Index #  Add more index:\n Bitmap Inverse  Column Family #  Support Column Family for super Wide Table.\nView \u0026amp; Function support #  Paimon Catalog supports views and functions.\nFiles Schema Evolution Ingestion #  Introduce a files Ingestion with Schema Evolution.\nForeign Key Join #  Explore Foreign Key Join solution.\n"});index.add({'id':14,'href':'/docs/0.9/maintenance/system-tables/','title':"System Tables",'section':"Maintenance",'content':"System Tables #  Table Specified System Table #  Table specified system tables contain metadata and information about each table, such as the snapshots created and the options in use. Users can access system tables with batch queries.\nCurrently, Flink, Spark, Trino and StarRocks support querying system tables.\nIn some cases, the table name needs to be enclosed with back quotes to avoid syntax parsing conflicts, for example triple access mode:\nSELECT * FROM my_catalog.my_db.`my_table$snapshots`; Snapshots Table #  You can query the snapshot history information of the table through snapshots table, including the record count occurred in the snapshot.\nSELECT * FROM my_table$snapshots; /* +--------------+------------+-----------------+-------------------+--------------+-------------------------+--------------------------------+------------------------------- +--------------------------------+---------------------+---------------------+-------------------------+----------------+ | snapshot_id | schema_id | commit_user | commit_identifier | commit_kind | commit_time | base_manifest_list | delta_manifest_list | changelog_manifest_list | total_record_count | delta_record_count | changelog_record_count | watermark | +--------------+------------+-----------------+-------------------+--------------+-------------------------+--------------------------------+------------------------------- +--------------------------------+---------------------+---------------------+-------------------------+----------------+ | 2 | 0 | 7ca4cd28-98e... | 2 | APPEND | 2022-10-26 11:44:15.600 | manifest-list-31323d5f-76e6... | manifest-list-31323d5f-76e6... | manifest-list-31323d5f-76e6... | 2 | 2 | 0 | 1666755855600 | | 1 | 0 | 870062aa-3e9... | 1 | APPEND | 2022-10-26 11:44:15.148 | manifest-list-31593d5f-76e6... | manifest-list-31593d5f-76e6... | manifest-list-31593d5f-76e6... | 1 | 1 | 0 | 1666755855148 | +--------------+------------+-----------------+-------------------+--------------+-------------------------+--------------------------------+------------------------------- +--------------------------------+---------------------+---------------------+-------------------------+----------------+ 2 rows in set */ By querying the snapshots table, you can know the commit and expiration information about that table and time travel through the data.\nSchemas Table #  You can query the historical schemas of the table through schemas table.\nSELECT * FROM my_table$schemas; /* +-----------+--------------------------------+----------------+--------------+---------+---------+-------------------------+ | schema_id | fields | partition_keys | primary_keys | options | comment | update_time | +-----------+--------------------------------+----------------+--------------+---------+---------+-------------------------+ | 0 | [{\u0026#34;id\u0026#34;:0,\u0026#34;name\u0026#34;:\u0026#34;word\u0026#34;,\u0026#34;typ... | [] | [\u0026#34;word\u0026#34;] | {} | | 2022-10-28 11:44:20.600 | | 1 | [{\u0026#34;id\u0026#34;:0,\u0026#34;name\u0026#34;:\u0026#34;word\u0026#34;,\u0026#34;typ... | [] | [\u0026#34;word\u0026#34;] | {} | | 2022-10-27 11:44:15.600 | | 2 | [{\u0026#34;id\u0026#34;:0,\u0026#34;name\u0026#34;:\u0026#34;word\u0026#34;,\u0026#34;typ... | [] | [\u0026#34;word\u0026#34;] | {} | | 2022-10-26 11:44:10.600 | +-----------+--------------------------------+----------------+--------------+---------+---------+-------------------------+ 3 rows in set */ You can join the snapshots table and schemas table to get the fields of given snapshots.\nSELECT s.snapshot_id, t.schema_id, t.fields FROM my_table$snapshots s JOIN my_table$schemas t ON s.schema_id=t.schema_id where s.snapshot_id=100; Options Table #  You can query the table\u0026rsquo;s option information which is specified from the DDL through options table. The options not shown will be the default value. You can take reference to Configuration.\nSELECT * FROM my_table$options; /* +------------------------+--------------------+ | key | value | +------------------------+--------------------+ | snapshot.time-retained | 5 h | +------------------------+--------------------+ 1 rows in set */ Audit log Table #  If you need to audit the changelog of the table, you can use the audit_log system table. Through audit_log table, you can get the rowkind column when you get the incremental data of the table. You can use this column for filtering and other operations to complete the audit.\nThere are four values for rowkind:\n +I: Insertion operation. -U: Update operation with the previous content of the updated row. +U: Update operation with new content of the updated row. -D: Deletion operation.  SELECT * FROM my_table$audit_log; /* +------------------+-----------------+-----------------+ | rowkind | column_0 | column_1 | +------------------+-----------------+-----------------+ | +I | ... | ... | +------------------+-----------------+-----------------+ | -U | ... | ... | +------------------+-----------------+-----------------+ | +U | ... | ... | +------------------+-----------------+-----------------+ 3 rows in set */ Read-optimized Table #  If you require extreme reading performance and can accept reading slightly old data, you can use the ro (read-optimized) system table. Read-optimized system table improves reading performance by only scanning files which does not need merging.\nFor primary-key tables, ro system table only scans files on the topmost level. That is to say, ro system table only produces the result of the latest full compaction.\nIt is possible that different buckets carry out full compaction at difference times, so it is possible that the values of different keys come from different snapshots.  For append tables, as all files can be read without merging, ro system table acts like the normal append table.\nSELECT * FROM my_table$ro; Files Table #  You can query the files of the table with specific snapshot.\n-- Query the files of latest snapshot SELECT * FROM my_table$files; /* +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+---------------------+---------------------+-----------------------+ | partition | bucket | file_path | file_format | schema_id | level | record_count | file_size_in_bytes | min_key | max_key | null_value_counts | min_value_stats | max_value_stats | min_sequence_number | max_sequence_number | creation_time | +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+---------------------+---------------------+-----------------------+ | [3] | 0 | data-8f64af95-29cc-4342-adc... | orc | 0 | 0 | 1 | 593 | [c] | [c] | {cnt=0, val=0, word=0} | {cnt=3, val=33, word=c} | {cnt=3, val=33, word=c} | 1691551246234 | 1691551246637 |2023-02-24T16:06:21.166| | [2] | 0 | data-8b369068-0d37-4011-aa5... | orc | 0 | 0 | 1 | 593 | [b] | [b] | {cnt=0, val=0, word=0} | {cnt=2, val=22, word=b} | {cnt=2, val=22, word=b} | 1691551246233 | 1691551246732 |2023-02-24T16:06:21.166| | [2] | 0 | data-83aa7973-060b-40b6-8c8... | orc | 0 | 0 | 1 | 605 | [d] | [d] | {cnt=0, val=0, word=0} | {cnt=2, val=32, word=d} | {cnt=2, val=32, word=d} | 1691551246267 | 1691551246798 |2023-02-24T16:06:21.166| | [5] | 0 | data-3d304f4a-bcea-44dc-a13... | orc | 0 | 0 | 1 | 593 | [c] | [c] | {cnt=0, val=0, word=0} | {cnt=5, val=51, word=c} | {cnt=5, val=51, word=c} | 1691551246788 | 1691551246152 |2023-02-24T16:06:21.166| | [1] | 0 | data-10abb5bc-0170-43ae-b6a... | orc | 0 | 0 | 1 | 595 | [a] | [a] | {cnt=0, val=0, word=0} | {cnt=1, val=11, word=a} | {cnt=1, val=11, word=a} | 1691551246722 | 1691551246273 |2023-02-24T16:06:21.166| | [4] | 0 | data-2c9b7095-65b7-4013-a7a... | orc | 0 | 0 | 1 | 593 | [a] | [a] | {cnt=0, val=0, word=0} | {cnt=4, val=12, word=a} | {cnt=4, val=12, word=a} | 1691551246321 | 1691551246109 |2023-02-24T16:06:21.166| +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+---------------------+---------------------+-----------------------+ 6 rows in set */ -- You can also query the files with specific snapshot SELECT * FROM my_table$files /*+ OPTIONS(\u0026#39;scan.snapshot-id\u0026#39;=\u0026#39;1\u0026#39;) */; /* +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+---------------------+---------------------+-----------------------+ | partition | bucket | file_path | file_format | schema_id | level | record_count | file_size_in_bytes | min_key | max_key | null_value_counts | min_value_stats | max_value_stats | min_sequence_number | max_sequence_number | creation_time | +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+---------------------+---------------------+-----------------------+ | [3] | 0 | data-8f64af95-29cc-4342-adc... | orc | 0 | 0 | 1 | 593 | [c] | [c] | {cnt=0, val=0, word=0} | {cnt=3, val=33, word=c} | {cnt=3, val=33, word=c} | 1691551246234 | 1691551246637 |2023-02-24T16:06:21.166| | [2] | 0 | data-8b369068-0d37-4011-aa5... | orc | 0 | 0 | 1 | 593 | [b] | [b] | {cnt=0, val=0, word=0} | {cnt=2, val=22, word=b} | {cnt=2, val=22, word=b} | 1691551246233 | 1691551246732 |2023-02-24T16:06:21.166| | [1] | 0 | data-10abb5bc-0170-43ae-b6a... | orc | 0 | 0 | 1 | 595 | [a] | [a] | {cnt=0, val=0, word=0} | {cnt=1, val=11, word=a} | {cnt=1, val=11, word=a} | 1691551246267 | 1691551246798 |2023-02-24T16:06:21.166| +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+---------------------+---------------------+-----------------------+ 3 rows in set */ Tags Table #  You can query the tag history information of the table through tags table, including which snapshots are the tags based on and some historical information of the snapshots. You can also get all tag names and time travel to a specific tag data by name.\nSELECT * FROM my_table$tags; /* +----------+-------------+-----------+-------------------------+--------------+--------------+ | tag_name | snapshot_id | schema_id | commit_time | record_count | branches | +----------+-------------+-----------+-------------------------+--------------+--------------+ | tag1 | 1 | 0 | 2023-06-28 14:55:29.344 | 3 | [] | | tag3 | 3 | 0 | 2023-06-28 14:58:24.691 | 7 | [branch-1] | +----------+-------------+-----------+-------------------------+--------------+--------------+ 2 rows in set */ Branches Table #  You can query the branches of the table.\nSELECT * FROM my_table$branches; /* +----------------------+---------------------------+--------------------------+-------------------------+ | branch_name | created_from_tag | created_from_snapshot | create_time | +----------------------+---------------------------+--------------------------+-------------------------+ | branch1 | tag1 | 2 | 2024-07-18 20:31:39.084 | | branch2 | tag2 | 5 | 2024-07-18 21:11:14.373 | +----------------------+---------------------------+--------------------------+-------------------------+ 2 rows in set */ Consumers Table #  You can query all consumers which contains next snapshot.\nSELECT * FROM my_table$consumers; /* +-------------+------------------+ | consumer_id | next_snapshot_id | +-------------+------------------+ | id1 | 1 | | id2 | 3 | +-------------+------------------+ 2 rows in set */ Manifests Table #  You can query all manifest files contained in the latest snapshot or the specified snapshot of the current table.\n-- Query the manifest of latest snapshot SELECT * FROM my_table$manifests; /* +--------------------------------+-------------+------------------+-------------------+---------------+ | file_name | file_size | num_added_files | num_deleted_files | schema_id | +--------------------------------+-------------+------------------+-------------------+---------------+ | manifest-f4dcab43-ef6b-4713... | 12365| 40 | 0 | 0 | | manifest-f4dcab43-ef6b-4713... | 1648 | 1 | 0 | 0 | +--------------------------------+-------------+------------------+-------------------+---------------+ 2 rows in set */ -- You can also query the manifest with specified snapshot SELECT * FROM my_table$manifests /*+ OPTIONS(\u0026#39;scan.snapshot-id\u0026#39;=\u0026#39;1\u0026#39;) */; /* +--------------------------------+-------------+------------------+-------------------+---------------+ | file_name | file_size | num_added_files | num_deleted_files | schema_id | +--------------------------------+-------------+------------------+-------------------+---------------+ | manifest-f4dcab43-ef6b-4713... | 12365| 40 | 0 | 0 | +--------------------------------+-------------+------------------+-------------------+---------------+ 1 rows in set */ Aggregation fields Table #  You can query the historical aggregation of the table through aggregation fields table.\nSELECT * FROM my_table$aggregation_fields; /* +------------+-----------------+--------------+--------------------------------+---------+ | field_name | field_type | function | function_options | comment | +------------+-----------------+--------------+--------------------------------+---------+ | product_id | BIGINT NOT NULL | [] | [] | \u0026lt;NULL\u0026gt; | | price | INT | [true,count] | [fields.price.ignore-retrac... | \u0026lt;NULL\u0026gt; | | sales | BIGINT | [sum] | [fields.sales.aggregate-fun... | \u0026lt;NULL\u0026gt; | +------------+-----------------+--------------+--------------------------------+---------+ 3 rows in set */ Partitions Table #  You can query the partition files of the table.\nSELECT * FROM my_table$partitions; /* +---------------+----------------+--------------------+--------------------+------------------------+ | partition | record_count | file_size_in_bytes| file_count| last_update_time| +---------------+----------------+--------------------+--------------------+------------------------+ | [1] | 1 | 645 | 1 | 2024-06-24 10:25:57.400| +---------------+----------------+--------------------+--------------------+------------------------+ */ Global System Table #  Global system tables contain the statistical information of all the tables exists in paimon. For convenient of searching, we create a reference system database called sys. We can display all the global system tables by sql in flink:\nUSE sys; SHOW TABLES; ALL Options Table #  This table is similar to Options Table, but it shows all the table options is all database.\nSELECT * FROM sys.all_table_options; /* +---------------+--------------------------------+--------------------------------+------------------+ | database_name | table_name | key | value | +---------------+--------------------------------+--------------------------------+------------------+ | my_db | Orders_orc | bucket | -1 | | my_db | Orders2 | bucket | -1 | | my_db | Orders2 | sink.parallelism | 7 | | my_db2| OrdersSum | bucket | 1 | +---------------+--------------------------------+--------------------------------+------------------+ 7 rows in set */ Catalog Options Table #  You can query the catalog\u0026rsquo;s option information through catalog options table. The options not shown will be the default value. You can take reference to Configuration.\nSELECT * FROM sys.catalog_options; /* +-----------+---------------------------+ | key | value | +-----------+---------------------------+ | warehouse | hdfs:///path/to/warehouse | +-----------+---------------------------+ 1 rows in set */ "});index.add({'id':15,'href':'/docs/0.9/learn-paimon/understand-files/','title':"Understand Files",'section':"Learn Paimon",'content':"Understand Files #  This article is specifically designed to clarify the impact that various file operations have on files.\nThis page provides concrete examples and practical tips for effectively managing them. Furthermore, through an in-depth exploration of operations such as commit and compact, we aim to offer insights into the creation and updates of files.\nPrerequisite #  Before delving further into this page, please ensure that you have read through the following sections:\n Basic Concepts, Primary Key Table and Append Table How to use Paimon in Flink.  Understand File Operations #  Create Catalog #  Start Flink SQL client via ./sql-client.sh and execute the following statements one by one to create a Paimon catalog.\nCREATE CATALOG paimon WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;file:///tmp/paimon\u0026#39; ); USE CATALOG paimon; This will only create a directory at given path file:///tmp/paimon.\nCreate Table #  Execute the following create table statement will create a Paimon table with 3 fields:\nCREATE TABLE T ( id BIGINT, a INT, b STRING, dt STRING COMMENT \u0026#39;timestamp string in format yyyyMMdd\u0026#39;, PRIMARY KEY(id, dt) NOT ENFORCED ) PARTITIONED BY (dt); This will create Paimon table T under the path /tmp/paimon/default.db/T, with its schema stored in /tmp/paimon/default.db/T/schema/schema-0\nInsert Records Into Table #  Run the following insert statement in Flink SQL:\nINSERT INTO T VALUES (1, 10001, \u0026#39;varchar00001\u0026#39;, \u0026#39;20230501\u0026#39;); Once the Flink job is completed, the records are written to the Paimon table through a successful commit. Users can verify the visibility of these records by executing the query SELECT * FROM T which will return a single row. The commit process creates a snapshot located at the path /tmp/paimon/default.db/T/snapshot/snapshot-1. The resulting file layout at snapshot-1 is as described below:\nThe content of snapshot-1 contains metadata of the snapshot, such as manifest list and schema id:\n{ \u0026#34;version\u0026#34; : 3, \u0026#34;id\u0026#34; : 1, \u0026#34;schemaId\u0026#34; : 0, \u0026#34;baseManifestList\u0026#34; : \u0026#34;manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-0\u0026#34;, \u0026#34;deltaManifestList\u0026#34; : \u0026#34;manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-1\u0026#34;, \u0026#34;changelogManifestList\u0026#34; : null, \u0026#34;commitUser\u0026#34; : \u0026#34;7d758485-981d-4b1a-a0c6-d34c3eb254bf\u0026#34;, \u0026#34;commitIdentifier\u0026#34; : 9223372036854775807, \u0026#34;commitKind\u0026#34; : \u0026#34;APPEND\u0026#34;, \u0026#34;timeMillis\u0026#34; : 1684155393354, \u0026#34;logOffsets\u0026#34; : { }, \u0026#34;totalRecordCount\u0026#34; : 1, \u0026#34;deltaRecordCount\u0026#34; : 1, \u0026#34;changelogRecordCount\u0026#34; : 0, \u0026#34;watermark\u0026#34; : -9223372036854775808 } Remind that a manifest list contains all changes of the snapshot, baseManifestList is the base file upon which the changes in deltaManifestList is applied. The first commit will result in 1 manifest file, and 2 manifest lists are created (the file names might differ from those in your experiment):\n./T/manifest: manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-1\tmanifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-0 manifest-2b833ea4-d7dc-4de0-ae0d-ad76eced75cc-0 manifest-2b833ea4-d7dc-4de0-ae0d-ad76eced75cc-0 is the manifest file (manifest-1-0 in the above graph), which stores the information about the data files in the snapshot.\nmanifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-0 is the baseManifestList (manifest-list-1-base in the above graph), which is effectively empty.\nmanifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-1 is the deltaManifestList (manifest-list-1-delta in the above graph), which contains a list of manifest entries that perform operations on data files, which, in this case, is manifest-1-0.\nNow let\u0026rsquo;s insert a batch of records across different partitions and see what happens. In Flink SQL, execute the following statement:\nINSERT INTO T VALUES (2, 10002, \u0026#39;varchar00002\u0026#39;, \u0026#39;20230502\u0026#39;), (3, 10003, \u0026#39;varchar00003\u0026#39;, \u0026#39;20230503\u0026#39;), (4, 10004, \u0026#39;varchar00004\u0026#39;, \u0026#39;20230504\u0026#39;), (5, 10005, \u0026#39;varchar00005\u0026#39;, \u0026#39;20230505\u0026#39;), (6, 10006, \u0026#39;varchar00006\u0026#39;, \u0026#39;20230506\u0026#39;), (7, 10007, \u0026#39;varchar00007\u0026#39;, \u0026#39;20230507\u0026#39;), (8, 10008, \u0026#39;varchar00008\u0026#39;, \u0026#39;20230508\u0026#39;), (9, 10009, \u0026#39;varchar00009\u0026#39;, \u0026#39;20230509\u0026#39;), (10, 10010, \u0026#39;varchar00010\u0026#39;, \u0026#39;20230510\u0026#39;); The second commit takes place and executing SELECT * FROM T will return 10 rows. A new snapshot, namely snapshot-2, is created and gives us the following physical file layout:\n% ls -1tR . ./T: dt=20230501 dt=20230502\tdt=20230503\tdt=20230504\tdt=20230505\tdt=20230506\tdt=20230507\tdt=20230508\tdt=20230509\tdt=20230510\tsnapshot schema manifest ./T/snapshot: LATEST snapshot-2 EARLIEST snapshot-1 ./T/manifest: manifest-list-9ac2-5e79-4978-a3bc-86c25f1a303f-1 # delta manifest list for snapshot-2 manifest-list-9ac2-5e79-4978-a3bc-86c25f1a303f-0 # base manifest list for snapshot-2\t manifest-f1267033-e246-4470-a54c-5c27fdbdd074-0\t# manifest file for snapshot-2 manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-1 # delta manifest list for snapshot-1  manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-0 # base manifest list for snapshot-1 manifest-2b833ea4-d7dc-4de0-ae0d-ad76eced75cc-0 # manifest file for snapshot-1 ./T/dt=20230501/bucket-0: data-b75b7381-7c8b-430f-b7e5-a204cb65843c-0.orc ... # each partition has the data written to bucket-0 ... ./T/schema: schema-0 The new file layout as of snapshot-2 looks like Delete Records From Table #  Now let\u0026rsquo;s delete records that meet the condition dt\u0026gt;=20230503. In Flink SQL, execute the following statement:\nBatch DELETE FROM T WHERE dt \u0026gt;= \u0026#39;20230503\u0026#39;; The third commit takes place and it gives us snapshot-3. Now, listing the files under the table and your will find out no partition is dropped. Instead, a new data file is created for partition 20230503 to 20230510:\n./T/dt=20230510/bucket-0: data-b93f468c-b56f-4a93-adc4-b250b3aa3462-0.orc # newer data file created by the delete statement  data-0fcacc70-a0cb-4976-8c88-73e92769a762-0.orc # older data file created by the insert statement This make sense since we insert a record in the second commit (represented by +I[10, 10010, 'varchar00010', '20230510']) and then delete the record in the third commit. Executing SELECT * FROM T will return 2 rows, namely:\n+I[1, 10001, 'varchar00001', '20230501'] +I[2, 10002, 'varchar00002', '20230502'] The new file layout as of snapshot-3 looks like Note that manifest-3-0 contains 8 manifest entries of ADD operation type, corresponding to 8 newly written data files.\nCompact Table #  As you may have noticed, the number of small files will augment over successive snapshots, which may lead to decreased read performance. Therefore, a full-compaction is needed in order to reduce the number of small files.\nLet\u0026rsquo;s trigger the full-compaction now, and run a dedicated compaction job through flink run:\nBatch \u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  -D execution.runtime-mode=batch \\  /path/to/paimon-flink-action-0.9.0.jar \\  compact \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  [--partition \u0026lt;partition-name\u0026gt;] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table_conf \u0026lt;paimon-table-dynamic-conf\u0026gt; [--table_conf \u0026lt;paimon-table-dynamic-conf\u0026gt;] ...] an example would be (suppose you\u0026rsquo;re already in Flink home)\n./bin/flink run \\  ./lib/paimon-flink-action-0.9.0.jar \\  compact \\  --path file:///tmp/paimon/default.db/T All current table files will be compacted and a new snapshot, namely snapshot-4, is made and contains the following information:\n{ \u0026#34;version\u0026#34; : 3, \u0026#34;id\u0026#34; : 4, \u0026#34;schemaId\u0026#34; : 0, \u0026#34;baseManifestList\u0026#34; : \u0026#34;manifest-list-9be16-82e7-4941-8b0a-7ce1c1d0fa6d-0\u0026#34;, \u0026#34;deltaManifestList\u0026#34; : \u0026#34;manifest-list-9be16-82e7-4941-8b0a-7ce1c1d0fa6d-1\u0026#34;, \u0026#34;changelogManifestList\u0026#34; : null, \u0026#34;commitUser\u0026#34; : \u0026#34;a3d951d5-aa0e-4071-a5d4-4c72a4233d48\u0026#34;, \u0026#34;commitIdentifier\u0026#34; : 9223372036854775807, \u0026#34;commitKind\u0026#34; : \u0026#34;COMPACT\u0026#34;, \u0026#34;timeMillis\u0026#34; : 1684163217960, \u0026#34;logOffsets\u0026#34; : { }, \u0026#34;totalRecordCount\u0026#34; : 38, \u0026#34;deltaRecordCount\u0026#34; : 20, \u0026#34;changelogRecordCount\u0026#34; : 0, \u0026#34;watermark\u0026#34; : -9223372036854775808 } The new file layout as of snapshot-4 looks like Note that manifest-4-0 contains 20 manifest entries (18 DELETE operations and 2 ADD operations)\n For partition 20230503 to 20230510, two DELETE operations for two data files For partition 20230501 to 20230502, one DELETE operation and one ADD operation for the same data file.  Alter Table #  Execute the following statement to configure full-compaction:\nALTER TABLE T SET (\u0026#39;full-compaction.delta-commits\u0026#39; = \u0026#39;1\u0026#39;); It will create a new schema for Paimon table, namely schema-1, but no snapshot has actually used this schema yet until the next commit.\nExpire Snapshots #  Remind that the marked data files are not truly deleted until the snapshot expires and no consumer depends on the snapshot. For more information, see Expiring Snapshots.\nDuring the process of snapshot expiration, the range of snapshots is initially determined, and then data files within these snapshots are marked for deletion. A data file is marked for deletion only when there is a manifest entry of kind DELETE that references that specific data file. This marking ensures that the file will not be utilized by subsequent snapshots and can be safely removed.\nLet\u0026rsquo;s say all 4 snapshots in the above diagram are about to expire. The expire process is as follows:\n  It first deletes all marked data files, and records any changed buckets.\n  It then deletes any changelog files and associated manifests.\n  Finally, it deletes the snapshots themselves and writes the earliest hint file.\n  If any directories are left empty after the deletion process, they will be deleted as well.\nLet\u0026rsquo;s say another snapshot, snapshot-5 is created and snapshot expiration is triggered. snapshot-1 to snapshot-4 are\nto be deleted. For simplicity, we will only focus on files from previous snapshots, the final layout after snapshot expiration looks like:\nAs a result, partition 20230503 to 20230510 are physically deleted.\nFlink Stream Write #  Finally, we will examine Flink Stream Write by utilizing the example of CDC ingestion. This section will address the capturing and writing of change data into Paimon, as well as the mechanisms behind asynchronous compact and snapshot commit and expiration.\nTo begin, let\u0026rsquo;s take a closer look at the CDC ingestion workflow and the unique roles played by each component involved.\n MySQL CDC Source uniformly reads snapshot and incremental data, with SnapshotReader reading snapshot data and BinlogReader reading incremental data, respectively. Paimon Sink writes data into Paimon table in bucket level. The CompactManager within it will trigger compaction asynchronously. Committer Operator is a singleton responsible for committing and expiring snapshots.  Next, we will go over end-to-end data flow.\nMySQL Cdc Source read snapshot and incremental data and emit them to downstream after normalization.\nPaimon Sink first buffers new records in a heap-based LSM tree, and flushes them to disk when the memory buffer is full. Note that each data file written is a sorted run. At this point, no manifest file and snapshot is created. Right before Flink checkpoint takes places, Paimon Sink will flush all buffered records and send committable message to downstream, which is read and committed by Committer Operator during checkpoint.\nDuring checkpoint, Committer Operator will create a new snapshot and associate it with manifest lists so that the snapshot\ncontains information about all data files in the table.\nAt later point asynchronous compaction might take place, and the committable produced by CompactManager contains information about previous files and merged files so that Committer Operator can construct corresponding manifest entries. In this case Committer Operator might produce two snapshot during Flink checkpoint, one for data written (snapshot of kind Append) and the other for compact (snapshot of kind Compact). If no data file is written during checkpoint interval, only snapshot of kind Compact will be created. Committer Operator will check against snapshot expiration and perform physical deletion of marked data files.\nUnderstand Small Files #  Many users are concerned about small files, which can lead to:\n Stability issue: Too many small files in HDFS, NameNode will be overstressed. Cost issue: A small file in HDFS will temporarily use the size of a minimum of one Block, for example 128 MB. Query efficiency: The efficiency of querying too many small files will be affected.  Understand Checkpoints #  Assuming you are using Flink Writer, each checkpoint generates 1-2 snapshots, and the checkpoint forces the files to be generated on DFS, so the smaller the checkpoint interval the more small files will be generated.\n So first thing is increase checkpoint interval.  By default, not only checkpoint will cause the file to be generated, but writer\u0026rsquo;s memory (write-buffer-size) exhaustion will also flush data to DFS and generate the corresponding file. You can enable write-buffer-spillable to generate spilled files in writer to generate bigger files in DFS.\nSo second thing is increase write-buffer-size or enable write-buffer-spillable.  Understand Snapshots #  Paimon maintains multiple versions of files, compaction and deletion of files are logical and do not actually delete files. Files are only really deleted when Snapshot is expired, so the first way to reduce files is to reduce the time it takes for snapshot to be expired. Flink writer will automatically expire snapshots.\nSee Expire Snapshots.\nUnderstand Partitions and Buckets #  Paimon files are organized in a layered style. The following image illustrates the file layout. Starting from a snapshot file, Paimon readers can recursively access all records from the table.\nFor example, the following table:\nCREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ) PARTITIONED BY (dt, hh) WITH ( \u0026#39;bucket\u0026#39; = \u0026#39;10\u0026#39; ); The table data will be physically sliced into different partitions, and different buckets inside, so if the overall data volume is too small, there is at least one file in a single bucket, I suggest you configure a smaller number of buckets, otherwise there will be quite a few small files as well.\nUnderstand LSM for Primary Table #  LSM tree organizes files into several sorted runs. A sorted run consists of one or multiple data files and each data file belongs to exactly one sorted run.\nBy default, sorted runs number depends on num-sorted-run.compaction-trigger, see Compaction for Primary Key Table, this means that there are at least 5 files in a bucket. If you want to reduce this number, you can keep fewer files, but write performance may suffer.\nUnderstand Files for Bucketed Append Table #  By default, Append also does automatic compaction to reduce the number of small files.\nHowever, for Bucketed Append table, it will only compact the files within the Bucket for sequential purposes, which may keep more small files. See Bucketed Append.\nUnderstand Full-Compaction #  Maybe you think the 5 files for the primary key table are actually okay, but the Append table (bucket) may have 50 small files in a single bucket, which is very difficult to accept. Worse still, partitions that are no longer active also keep so many small files.\nConfigure ‘full-compaction.delta-commits’ perform full-compaction periodically in Flink writing. And it can ensure that partitions are full compacted before writing ends.\n"});index.add({'id':16,'href':'/docs/0.9/migration/upsert-to-partitioned/','title':"Upsert To Partitioned",'section':"Migration",'content':"Upsert To Partitioned #  The Tag Management will maintain the manifests and data files of the snapshot. A typical usage is creating tags daily, then you can maintain the historical data of each day for batch reading.\nWhen using primary key tables, a non-partitioned approach is often used to maintain updates, in order to mirror and synchronize tables from upstream database tables. This allows users to query the latest data. The tradition of Hive data warehouses is not like this. Offline data warehouses require an immutable view every day to ensure the idempotence of calculations. So we created a Tag mechanism to output these views.\nHowever, the traditional use of Hive data warehouses is more accustomed to using partitions to specify the query\u0026rsquo;s Tag, and is more accustomed to using Hive computing engines.\nSo, we introduce 'metastore.tag-to-partition' and 'metastore.tag-to-partition.preview' to mapping a non-partitioned primary key table to the partition table in Hive metastore, and mapping the partition field to the name of the Tag to be fully compatible with Hive.\nExample for Tag to Partition #  Step 1: Create table and tag in Flink SQL\nFlink CREATE CATALOG my_hive WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;hive\u0026#39;, -- \u0026#39;uri\u0026#39; = \u0026#39;thrift://\u0026lt;hive-metastore-host-name\u0026gt;:\u0026lt;port\u0026gt;\u0026#39;, default use \u0026#39;hive.metastore.uris\u0026#39; in HiveConf  -- \u0026#39;hive-conf-dir\u0026#39; = \u0026#39;...\u0026#39;, this is recommended in the kerberos environment  -- \u0026#39;hadoop-conf-dir\u0026#39; = \u0026#39;...\u0026#39;, this is recommended in the kerberos environment  -- \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs:///path/to/table/store/warehouse\u0026#39;, default use \u0026#39;hive.metastore.warehouse.dir\u0026#39; in HiveConf ); USE CATALOG my_hive; CREATE TABLE mydb.t ( pk INT, col1 STRING, col2 STRING ) WITH ( \u0026#39;bucket\u0026#39; = \u0026#39;-1\u0026#39;, \u0026#39;metastore.tag-to-partition\u0026#39; = \u0026#39;dt\u0026#39; ); INSERT INTO t VALUES (1, \u0026#39;10\u0026#39;, \u0026#39;100\u0026#39;), (2, \u0026#39;20\u0026#39;, \u0026#39;200\u0026#39;); -- create tag \u0026#39;2023-10-16\u0026#39; for snapshot 1 CALL sys.create_tag(\u0026#39;mydb.t\u0026#39;, \u0026#39;2023-10-16\u0026#39;, 1);  Step 2: Query table in Hive with Partition Pruning\nHive SHOW PARTITIONS t; /* OK dt=2023-10-16 */ SELECT * FROM t WHERE dt=\u0026#39;2023-10-16\u0026#39;; /* OK 1 10 100 2023-10-16 2 20 200 2023-10-16 */  Example for Tag Preview #  The above example can only query tags that have already been created, but Paimon is a real-time data lake, and you also need to query the latest data. Therefore, Paimon provides a preview feature:\nStep 1: Create table and tag in Flink SQL\nFlink CREATE CATALOG my_hive WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;hive\u0026#39;, -- \u0026#39;uri\u0026#39; = \u0026#39;thrift://\u0026lt;hive-metastore-host-name\u0026gt;:\u0026lt;port\u0026gt;\u0026#39;, default use \u0026#39;hive.metastore.uris\u0026#39; in HiveConf  -- \u0026#39;hive-conf-dir\u0026#39; = \u0026#39;...\u0026#39;, this is recommended in the kerberos environment  -- \u0026#39;hadoop-conf-dir\u0026#39; = \u0026#39;...\u0026#39;, this is recommended in the kerberos environment  -- \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs:///path/to/table/store/warehouse\u0026#39;, default use \u0026#39;hive.metastore.warehouse.dir\u0026#39; in HiveConf ); USE CATALOG my_hive; CREATE TABLE mydb.t ( pk INT, col1 STRING, col2 STRING ) WITH ( \u0026#39;bucket\u0026#39; = \u0026#39;-1\u0026#39;, \u0026#39;metastore.tag-to-partition\u0026#39; = \u0026#39;dt\u0026#39;, -- preview tag creation mode process-time  -- paimon will create partitions early based on process-time  \u0026#39;metastore.tag-to-partition.preview\u0026#39; = \u0026#39;process-time\u0026#39; ); INSERT INTO t VALUES (1, \u0026#39;10\u0026#39;, \u0026#39;100\u0026#39;), (2, \u0026#39;20\u0026#39;, \u0026#39;200\u0026#39;); -- create tag \u0026#39;2023-10-16\u0026#39; for snapshot 1 CALL sys.create_tag(\u0026#39;mydb.t\u0026#39;, \u0026#39;2023-10-16\u0026#39;, 1); -- new data in \u0026#39;2023-10-17\u0026#39; INSERT INTO t VALUES (3, \u0026#39;30\u0026#39;, \u0026#39;300\u0026#39;), (4, \u0026#39;40\u0026#39;, \u0026#39;400\u0026#39;); -- haven\u0026#39;t finished writing the data for \u0026#39;2023-10-17\u0026#39; yet, so there\u0026#39;s no need to create a tag for now -- but the data is already visible for Hive  Step 2: Query table in Hive with Partition Pruning\nHive SHOW PARTITIONS t; /* OK dt=2023-10-16 dt=2023-10-17 */ SELECT * FROM t WHERE dt=\u0026#39;2023-10-17\u0026#39;; -- preview tag \u0026#39;2023-10-17\u0026#39; /* OK 1 10 100 2023-10-17 2 20 200 2023-10-17 3 30 300 2023-10-17 4 40 400 2023-10-17 */  "});index.add({'id':17,'href':'/docs/0.9/concepts/basic-concepts/','title':"Basic Concepts",'section':"Concepts",'content':"Basic Concepts #  File Layouts #  All files of a table are stored under one base directory. Paimon files are organized in a layered style. The following image illustrates the file layout. Starting from a snapshot file, Paimon readers can recursively access all records from the table.\nSnapshot #  All snapshot files are stored in the snapshot directory.\nA snapshot file is a JSON file containing information about this snapshot, including\n the schema file in use the manifest list containing all changes of this snapshot  A snapshot captures the state of a table at some point in time. Users can access the latest data of a table through the latest snapshot. By time traveling, users can also access the previous state of a table through an earlier snapshot.\nManifest Files #  All manifest lists and manifest files are stored in the manifest directory.\nA manifest list is a list of manifest file names.\nA manifest file is a file containing changes about LSM data files and changelog files. For example, which LSM data file is created and which file is deleted in the corresponding snapshot.\nData Files #  Data files are grouped by partitions. Currently, Paimon supports using parquet (default), orc and avro as data file\u0026rsquo;s format.\nPartition #  Paimon adopts the same partitioning concept as Apache Hive to separate data.\nPartitioning is an optional way of dividing a table into related parts based on the values of particular columns like date, city, and department. Each table can have one or more partition keys to identify a particular partition.\nBy partitioning, users can efficiently operate on a slice of records in the table.\nConsistency Guarantees #  Paimon writers use two-phase commit protocol to atomically commit a batch of records to the table. Each commit produces at most two snapshots at commit time. It depends on the incremental write and compaction strategy. If only incremental writes are performed without triggering a compaction operation, only an incremental snapshot will be created. If a compaction operation is triggered, an incremental snapshot and a compacted snapshot will be created.\nFor any two writers modifying a table at the same time, as long as they do not modify the same partition, their commits can occur in parallel. If they modify the same partition, only snapshot isolation is guaranteed. That is, the final table state may be a mix of the two commits, but no changes are lost. See dedicated compaction job for more info.\n"});index.add({'id':18,'href':'/docs/0.9/primary-key-table/data-distribution/','title':"Data Distribution",'section':"Table with PK",'content':"Data Distribution #  A bucket is the smallest storage unit for reads and writes, each bucket directory contains an LSM tree.\nFixed Bucket #  Configure a bucket greater than 0, using Fixed Bucket mode, according to Math.abs(key_hashcode % numBuckets) to compute the bucket of record.\nRescaling buckets can only be done through offline processes, see Rescale Bucket. A too large number of buckets leads to too many small files, and a too small number of buckets leads to poor write performance.\nDynamic Bucket #  Default mode for primary key table, or configure 'bucket' = '-1'.\nThe keys that arrive first will fall into the old buckets, and the new keys will fall into the new buckets, the distribution of buckets and keys depends on the order in which the data arrives. Paimon maintains an index to determine which key corresponds to which bucket.\nPaimon will automatically expand the number of buckets.\n Option1: 'dynamic-bucket.target-row-num': controls the target row number for one bucket. Option2: 'dynamic-bucket.initial-buckets': controls the number of initialized bucket.  Dynamic Bucket only support single write job. Please do not start multiple jobs to write to the same partition (this can lead to duplicate data). Even if you enable 'write-only' and start a dedicated compaction job, it won\u0026rsquo;t work.  Normal Dynamic Bucket Mode #  When your updates do not cross partitions (no partitions, or primary keys contain all partition fields), Dynamic Bucket mode uses HASH index to maintain mapping from key to bucket, it requires more memory than fixed bucket mode.\nPerformance:\n Generally speaking, there is no performance loss, but there will be some additional memory consumption, 100 million entries in a partition takes up 1 GB more memory, partitions that are no longer active do not take up memory. For tables with low update rates, this mode is recommended to significantly improve performance.  Normal Dynamic Bucket Mode supports sort-compact to speed up queries. See Sort Compact.\nCross Partitions Upsert Dynamic Bucket Mode #  When you need cross partition upsert (primary keys not contain all partition fields), Dynamic Bucket mode directly maintains the mapping of keys to partition and bucket, uses local disks, and initializes indexes by reading all existing keys in the table when starting stream write job. Different merge engines have different behaviors:\n Deduplicate: Delete data from the old partition and insert new data into the new partition. PartialUpdate \u0026amp; Aggregation: Insert new data into the old partition. FirstRow: Ignore new data if there is old value.  Performance: For tables with a large amount of data, there will be a significant loss in performance. Moreover, initialization takes a long time.\nIf your upsert does not rely on too old data, you can consider configuring index TTL to reduce Index and initialization time:\n 'cross-partition-upsert.index-ttl': The TTL in rocksdb index and initialization, this can avoid maintaining too many indexes and lead to worse and worse performance.  But please note that this may also cause data duplication.\nPick Partition Fields #  The following three types of fields may be defined as partition fields in the warehouse:\n Creation Time (Recommended): The creation time is generally immutable, so you can confidently treat it as a partition field and add it to the primary key. Event Time: Event time is a field in the original table. For CDC data, such as tables synchronized from MySQL CDC or Changelogs generated by Paimon, they are all complete CDC data, including UPDATE_BEFORE records, even if you declare the primary key containing partition field, you can achieve the unique effect (require 'changelog-producer'='input'). CDC op_ts: It cannot be defined as a partition field, unable to know previous record timestamp. So you need to use cross partition upsert, it will consume more resources.  "});index.add({'id':19,'href':'/docs/0.9/project/download/','title':"Download",'section':"Project",'content':"Download #  This documentation is a guide for downloading Paimon Jars.\nEngine Jars #     Version Jar     Flink 1.20 paimon-flink-1.20-0.9.0.jar   Flink 1.19 paimon-flink-1.19-0.9.0.jar   Flink 1.18 paimon-flink-1.18-0.9.0.jar   Flink 1.17 paimon-flink-1.17-0.9.0.jar   Flink 1.16 paimon-flink-1.16-0.9.0.jar   Flink 1.15 paimon-flink-1.15-0.9.0.jar   Flink Action paimon-flink-action-0.9.0.jar   Spark 3.5 paimon-spark-3.5-0.9.0.jar   Spark 3.4 paimon-spark-3.4-0.9.0.jar   Spark 3.3 paimon-spark-3.3-0.9.0.jar   Spark 3.2 paimon-spark-3.2-0.9.0.jar   Spark 3.1 paimon-spark-3.1-0.9.0.jar   Hive 3.1 paimon-hive-connector-3.1-0.9.0.jar   Hive 2.3 paimon-hive-connector-2.3-0.9.0.jar   Hive 2.2 paimon-hive-connector-2.2-0.9.0.jar   Hive 2.1 paimon-hive-connector-2.1-0.9.0.jar   Hive 2.1-cdh-6.3 paimon-hive-connector-2.1-cdh-6.3-0.9.0.jar   Presto Download from master   Trino Download from master    Filesystem Jars #     Version Jar     paimon-oss paimon-oss-0.9.0.jar   paimon-s3 paimon-s3-0.9.0.jar    API Jars #     Version Jar     paimon-bundle paimon-bundle-0.9.0.jar    "});index.add({'id':20,'href':'/docs/0.9/program-api/flink-api/','title':"Flink API",'section':"Program API",'content':"Flink API #  We do not recommend using programming API. Paimon is designed for SQL first, unless you are a professional Flink developer, even if you do, it can be very difficult.\nWe strongly recommend that you use Flink SQL or Spark SQL, or simply use SQL APIs in programs.\nThe following documents are not detailed and are for reference only.\n Dependency #  Maven dependency:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.paimon\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;paimon-flink-1.20\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.9.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-table-api-java-bridge\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.20.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; Or download the jar file: Paimon Flink. Please choose your Flink version.\nPaimon relies on Hadoop environment, you should add hadoop classpath or bundled jar.\nNot only DataStream API, you can also read or write to Paimon tables by the conversion between DataStream and Table in Flink. See DataStream API Integration.\nWrite to Table #  import org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; import org.apache.paimon.flink.FlinkCatalogFactory; import org.apache.paimon.flink.sink.FlinkSinkBuilder; import org.apache.paimon.options.Options; import org.apache.paimon.table.Table; import org.apache.flink.api.common.typeinfo.Types; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.table.api.DataTypes; import org.apache.flink.table.types.DataType; import org.apache.flink.types.Row; import org.apache.flink.types.RowKind; public class WriteToTable { public static void writeTo() throws Exception { // create environments of both APIs  StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // for CONTINUOUS_UNBOUNDED source, set checkpoint interval  // env.enableCheckpointing(60_000);  // create a changelog DataStream  DataStream\u0026lt;Row\u0026gt; input = env.fromElements( Row.ofKind(RowKind.INSERT, \u0026#34;Alice\u0026#34;, 12), Row.ofKind(RowKind.INSERT, \u0026#34;Bob\u0026#34;, 5), Row.ofKind(RowKind.UPDATE_BEFORE, \u0026#34;Alice\u0026#34;, 12), Row.ofKind(RowKind.UPDATE_AFTER, \u0026#34;Alice\u0026#34;, 100)) .returns( Types.ROW_NAMED( new String[] {\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;}, Types.STRING, Types.INT)); // get table from catalog  Options catalogOptions = new Options(); catalogOptions.set(\u0026#34;warehouse\u0026#34;, \u0026#34;/path/to/warehouse\u0026#34;); Catalog catalog = FlinkCatalogFactory.createPaimonCatalog(catalogOptions); Table table = catalog.getTable(Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;T\u0026#34;)); DataType inputType = DataTypes.ROW( DataTypes.FIELD(\u0026#34;name\u0026#34;, DataTypes.STRING()), DataTypes.FIELD(\u0026#34;age\u0026#34;, DataTypes.INT())); FlinkSinkBuilder builder = new FlinkSinkBuilder(table).forRow(input, inputType); // set sink parallelism  // builder.parallelism(_your_parallelism)  // set overwrite mode  // builder.overwrite(...)  builder.build(); env.execute(); } } Read from Table #  import org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; import org.apache.paimon.flink.FlinkCatalogFactory; import org.apache.paimon.flink.source.FlinkSourceBuilder; import org.apache.paimon.options.Options; import org.apache.paimon.table.Table; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.types.Row; public class ReadFromTable { public static void readFrom() throws Exception { // create environments of both APIs  StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // get table from catalog  Options catalogOptions = new Options(); catalogOptions.set(\u0026#34;warehouse\u0026#34;, \u0026#34;/path/to/warehouse\u0026#34;); Catalog catalog = FlinkCatalogFactory.createPaimonCatalog(catalogOptions); Table table = catalog.getTable(Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;T\u0026#34;)); // table = table.copy(Collections.singletonMap(\u0026#34;scan.file-creation-time-millis\u0026#34;, \u0026#34;...\u0026#34;));  FlinkSourceBuilder builder = new FlinkSourceBuilder(table).env(env); // builder.sourceBounded(true);  // builder.projection(...);  // builder.predicate(...);  // builder.limit(...);  // builder.sourceParallelism(...);  DataStream\u0026lt;Row\u0026gt; dataStream = builder.buildForRow(); // use this datastream  dataStream.executeAndCollect().forEachRemaining(System.out::println); // prints:  // +I[Bob, 12]  // +I[Alice, 12]  // -U[Alice, 12]  // +U[Alice, 14]  } } Cdc ingestion Table #  Paimon supports ingest data into Paimon tables with schema evolution.\n You can use Java API to write cdc records into Paimon Tables. You can write records to Paimon\u0026rsquo;s partial-update table with adding columns dynamically.  Here is an example to use RichCdcSinkBuilder API:\nimport org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.CatalogContext; import org.apache.paimon.catalog.CatalogFactory; import org.apache.paimon.flink.FlinkCatalogFactory; import org.apache.paimon.catalog.Identifier; import org.apache.paimon.flink.sink.cdc.RichCdcRecord; import org.apache.paimon.flink.sink.cdc.RichCdcSinkBuilder; import org.apache.paimon.fs.Path; import org.apache.paimon.options.Options; import org.apache.paimon.schema.Schema; import org.apache.paimon.table.Table; import org.apache.paimon.types.DataTypes; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import static org.apache.paimon.types.RowKind.INSERT; public class WriteCdcToTable { public static void writeTo() throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // for CONTINUOUS_UNBOUNDED source, set checkpoint interval  // env.enableCheckpointing(60_000);  DataStream\u0026lt;RichCdcRecord\u0026gt; dataStream = env.fromElements( RichCdcRecord.builder(INSERT) .field(\u0026#34;order_id\u0026#34;, DataTypes.BIGINT(), \u0026#34;123\u0026#34;) .field(\u0026#34;price\u0026#34;, DataTypes.DOUBLE(), \u0026#34;62.2\u0026#34;) .build(), // dt field will be added with schema evolution  RichCdcRecord.builder(INSERT) .field(\u0026#34;order_id\u0026#34;, DataTypes.BIGINT(), \u0026#34;245\u0026#34;) .field(\u0026#34;price\u0026#34;, DataTypes.DOUBLE(), \u0026#34;82.1\u0026#34;) .field(\u0026#34;dt\u0026#34;, DataTypes.TIMESTAMP(), \u0026#34;2023-06-12 20:21:12\u0026#34;) .build()); Identifier identifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;T\u0026#34;); Options catalogOptions = new Options(); catalogOptions.set(\u0026#34;warehouse\u0026#34;, \u0026#34;/path/to/warehouse\u0026#34;); Catalog.Loader catalogLoader = () -\u0026gt; FlinkCatalogFactory.createPaimonCatalog(catalogOptions); Table table = catalogLoader.load().getTable(identifier); new RichCdcSinkBuilder(table) .forRichCdcRecord(dataStream) .identifier(identifier) .catalogLoader(catalogLoader) .build(); env.execute(); } } "});index.add({'id':21,'href':'/docs/0.9/filesystems/hdfs/','title':"HDFS",'section':"Filesystems",'content':"HDFS #  You don\u0026rsquo;t need any additional dependencies to access HDFS because you have already taken care of the Hadoop dependencies.\nHDFS Configuration #  For HDFS, the most important thing is to be able to read your HDFS configuration.\nFlink/Trino/JavaAPI You may not have to do anything, if you are in a hadoop environment. Otherwise pick one of the following ways to configure your HDFS:\n Set environment variable HADOOP_HOME or HADOOP_CONF_DIR. Configure 'hadoop-conf-dir' in the paimon catalog. Configure Hadoop options through prefix 'hadoop.' in the paimon catalog.  The first approach is recommended.\nIf you do not want to include the value of the environment variable, you can configure hadoop-conf-loader to option.\nHive/Spark HDFS Configuration is available directly through the computation cluster, see cluster configuration of Hive and Spark for details. Hadoop-compatible file systems (HCFS) #  All Hadoop file systems are automatically available when the Hadoop libraries are on the classpath.\nThis way, Paimon seamlessly supports all of Hadoop file systems implementing the org.apache.hadoop.fs.FileSystem interface, and all Hadoop-compatible file systems (HCFS).\n HDFS Alluxio (see configuration specifics below) XtreemFS …  The Hadoop configuration has to have an entry for the required file system implementation in the core-site.xml file.\nFor Alluxio support add the following entry into the core-site.xml file:\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt;fs.alluxio.impl\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;alluxio.hadoop.FileSystem\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; Kerberos #  Flink It is recommended to use Flink Kerberos Keytab.Spark It is recommended to use Spark Kerberos Keytab.Hive An intuitive approach is to configure Hive\u0026rsquo;s kerberos authentication.Trino/JavaAPI Configure the following three options in your catalog configuration:\n security.kerberos.login.keytab: Absolute path to a Kerberos keytab file that contains the user credentials. Please make sure it is copied to each machine. security.kerberos.login.principal: Kerberos principal name associated with the keytab. security.kerberos.login.use-ticket-cache: True or false, indicates whether to read from your Kerberos ticket cache.  For JavaAPI:\nSecurityContext.install(catalogOptions);  HDFS HA #  Ensure that hdfs-site.xml and core-site.xml contain the necessary HA configuration.\nHDFS ViewFS #  Ensure that hdfs-site.xml and core-site.xml contain the necessary ViewFs configuration.\n"});index.add({'id':22,'href':'/docs/0.9/flink/cdc-ingestion/mysql-cdc/','title':"Mysql CDC",'section':"CDC Ingestion",'content':"MySQL CDC #  Paimon supports synchronizing changes from different databases using change data capture (CDC). This feature requires Flink and its CDC connectors.\nPrepare CDC Bundled Jar #  Download CDC Bundled Jar and put them under \u0026lt;FLINK_HOME\u0026gt;/lib/.\n   Version Bundled Jar     2.3.x Only supported in versions below 0.8.2 flink-sql-connector-mysql-cdc-2.3.x.jar   2.4.x Only supported in versions below 0.8.2 flink-sql-connector-mysql-cdc-2.4.x.jar   3.0.x Only supported in versions below 0.8.2 flink-sql-connector-mysql-cdc-3.0.x.jar  flink-cdc-common-3.0.x.jar   3.1.x  flink-sql-connector-mysql-cdc-3.1.x.jar  mysql-connector-java-8.0.27.jar    Synchronizing Tables #  By using MySqlSyncTableAction in a Flink DataStream job or directly through flink run, users can synchronize one or multiple tables from MySQL into one Paimon table.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  mysql_sync_table --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  [--partition_keys \u0026lt;partition_keys\u0026gt;] \\  [--primary_keys \u0026lt;primary-keys\u0026gt;] \\  [--type_mapping \u0026lt;option1,option2...\u0026gt;] \\  [--computed_column \u0026lt;\u0026#39;column-name=expr-name(args[, ...])\u0026#39;\u0026gt; [--computed_column ...]] \\  [--metadata_column \u0026lt;metadata-column\u0026gt;] \\  [--mysql_conf \u0026lt;mysql-cdc-source-conf\u0026gt; [--mysql_conf \u0026lt;mysql-cdc-source-conf\u0026gt; ...]] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; ...]]    Configuration Description     --warehouse The path to Paimon warehouse.   --database The database name in Paimon catalog.   --table The Paimon table name.   --partition_keys The partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example \"dt,hh,mm\".   --primary_keys The primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example \"buyer_id,seller_id\".   --type_mapping It is used to specify how to map MySQL data type to Paimon type.\nSupported options:  \"tinyint1-not-bool\": maps MySQL TINYINT(1) to TINYINT instead of BOOLEAN. \"to-nullable\": ignores all NOT NULL constraints (except for primary keys). This is used to solve the problem that Flink cannot accept the MySQL 'ALTER TABLE ADD COLUMN column type NOT NULL DEFAULT x' operation.  \"to-string\": maps all MySQL types to STRING. \"char-to-string\": maps MySQL CHAR(length)/VARCHAR(length) types to STRING. \"longtext-to-bytes\": maps MySQL LONGTEXT types to BYTES. \"bigint-unsigned-to-bigint\": maps MySQL BIGINT UNSIGNED, BIGINT UNSIGNED ZEROFILL, SERIAL to BIGINT. You should ensure overflow won't occur when using this option.     --computed_column The definitions of computed columns. The argument field is from MySQL table field name. See here for a complete list of configurations.    --metadata_column --metadata_column is used to specify which metadata columns to include in the output schema of the connector. Metadata columns provide additional information related to the source data, for example: --metadata_column table_name,database_name,op_ts. See its document for a complete list of available metadata.   --mysql_conf The configuration for Flink CDC MySQL sources. Each configuration should be specified in the format \"key=value\". hostname, username, password, database-name and table-name are required configurations, others are optional. See its document for a complete list of configurations.   --catalog_conf The configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.   --table_conf The configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.    If the Paimon table you specify does not exist, this action will automatically create the table. Its schema will be derived from all specified MySQL tables. If the Paimon table already exists, its schema will be compared against the schema of all specified MySQL tables.\nExample 1: synchronize tables into one Paimon table\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  mysql_sync_table \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --partition_keys pt \\  --primary_keys pt,uid \\  --computed_column \u0026#39;_year=year(age)\u0026#39; \\  --mysql_conf hostname=127.0.0.1 \\  --mysql_conf username=root \\  --mysql_conf password=123456 \\  --mysql_conf database-name=\u0026#39;source_db\u0026#39; \\  --mysql_conf table-name=\u0026#39;source_table1|source_table2\u0026#39; \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --table_conf changelog-producer=input \\  --table_conf sink.parallelism=4 As example shows, the mysql_conf\u0026rsquo;s table-name supports regular expressions to monitor multiple tables that satisfy the regular expressions. The schemas of all the tables will be merged into one Paimon table schema.\nExample 2: synchronize shards into one Paimon table\nYou can also set \u0026lsquo;database-name\u0026rsquo; with a regular expression to capture multiple databases. A typical scenario is that a table \u0026lsquo;source_table\u0026rsquo; is split into database \u0026lsquo;source_db1\u0026rsquo;, \u0026lsquo;source_db2\u0026rsquo; \u0026hellip;, then you can synchronize data of all the \u0026lsquo;source_table\u0026rsquo;s into one Paimon table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  mysql_sync_table \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --partition_keys pt \\  --primary_keys pt,uid \\  --computed_column \u0026#39;_year=year(age)\u0026#39; \\  --mysql_conf hostname=127.0.0.1 \\  --mysql_conf username=root \\  --mysql_conf password=123456 \\  --mysql_conf database-name=\u0026#39;source_db.+\u0026#39; \\  --mysql_conf table-name=\u0026#39;source_table\u0026#39; \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --table_conf changelog-producer=input \\  --table_conf sink.parallelism=4 Synchronizing Databases #  By using MySqlSyncDatabaseAction in a Flink DataStream job or directly through flink run, users can synchronize the whole MySQL database into one Paimon database.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  mysql_sync_database --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  [--ignore_incompatible \u0026lt;true/false\u0026gt;] \\  [--merge_shards \u0026lt;true/false\u0026gt;] \\  [--table_prefix \u0026lt;paimon-table-prefix\u0026gt;] \\  [--table_suffix \u0026lt;paimon-table-suffix\u0026gt;] \\  [--including_tables \u0026lt;mysql-table-name|name-regular-expr\u0026gt;] \\  [--excluding_tables \u0026lt;mysql-table-name|name-regular-expr\u0026gt;] \\  [--mode \u0026lt;sync-mode\u0026gt;] \\  [--metadata_column \u0026lt;metadata-column\u0026gt;] \\  [--type_mapping \u0026lt;option1,option2...\u0026gt;] \\  [--partition_keys \u0026lt;partition_keys\u0026gt;] \\  [--primary_keys \u0026lt;primary-keys\u0026gt;] \\  [--mysql_conf \u0026lt;mysql-cdc-source-conf\u0026gt; [--mysql_conf \u0026lt;mysql-cdc-source-conf\u0026gt; ...]] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; ...]]    Configuration Description     --warehouse The path to Paimon warehouse.   --database The database name in Paimon catalog.   --ignore_incompatible It is default false, in this case, if MySQL table name exists in Paimon and their schema is incompatible,an exception will be thrown. You can specify it to true explicitly to ignore the incompatible tables and exception.   --merge_shards It is default true, in this case, if some tables in different databases have the same name, their schemas will be merged and their records will be synchronized into one Paimon table. Otherwise, each table's records will be synchronized to a corresponding Paimon table, and the Paimon table will be named to 'databaseName_tableName' to avoid potential name conflict.   --table_prefix The prefix of all Paimon tables to be synchronized. For example, if you want all synchronized tables to have \"ods_\" as prefix, you can specify \"--table_prefix ods_\".   --table_suffix The suffix of all Paimon tables to be synchronized. The usage is same as \"--table_prefix\".   --including_tables It is used to specify which source tables are to be synchronized. You must use '|' to separate multiple tables.Because '|' is a special character, a comma is required, for example: 'a|b|c'.Regular expression is supported, for example, specifying \"--including_tables test|paimon.*\" means to synchronize table 'test' and all tables start with 'paimon'.   --excluding_tables It is used to specify which source tables are not to be synchronized. The usage is same as \"--including_tables\". \"--excluding_tables\" has higher priority than \"--including_tables\" if you specified both.   --mode It is used to specify synchronization mode.\nPossible values:\"divided\" (the default mode if you haven't specified one): start a sink for each table, the synchronization of the new table requires restarting the job.\"combined\": start a single combined sink for all tables, the new table will be automatically synchronized.   --metadata_column --metadata_column is used to specify which metadata columns to include in the output schema of the connector. Metadata columns provide additional information related to the source data, for example: --metadata_column table_name,database_name,op_ts. See its document for a complete list of available metadata.   --type_mapping It is used to specify how to map MySQL data type to Paimon type.\nSupported options:  \"tinyint1-not-bool\": maps MySQL TINYINT(1) to TINYINT instead of BOOLEAN. \"to-nullable\": ignores all NOT NULL constraints (except for primary keys). This is used to solve the problem that Flink cannot accept the MySQL 'ALTER TABLE ADD COLUMN column type NOT NULL DEFAULT x' operation.  \"to-string\": maps all MySQL types to STRING. \"char-to-string\": maps MySQL CHAR(length)/VARCHAR(length) types to STRING. \"longtext-to-bytes\": maps MySQL LONGTEXT types to BYTES. \"bigint-unsigned-to-bigint\": maps MySQL BIGINT UNSIGNED, BIGINT UNSIGNED ZEROFILL, SERIAL to BIGINT. You should ensure overflow won't occur when using this option.     --partition_keys The partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example \"dt,hh,mm\". If the keys are not in source table, the sink table won't set partition keys.   --primary_keys The primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example \"buyer_id,seller_id\". If the keys are not in source table, but the source table has primary keys, the sink table will use source table's primary keys. Otherwise, the sink table won't set primary keys.   --mysql_conf The configuration for Flink CDC MySQL sources. Each configuration should be specified in the format \"key=value\". hostname, username, password, database-name and table-name are required configurations, others are optional. See its document for a complete list of configurations.   --catalog_conf The configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.   --table_conf The configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.    Only tables with primary keys will be synchronized.\nFor each MySQL table to be synchronized, if the corresponding Paimon table does not exist, this action will automatically create the table. Its schema will be derived from all specified MySQL tables. If the Paimon table already exists, its schema will be compared against the schema of all specified MySQL tables.\nExample 1: synchronize entire database\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  mysql_sync_database \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --mysql_conf hostname=127.0.0.1 \\  --mysql_conf username=root \\  --mysql_conf password=123456 \\  --mysql_conf database-name=source_db \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --table_conf changelog-producer=input \\  --table_conf sink.parallelism=4 Example 2: synchronize newly added tables under database\nLet\u0026rsquo;s say at first a Flink job is synchronizing tables [product, user, address] under database source_db. The command to submit the job looks like:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  mysql_sync_database \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --mysql_conf hostname=127.0.0.1 \\  --mysql_conf username=root \\  --mysql_conf password=123456 \\  --mysql_conf database-name=source_db \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --table_conf changelog-producer=input \\  --table_conf sink.parallelism=4 \\  --including_tables \u0026#39;product|user|address\u0026#39; At a later point we would like the job to also synchronize tables [order, custom], which contains history data. We can achieve this by recovering from the previous snapshot of the job and thus reusing existing state of the job. The recovered job will first snapshot newly added tables, and then continue reading changelog from previous position automatically.\nThe command to recover from previous snapshot and add new tables to synchronize looks like:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  --fromSavepoint savepointPath \\  /path/to/paimon-flink-action-0.9.0.jar \\  mysql_sync_database \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --mysql_conf hostname=127.0.0.1 \\  --mysql_conf username=root \\  --mysql_conf password=123456 \\  --mysql_conf database-name=source_db \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --including_tables \u0026#39;product|user|address|order|custom\u0026#39; You can set --mode combined to enable synchronizing newly added tables without restarting job.  Example 3: synchronize and merge multiple shards\nLet\u0026rsquo;s say you have multiple database shards db1, db2, \u0026hellip; and each database has tables tbl1, tbl2, \u0026hellip;. You can synchronize all the db.+.tbl.+ into tables test_db.tbl1, test_db.tbl2 \u0026hellip; by following command:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  mysql_sync_database \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --mysql_conf hostname=127.0.0.1 \\  --mysql_conf username=root \\  --mysql_conf password=123456 \\  --mysql_conf database-name=\u0026#39;db.+\u0026#39; \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --table_conf changelog-producer=input \\  --table_conf sink.parallelism=4 \\  --including_tables \u0026#39;tbl.+\u0026#39; By setting database-name to a regular expression, the synchronization job will capture all tables under matched databases and merge tables of the same name into one table.\nYou can set --merge_shards false to prevent merging shards. The synchronized tables will be named to \u0026lsquo;databaseName_tableName\u0026rsquo; to avoid potential name conflict.  FAQ #   Chinese characters in records ingested from MySQL are garbled.   Try to set env.java.opts: -Dfile.encoding=UTF-8 in flink-conf.yaml (the option is changed to env.java.opts.all since Flink-1.17).  "});index.add({'id':23,'href':'/docs/0.9/primary-key-table/merge-engine/partial-update/','title':"Partial Update",'section':"Merge Engine",'content':"Partial Update #  By specifying 'merge-engine' = 'partial-update', users have the ability to update columns of a record through multiple updates until the record is complete. This is achieved by updating the value fields one by one, using the latest data under the same primary key. However, null values are not overwritten in the process.\nFor example, suppose Paimon receives three records:\n \u0026lt;1, 23.0, 10, NULL\u0026gt;- \u0026lt;1, NULL, NULL, 'This is a book'\u0026gt; \u0026lt;1, 25.2, NULL, NULL\u0026gt;  Assuming that the first column is the primary key, the final result would be \u0026lt;1, 25.2, 10, 'This is a book'\u0026gt;.\nFor streaming queries, partial-update merge engine must be used together with lookup or full-compaction changelog producer. (\u0026lsquo;input\u0026rsquo; changelog producer is also supported, but only returns input records.)  By default, Partial update can not accept delete records, you can choose one of the following solutions:\n Configure \u0026lsquo;ignore-delete\u0026rsquo; to ignore delete records. Configure \u0026lsquo;partial-update.remove-record-on-delete\u0026rsquo; to remove the whole row when receiving delete records. Configure \u0026lsquo;sequence-group\u0026rsquo;s to retract partial columns.   Sequence Group #  A sequence-field may not solve the disorder problem of partial-update tables with multiple stream updates, because the sequence-field may be overwritten by the latest data of another stream during multi-stream update.\nSo we introduce sequence group mechanism for partial-update tables. It can solve:\n Disorder during multi-stream update. Each stream defines its own sequence-groups. A true partial-update, not just a non-null update.  See example:\nCREATE TABLE t ( k INT, a INT, b INT, g_1 INT, c INT, d INT, g_2 INT, PRIMARY KEY (k) NOT ENFORCED ) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;partial-update\u0026#39;, \u0026#39;fields.g_1.sequence-group\u0026#39; = \u0026#39;a,b\u0026#39;, \u0026#39;fields.g_2.sequence-group\u0026#39; = \u0026#39;c,d\u0026#39; ); INSERT INTO t VALUES (1, 1, 1, 1, 1, 1, 1); -- g_2 is null, c, d should not be updated INSERT INTO t VALUES (1, 2, 2, 2, 2, 2, CAST(NULL AS INT)); SELECT * FROM t; -- output 1, 2, 2, 2, 1, 1, 1  -- g_1 is smaller, a, b should not be updated INSERT INTO t VALUES (1, 3, 3, 1, 3, 3, 3); SELECT * FROM t; -- output 1, 2, 2, 2, 3, 3, 3 For fields.\u0026lt;field-name\u0026gt;.sequence-group, valid comparative data types include: DECIMAL, TINYINT, SMALLINT, INTEGER, BIGINT, FLOAT, DOUBLE, DATE, TIME, TIMESTAMP, and TIMESTAMP_LTZ.\nYou can also configure multiple sorted fields in a sequence-group, like fields.\u0026lt;field-name1\u0026gt;,\u0026lt;field-name2\u0026gt;.sequence-group, multiple fields will be compared in order.\nSee example:\nCREATE TABLE SG ( k INT, a INT, b INT, g_1 INT, c INT, d INT, g_2 INT, g_3 INT, PRIMARY KEY (k) NOT ENFORCED ) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;partial-update\u0026#39;, \u0026#39;fields.g_1.sequence-group\u0026#39; = \u0026#39;a,b\u0026#39;, \u0026#39;fields.g_2,g_3.sequence-group\u0026#39; = \u0026#39;c,d\u0026#39; ); INSERT INTO SG VALUES (1, 1, 1, 1, 1, 1, 1, 1); -- g_2, g_3 should not be updated INSERT INTO SG VALUES (1, 2, 2, 2, 2, 2, 1, CAST(NULL AS INT)); SELECT * FROM SG; -- output 1, 2, 2, 2, 1, 1, 1, 1  -- g_1 should not be updated INSERT INTO SG VALUES (1, 3, 3, 1, 3, 3, 3, 1); SELECT * FROM SG; -- output 1, 2, 2, 2, 3, 3, 3, 1 Aggregation For Partial Update #  You can specify aggregation function for the input field, all the functions in the Aggregation are supported.\nSee example:\nCREATE TABLE t ( k INT, a INT, b INT, c INT, d INT, PRIMARY KEY (k) NOT ENFORCED ) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;partial-update\u0026#39;, \u0026#39;fields.a.sequence-group\u0026#39; = \u0026#39;b\u0026#39;, \u0026#39;fields.b.aggregate-function\u0026#39; = \u0026#39;first_value\u0026#39;, \u0026#39;fields.c.sequence-group\u0026#39; = \u0026#39;d\u0026#39;, \u0026#39;fields.d.aggregate-function\u0026#39; = \u0026#39;sum\u0026#39; ); INSERT INTO t VALUES (1, 1, 1, CAST(NULL AS INT), CAST(NULL AS INT)); INSERT INTO t VALUES (1, CAST(NULL AS INT), CAST(NULL AS INT), 1, 1); INSERT INTO t VALUES (1, 2, 2, CAST(NULL AS INT), CAST(NULL AS INT)); INSERT INTO t VALUES (1, CAST(NULL AS INT), CAST(NULL AS INT), 2, 2); SELECT * FROM t; -- output 1, 2, 1, 2, 3 You can also configure an aggregation function for a sequence-group within multiple sorted fields.\nSee example:\nCREATE TABLE AGG ( k INT, a INT, b INT, g_1 INT, c VARCHAR, g_2 INT, g_3 INT, PRIMARY KEY (k) NOT ENFORCED ) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;partial-update\u0026#39;, \u0026#39;fields.a.aggregate-function\u0026#39; = \u0026#39;sum\u0026#39;, \u0026#39;fields.g_1,g_3.sequence-group\u0026#39; = \u0026#39;a\u0026#39;, \u0026#39;fields.g_2.sequence-group\u0026#39; = \u0026#39;c\u0026#39;); -- a in sequence-group g_1, g_3 with sum agg -- b not in sequence-group -- c in sequence-group g_2 without agg  INSERT INTO AGG VALUES (1, 1, 1, 1, \u0026#39;1\u0026#39;, 1, 1); -- g_2 should not be updated INSERT INTO AGG VALUES (1, 2, 2, 2, \u0026#39;2\u0026#39;, CAST(NULL AS INT), 2); SELECT * FROM AGG; -- output 1, 3, 2, 2, \u0026#34;1\u0026#34;, 1, 2  -- g_1, g_3 should not be updated INSERT INTO AGG VALUES (1, 3, 3, 2, \u0026#39;3\u0026#39;, 3, 1); SELECT * FROM AGG; -- output 1, 6, 3, 2, \u0026#34;3\u0026#34;, 3, 2 You can specify a default aggregation function for all the input fields with fields.default-aggregate-function, see example:\nCREATE TABLE t ( k INT, a INT, b INT, c INT, d INT, PRIMARY KEY (k) NOT ENFORCED ) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;partial-update\u0026#39;, \u0026#39;fields.a.sequence-group\u0026#39; = \u0026#39;b\u0026#39;, \u0026#39;fields.c.sequence-group\u0026#39; = \u0026#39;d\u0026#39;, \u0026#39;fields.default-aggregate-function\u0026#39; = \u0026#39;last_non_null_value\u0026#39;, \u0026#39;fields.d.aggregate-function\u0026#39; = \u0026#39;sum\u0026#39; ); INSERT INTO t VALUES (1, 1, 1, CAST(NULL AS INT), CAST(NULL AS INT)); INSERT INTO t VALUES (1, CAST(NULL AS INT), CAST(NULL AS INT), 1, 1); INSERT INTO t VALUES (1, 2, 2, CAST(NULL AS INT), CAST(NULL AS INT)); INSERT INTO t VALUES (1, CAST(NULL AS INT), CAST(NULL AS INT), 2, 2); SELECT * FROM t; -- output 1, 2, 2, 2, 3  "});index.add({'id':24,'href':'/docs/0.9/flink/cdc-ingestion/postgres-cdc/','title':"Postgres CDC",'section':"CDC Ingestion",'content':"Postgres CDC #  Paimon supports synchronizing changes from different databases using change data capture (CDC). This feature requires Flink and its CDC connectors.\nPrepare CDC Bundled Jar #  flink-connector-postgres-cdc-*.jar Synchronizing Tables #  By using PostgresSyncTableAction in a Flink DataStream job or directly through flink run, users can synchronize one or multiple tables from PostgreSQL into one Paimon table.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  postgres_sync_table --warehouse \u0026lt;warehouse_path\u0026gt; \\  --database \u0026lt;database_name\u0026gt; \\  --table \u0026lt;table_name\u0026gt; \\  [--partition_keys \u0026lt;partition_keys\u0026gt;] \\  [--primary_keys \u0026lt;primary_keys\u0026gt;] \\  [--type_mapping \u0026lt;option1,option2...\u0026gt;] \\  [--computed_column \u0026lt;\u0026#39;column-name=expr-name(args[, ...])\u0026#39;\u0026gt; [--computed_column ...]] \\  [--metadata_column \u0026lt;metadata_column\u0026gt;] \\  [--postgres_conf \u0026lt;postgres_cdc_source_conf\u0026gt; [--postgres_conf \u0026lt;postgres_cdc_source_conf\u0026gt; ...]] \\  [--catalog_conf \u0026lt;paimon_catalog_conf\u0026gt; [--catalog_conf \u0026lt;paimon_catalog_conf\u0026gt; ...]] \\  [--table_conf \u0026lt;paimon_table_sink_conf\u0026gt; [--table_conf \u0026lt;paimon_table_sink_conf\u0026gt; ...]]    Configuration Description     --warehouse The path to Paimon warehouse.   --database The database name in Paimon catalog.   --table The Paimon table name.   --partition_keys The partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example \"dt,hh,mm\".   --primary_keys The primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example \"buyer_id,seller_id\".   --type_mapping It is used to specify how to map PostgreSQL data type to Paimon type.\nSupported options:  \"to-string\": maps all PostgreSQL types to STRING.     --computed_column The definitions of computed columns. The argument field is from PostgreSQL table field name. See here for a complete list of configurations.    --metadata_column --metadata_column is used to specify which metadata columns to include in the output schema of the connector. Metadata columns provide additional information related to the source data, for example: --metadata_column table_name,database_name,schema_name,op_ts. See its document for a complete list of available metadata.   --postgres_conf The configuration for Flink CDC Postgres sources. Each configuration should be specified in the format \"key=value\". hostname, username, password, database-name, schema-name, table-name and slot.name are required configurations, others are optional. See its document for a complete list of configurations.   --catalog_conf The configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.   --table_conf The configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.    If the Paimon table you specify does not exist, this action will automatically create the table. Its schema will be derived from all specified PostgreSQL tables. If the Paimon table already exists, its schema will be compared against the schema of all specified PostgreSQL tables.\nExample 1: synchronize tables into one Paimon table\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  postgres_sync_table \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --partition_keys pt \\  --primary_keys pt,uid \\  --computed_column \u0026#39;_year=year(age)\u0026#39; \\  --postgres_conf hostname=127.0.0.1 \\  --postgres_conf username=root \\  --postgres_conf password=123456 \\  --postgres_conf database-name=\u0026#39;source_db\u0026#39; \\  --postgres_conf schema-name=\u0026#39;public\u0026#39; \\  --postgres_conf table-name=\u0026#39;source_table1|source_table2\u0026#39; \\  --postgres_conf slot.name=\u0026#39;paimon_cdc\u0026#39; \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --table_conf changelog-producer=input \\  --table_conf sink.parallelism=4 As example shows, the postgres_conf\u0026rsquo;s table-name supports regular expressions to monitor multiple tables that satisfy the regular expressions. The schemas of all the tables will be merged into one Paimon table schema.\nExample 2: synchronize shards into one Paimon table\nYou can also set \u0026lsquo;schema-name\u0026rsquo; with a regular expression to capture multiple schemas. A typical scenario is that a table \u0026lsquo;source_table\u0026rsquo; is split into schema \u0026lsquo;source_schema1\u0026rsquo;, \u0026lsquo;source_schema2\u0026rsquo; \u0026hellip;, then you can synchronize data of all the \u0026lsquo;source_table\u0026rsquo;s into one Paimon table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  postgres_sync_table \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --partition_keys pt \\  --primary_keys pt,uid \\  --computed_column \u0026#39;_year=year(age)\u0026#39; \\  --postgres_conf hostname=127.0.0.1 \\  --postgres_conf username=root \\  --postgres_conf password=123456 \\  --postgres_conf database-name=\u0026#39;source_db\u0026#39; \\  --postgres_conf schema-name=\u0026#39;source_schema.+\u0026#39; \\  --postgres_conf table-name=\u0026#39;source_table\u0026#39; \\  --postgres_conf slot.name=\u0026#39;paimon_cdc\u0026#39; \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --table_conf changelog-producer=input \\  --table_conf sink.parallelism=4 "});index.add({'id':25,'href':'/docs/0.9/concepts/spec/schema/','title':"Schema",'section':"Specification",'content':"Schema #  The version of the schema file starts from 0 and currently retains all versions of the schema. There may be old files that rely on the old schema version, so its deletion should be done with caution.\nSchema File is JSON, it includes:\n fields: data field list, data field contains id, name, type, field id is used to support schema evolution. partitionKeys: field name list, partition definition of the table, it cannot be modified. primaryKeys: field name list, primary key definition of the table, it cannot be modified. options: map\u0026lt;string, string\u0026gt;, no ordered, options of the table, including a lot of capabilities and optimizations.  Example #  { \u0026#34;version\u0026#34; : 3, \u0026#34;id\u0026#34; : 0, \u0026#34;fields\u0026#34; : [ { \u0026#34;id\u0026#34; : 0, \u0026#34;name\u0026#34; : \u0026#34;order_id\u0026#34;, \u0026#34;type\u0026#34; : \u0026#34;BIGINT NOT NULL\u0026#34; }, { \u0026#34;id\u0026#34; : 1, \u0026#34;name\u0026#34; : \u0026#34;order_name\u0026#34;, \u0026#34;type\u0026#34; : \u0026#34;STRING\u0026#34; }, { \u0026#34;id\u0026#34; : 2, \u0026#34;name\u0026#34; : \u0026#34;order_user_id\u0026#34;, \u0026#34;type\u0026#34; : \u0026#34;BIGINT\u0026#34; }, { \u0026#34;id\u0026#34; : 3, \u0026#34;name\u0026#34; : \u0026#34;order_shop_id\u0026#34;, \u0026#34;type\u0026#34; : \u0026#34;BIGINT\u0026#34; } ], \u0026#34;highestFieldId\u0026#34; : 3, \u0026#34;partitionKeys\u0026#34; : [ ], \u0026#34;primaryKeys\u0026#34; : [ \u0026#34;order_id\u0026#34; ], \u0026#34;options\u0026#34; : { \u0026#34;bucket\u0026#34; : \u0026#34;5\u0026#34; }, \u0026#34;comment\u0026#34; : \u0026#34;\u0026#34;, \u0026#34;timeMillis\u0026#34; : 1720496663041 } Compatibility #  For old versions:\n version 1: should put bucket -\u0026gt; 1 to options if there is no bucket key. version 1 \u0026amp; 2: should put file.format -\u0026gt; orc to options if there is no file.format key.  DataField #  DataField represents a column of the table.\n id: int, column id, automatic increment, it is used for schema evolution. name: string, column name. type: data type, it is very similar to SQL type string. description: string.  Update Schema #  Updating the schema should generate a new schema file.\nwarehouse └── default.db └── my_table ├── schema ├── schema-0 ├── schema-1 └── schema-2 There is a reference to schema in the snapshot. The schema file with the highest numerical value is usually the latest schema file.\nOld schema files cannot be directly deleted because there may be old data files that reference old schema files. When reading table, it is necessary to rely on them for schema evolution reading.\n"});index.add({'id':26,'href':'/docs/0.9/flink/sql-ddl/','title':"SQL DDL",'section':"Engine Flink",'content':"SQL DDL #  Create Catalog #  Paimon catalogs currently support three types of metastores:\n filesystem metastore (default), which stores both metadata and table files in filesystems. hive metastore, which additionally stores metadata in Hive metastore. Users can directly access the tables from Hive. jdbc metastore, which additionally stores metadata in relational databases such as MySQL, Postgres, etc.  See CatalogOptions for detailed options when creating a catalog.\nCreate Filesystem Catalog #  The following Flink SQL registers and uses a Paimon catalog named my_catalog. Metadata and table files are stored under hdfs:///path/to/warehouse.\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs:///path/to/warehouse\u0026#39; ); USE CATALOG my_catalog; You can define any default table options with the prefix table-default. for tables created in the catalog.\nCreating Hive Catalog #  By using Paimon Hive catalog, changes to the catalog will directly affect the corresponding Hive metastore. Tables created in such catalog can also be accessed directly from Hive.\nTo use Hive catalog, Database name, Table name and Field names should be lower case.\nPaimon Hive catalog in Flink relies on Flink Hive connector bundled jar. You should first download Hive connector bundled jar and add it to classpath.\n   Metastore version Bundle Name SQL Client JAR     2.3.0 - 3.1.3 Flink Bundle Download   1.2.0 - x.x.x Presto Bundle Download    The following Flink SQL registers and uses a Paimon Hive catalog named my_hive. Metadata and table files are stored under hdfs:///path/to/warehouse. In addition, metadata is also stored in Hive metastore.\nIf your Hive requires security authentication such as Kerberos, LDAP, Ranger or you want the paimon table to be managed by Apache Atlas(Setting \u0026lsquo;hive.metastore.event.listeners\u0026rsquo; in hive-site.xml). You can specify the hive-conf-dir and hadoop-conf-dir parameter to the hive-site.xml file path.\nCREATE CATALOG my_hive WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;hive\u0026#39;, -- \u0026#39;uri\u0026#39; = \u0026#39;thrift://\u0026lt;hive-metastore-host-name\u0026gt;:\u0026lt;port\u0026gt;\u0026#39;, default use \u0026#39;hive.metastore.uris\u0026#39; in HiveConf  -- \u0026#39;hive-conf-dir\u0026#39; = \u0026#39;...\u0026#39;, this is recommended in the kerberos environment  -- \u0026#39;hadoop-conf-dir\u0026#39; = \u0026#39;...\u0026#39;, this is recommended in the kerberos environment  -- \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs:///path/to/warehouse\u0026#39;, default use \u0026#39;hive.metastore.warehouse.dir\u0026#39; in HiveConf ); USE CATALOG my_hive; You can define any default table options with the prefix table-default. for tables created in the catalog.\nAlso, you can create FlinkGenericCatalog.\n When using hive catalog to change incompatible column types through alter table, you need to configure hive.metastore.disallow.incompatible.col.type.changes=false. see HIVE-17832.\n  If you are using Hive3, please disable Hive ACID:\nhive.strict.managed.tables=false hive.create.as.insert.only=false metastore.create.as.acid=false  Synchronizing Partitions into Hive Metastore #  By default, Paimon does not synchronize newly created partitions into Hive metastore. Users will see an unpartitioned table in Hive. Partition push-down will be carried out by filter push-down instead.\nIf you want to see a partitioned table in Hive and also synchronize newly created partitions into Hive metastore, please set the table property metastore.partitioned-table to true. Also see CoreOptions.\nAdding Parameters to a Hive Table #  Using the table option facilitates the convenient definition of Hive table parameters. Parameters prefixed with hive. will be automatically defined in the TBLPROPERTIES of the Hive table. For instance, using the option hive.table.owner=Jon will automatically add the parameter table.owner=Jon to the table properties during the creation process.\nSetting Location in Properties #  If you are using an object storage , and you don\u0026rsquo;t want that the location of paimon table/database is accessed by the filesystem of hive, which may lead to the error such as \u0026ldquo;No FileSystem for scheme: s3a\u0026rdquo;. You can set location in the properties of table/database by the config of location-in-properties. See setting the location of table/database in properties \nCreating JDBC Catalog #  By using the Paimon JDBC catalog, changes to the catalog will be directly stored in relational databases such as SQLite, MySQL, postgres, etc.\nCurrently, lock configuration is only supported for MySQL and SQLite. If you are using a different type of database for catalog storage, please do not configure lock.enabled.\nPaimon JDBC Catalog in Flink needs to correctly add the corresponding jar package for connecting to the database. You should first download JDBC connector bundled jar and add it to classpath. such as MySQL, postgres\n   database type Bundle Name SQL Client JAR     mysql mysql-connector-java Download   postgres postgresql Download    CREATE CATALOG my_jdbc WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;jdbc\u0026#39;, \u0026#39;uri\u0026#39; = \u0026#39;jdbc:mysql://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;databaseName\u0026gt;\u0026#39;, \u0026#39;jdbc.user\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;jdbc.password\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;catalog-key\u0026#39;=\u0026#39;jdbc\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs:///path/to/warehouse\u0026#39; ); USE CATALOG my_jdbc; You can configure any connection parameters that have been declared by JDBC through \u0026ldquo;jdbc.\u0026rdquo;, the connection parameters may be different between different databases, please configure according to the actual situation.\nYou can also perform logical isolation for databases under multiple catalogs by specifying \u0026ldquo;catalog-key\u0026rdquo;.\nAdditionally, when creating a JdbcCatalog, you can specify the maximum length for the lock key by configuring \u0026ldquo;lock-key-max-length,\u0026rdquo; which defaults to 255. Since this value is a combination of {catalog-key}.{database-name}.{table-name}, please adjust accordingly.\nYou can define any default table options with the prefix table-default. for tables created in the catalog.\nCreate Table #  After use Paimon catalog, you can create and drop tables. Tables created in Paimon Catalogs are managed by the catalog. When the table is dropped from catalog, its table files will also be deleted.\nThe following SQL assumes that you have registered and are using a Paimon catalog. It creates a managed table named my_table with five columns in the catalog\u0026rsquo;s default database, where dt, hh and user_id are the primary keys.\nCREATE TABLE my_table ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ); You can create partitioned table:\nCREATE TABLE my_table ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ) PARTITIONED BY (dt, hh); If you need cross partition upsert (primary keys not contain all partition fields), see Cross partition Upsert mode.  By configuring partition.expiration-time, expired partitions can be automatically deleted.  Specify Statistics Mode #  Paimon will automatically collect the statistics of the data file for speeding up the query process. There are four modes supported:\n full: collect the full metrics: null_count, min, max . truncate(length): length can be any positive number, the default mode is truncate(16), which means collect the null count, min/max value with truncated length of 16. This is mainly to avoid too big column which will enlarge the manifest file. counts: only collect the null count. none: disable the metadata stats collection.  The statistics collector mode can be configured by 'metadata.stats-mode', by default is 'truncate(16)'. You can configure the field level by setting 'fields.{field_name}.stats-mode'.\nField Default Value #  Paimon table currently supports setting default values for fields in table properties by 'fields.item_id.default-value', note that partition fields and primary key fields can not be specified.\nCreate Table As Select #  Table can be created and populated by the results of a query, for example, we have a sql like this: CREATE TABLE table_b AS SELECT id, name FORM table_a, The resulting table table_b will be equivalent to create the table and insert the data with the following statement: CREATE TABLE table_b (id INT, name STRING); INSERT INTO table_b SELECT id, name FROM table_a;\nWe can specify the primary key or partition when use CREATE TABLE AS SELECT, for syntax, please refer to the following sql.\n/* For streaming mode, you need to enable the checkpoint. */ CREATE TABLE my_table ( user_id BIGINT, item_id BIGINT ); CREATE TABLE my_table_as AS SELECT * FROM my_table; /* partitioned table */ CREATE TABLE my_table_partition ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh); CREATE TABLE my_table_partition_as WITH (\u0026#39;partition\u0026#39; = \u0026#39;dt\u0026#39;) AS SELECT * FROM my_table_partition; /* change options */ CREATE TABLE my_table_options ( user_id BIGINT, item_id BIGINT ) WITH (\u0026#39;file.format\u0026#39; = \u0026#39;orc\u0026#39;); CREATE TABLE my_table_options_as WITH (\u0026#39;file.format\u0026#39; = \u0026#39;parquet\u0026#39;) AS SELECT * FROM my_table_options; /* primary key */ CREATE TABLE my_table_pk ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ); CREATE TABLE my_table_pk_as WITH (\u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh\u0026#39;) AS SELECT * FROM my_table_pk; /* primary key + partition */ CREATE TABLE my_table_all ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ) PARTITIONED BY (dt, hh); CREATE TABLE my_table_all_as WITH (\u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh\u0026#39;, \u0026#39;partition\u0026#39; = \u0026#39;dt\u0026#39;) AS SELECT * FROM my_table_all; Create Table Like #  To create a table with the same schema, partition, and table properties as another table, use CREATE TABLE LIKE.\nCREATE TABLE my_table ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ); CREATE TABLE my_table_like LIKE my_table (EXCLUDING OPTIONS); Work with Flink Temporary Tables #  Flink Temporary tables are just recorded but not managed by the current Flink SQL session. If the temporary table is dropped, its resources will not be deleted. Temporary tables are also dropped when Flink SQL session is closed.\nIf you want to use Paimon catalog along with other tables but do not want to store them in other catalogs, you can create a temporary table. The following Flink SQL creates a Paimon catalog and a temporary table and also illustrates how to use both tables together.\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs:///path/to/warehouse\u0026#39; ); USE CATALOG my_catalog; -- Assume that there is already a table named my_table in my_catalog  CREATE TEMPORARY TABLE temp_table ( k INT, v STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;hdfs:///path/to/temp_table.csv\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ); SELECT my_table.k, my_table.v, temp_table.v FROM my_table JOIN temp_table ON my_table.k = temp_table.k; "});index.add({'id':27,'href':'/docs/0.9/spark/sql-ddl/','title':"SQL DDL",'section':"Engine Spark",'content':"SQL DDL #  Create Catalog #  Paimon catalogs currently support three types of metastores:\n filesystem metastore (default), which stores both metadata and table files in filesystems. hive metastore, which additionally stores metadata in Hive metastore. Users can directly access the tables from Hive. jdbc metastore, which additionally stores metadata in relational databases such as MySQL, Postgres, etc.  See CatalogOptions for detailed options when creating a catalog.\nCreate Filesystem Catalog #  The following Spark SQL registers and uses a Paimon catalog named my_catalog. Metadata and table files are stored under hdfs:///path/to/warehouse.\nThe following shell command registers a paimon catalog named paimon. Metadata and table files are stored under hdfs:///path/to/warehouse.\nspark-sql ... \\  --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\  --conf spark.sql.catalog.paimon.warehouse=hdfs:///path/to/warehouse You can define any default table options with the prefix spark.sql.catalog.paimon.table-default. for tables created in the catalog.\nAfter spark-sql is started, you can switch to the default database of the paimon catalog with the following SQL.\nUSE paimon.default; Creating Hive Catalog #  By using Paimon Hive catalog, changes to the catalog will directly affect the corresponding Hive metastore. Tables created in such catalog can also be accessed directly from Hive.\nTo use Hive catalog, Database name, Table name and Field names should be lower case.\nYour Spark installation should be able to detect, or already contains Hive dependencies. See here for more information.\nThe following shell command registers a Paimon Hive catalog named paimon. Metadata and table files are stored under hdfs:///path/to/warehouse. In addition, metadata is also stored in Hive metastore.\nspark-sql ... \\  --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\  --conf spark.sql.catalog.paimon.warehouse=hdfs:///path/to/warehouse \\  --conf spark.sql.catalog.paimon.metastore=hive \\  --conf spark.sql.catalog.paimon.uri=thrift://\u0026lt;hive-metastore-host-name\u0026gt;:\u0026lt;port\u0026gt; You can define any default table options with the prefix spark.sql.catalog.paimon.table-default. for tables created in the catalog.\nAfter spark-sql is started, you can switch to the default database of the paimon catalog with the following SQL.\nUSE paimon.default; Also, you can create SparkGenericCatalog.\nSynchronizing Partitions into Hive Metastore #  By default, Paimon does not synchronize newly created partitions into Hive metastore. Users will see an unpartitioned table in Hive. Partition push-down will be carried out by filter push-down instead.\nIf you want to see a partitioned table in Hive and also synchronize newly created partitions into Hive metastore, please set the table property metastore.partitioned-table to true. Also see CoreOptions.\nCreating JDBC Catalog #  By using the Paimon JDBC catalog, changes to the catalog will be directly stored in relational databases such as SQLite, MySQL, postgres, etc.\nCurrently, lock configuration is only supported for MySQL and SQLite. If you are using a different type of database for catalog storage, please do not configure lock.enabled.\nPaimon JDBC Catalog in Spark needs to correctly add the corresponding jar package for connecting to the database. You should first download JDBC connector bundled jar and add it to classpath. such as MySQL, postgres\n   database type Bundle Name SQL Client JAR     mysql mysql-connector-java Download   postgres postgresql Download    spark-sql ... \\  --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\  --conf spark.sql.catalog.paimon.warehouse=hdfs:///path/to/warehouse \\  --conf spark.sql.catalog.paimon.metastore=jdbc \\  --conf spark.sql.catalog.paimon.uri=jdbc:mysql://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;databaseName\u0026gt; \\  --conf spark.sql.catalog.paimon.jdbc.user=... \\  --conf spark.sql.catalog.paimon.jdbc.password=... USE paimon.default; Create Table #  After use Paimon catalog, you can create and drop tables. Tables created in Paimon Catalogs are managed by the catalog. When the table is dropped from catalog, its table files will also be deleted.\nThe following SQL assumes that you have registered and are using a Paimon catalog. It creates a managed table named my_table with five columns in the catalog\u0026rsquo;s default database, where dt, hh and user_id are the primary keys.\nCREATE TABLE my_table ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39; ); You can create partitioned table:\nCREATE TABLE my_table ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39; ); Create Table As Select #  Table can be created and populated by the results of a query, for example, we have a sql like this: CREATE TABLE table_b AS SELECT id, name FORM table_a, The resulting table table_b will be equivalent to create the table and insert the data with the following statement: CREATE TABLE table_b (id INT, name STRING); INSERT INTO table_b SELECT id, name FROM table_a;\nWe can specify the primary key or partition when use CREATE TABLE AS SELECT, for syntax, please refer to the following sql.\nCREATE TABLE my_table ( user_id BIGINT, item_id BIGINT ); CREATE TABLE my_table_as AS SELECT * FROM my_table; /* partitioned table*/ CREATE TABLE my_table_partition ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh); CREATE TABLE my_table_partition_as PARTITIONED BY (dt) AS SELECT * FROM my_table_partition; /* change TBLPROPERTIES */ CREATE TABLE my_table_options ( user_id BIGINT, item_id BIGINT ) TBLPROPERTIES (\u0026#39;file.format\u0026#39; = \u0026#39;orc\u0026#39;); CREATE TABLE my_table_options_as TBLPROPERTIES (\u0026#39;file.format\u0026#39; = \u0026#39;parquet\u0026#39;) AS SELECT * FROM my_table_options; /* primary key */ CREATE TABLE my_table_pk ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39; ); CREATE TABLE my_table_pk_as TBLPROPERTIES (\u0026#39;primary-key\u0026#39; = \u0026#39;dt\u0026#39;) AS SELECT * FROM my_table_pk; /* primary key + partition */ CREATE TABLE my_table_all ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39; ); CREATE TABLE my_table_all_as PARTITIONED BY (dt) TBLPROPERTIES (\u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh\u0026#39;) AS SELECT * FROM my_table_all; "});index.add({'id':28,'href':'/docs/0.9/flink/sql-write/','title':"SQL Write",'section':"Engine Flink",'content':"SQL Write #  Syntax #  INSERT { INTO | OVERWRITE } table_identifier [ part_spec ] [ column_list ] { value_expr | query }; For more information, please check the syntax document:\nFlink INSERT Statement\nINSERT INTO #  Use INSERT INTO to apply records and changes to tables.\nINSERT INTO my_table SELECT ... INSERT INTO supports both batch and streaming mode. In Streaming mode, by default, it will also perform compaction, snapshot expiration, and even partition expiration in Flink Sink (if it is configured).\nFor multiple jobs to write the same table, you can refer to dedicated compaction job for more info.\nClustering #  In Paimon, clustering is a feature that allows you to cluster data in your Append Table based on the values of certain columns during the write process. This organization of data can significantly enhance the efficiency of downstream tasks when reading the data, as it enables faster and more targeted data retrieval. This feature is only supported for Append Table(bucket = -1) and batch execution mode.\nTo utilize clustering, you can specify the columns you want to cluster when creating or writing to a table. Here\u0026rsquo;s a simple example of how to enable clustering:\nCREATE TABLE my_table ( a STRING, b STRING, c STRING, ) WITH ( \u0026#39;sink.clustering.by-columns\u0026#39; = \u0026#39;a,b\u0026#39;, ); You can also use SQL hints to dynamically set clustering options:\nINSERT INTO my_table /*+ OPTIONS(\u0026#39;sink.clustering.by-columns\u0026#39; = \u0026#39;a,b\u0026#39;) */ SELECT * FROM source; The data is clustered using an automatically chosen strategy (such as ORDER, ZORDER, or HILBERT), but you can manually specify the clustering strategy by setting the sink.clustering.strategy. Clustering relies on sampling and sorting. If the clustering process takes too much time, you can decrease the total sample number by setting the sink.clustering.sample-factor or disable the sorting step by setting the sink.clustering.sort-in-cluster to false.\nYou can refer to FlinkConnectorOptions for more info about the configurations above.\nOverwriting the Whole Table #  For unpartitioned tables, Paimon supports overwriting the whole table. (or for partitioned table which disables dynamic-partition-overwrite option).\nUse INSERT OVERWRITE to overwrite the whole unpartitioned table.\nINSERT OVERWRITE my_table SELECT ... Overwriting a Partition #  For partitioned tables, Paimon supports overwriting a partition.\nUse INSERT OVERWRITE to overwrite a partition.\nINSERT OVERWRITE my_table PARTITION (key1 = value1, key2 = value2, ...) SELECT ... Dynamic Overwrite #  Flink\u0026rsquo;s default overwrite mode is dynamic partition overwrite (that means Paimon only deletes the partitions appear in the overwritten data). You can configure dynamic-partition-overwrite to change it to static overwritten.\n-- MyTable is a Partitioned Table  -- Dynamic overwrite INSERT OVERWRITE my_table SELECT ... -- Static overwrite (Overwrite whole table) INSERT OVERWRITE my_table /*+ OPTIONS(\u0026#39;dynamic-partition-overwrite\u0026#39; = \u0026#39;false\u0026#39;) */ SELECT ... Truncate tables #  Flink 1.17- You can use INSERT OVERWRITE to purge tables by inserting empty value.\nINSERT OVERWRITE my_table /*+ OPTIONS(\u0026#39;dynamic-partition-overwrite\u0026#39;=\u0026#39;false\u0026#39;) */ SELECT * FROM my_table WHERE false; Flink 1.18\u0026#43; TRUNCATE TABLE my_table;  Purging Partitions #  Currently, Paimon supports two ways to purge partitions.\n  Like purging tables, you can use INSERT OVERWRITE to purge data of partitions by inserting empty value to them.\n  Method #1 does not support to drop multiple partitions. In case that you need to drop multiple partitions, you can submit the drop_partition job through flink run.\n  -- Syntax INSERT OVERWRITE my_table /*+ OPTIONS(\u0026#39;dynamic-partition-overwrite\u0026#39;=\u0026#39;false\u0026#39;) */ PARTITION (key1 = value1, key2 = value2, ...) SELECT selectSpec FROM my_table WHERE false; -- The following SQL is an example: -- table definition CREATE TABLE my_table ( k0 INT, k1 INT, v STRING ) PARTITIONED BY (k0, k1); -- you can use INSERT OVERWRITE my_table /*+ OPTIONS(\u0026#39;dynamic-partition-overwrite\u0026#39;=\u0026#39;false\u0026#39;) */ PARTITION (k0 = 0) SELECT k1, v FROM my_table WHERE false; -- or INSERT OVERWRITE my_table /*+ OPTIONS(\u0026#39;dynamic-partition-overwrite\u0026#39;=\u0026#39;false\u0026#39;) */ PARTITION (k0 = 0, k1 = 0) SELECT v FROM my_table WHERE false; Updating tables #  Important table properties setting:\n Only primary key table supports this feature. MergeEngine needs to be deduplicate or partial-update to support this feature. Do not support updating primary keys.   Currently, Paimon supports updating records by using UPDATE in Flink 1.17 and later versions. You can perform UPDATE in Flink\u0026rsquo;s batch mode.\n-- Syntax UPDATE table_identifier SET column1 = value1, column2 = value2, ... WHERE condition; -- The following SQL is an example: -- table definition CREATE TABLE my_table ( a STRING, b INT, c INT, PRIMARY KEY (a) NOT ENFORCED ) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;deduplicate\u0026#39; ); -- you can use UPDATE my_table SET b = 1, c = 2 WHERE a = \u0026#39;myTable\u0026#39;; Deleting from table #  Flink 1.17\u0026#43; Important table properties setting:\n Only primary key tables support this feature. If the table has primary keys, MergeEngine needs to be deduplicate to support this feature. Do not support deleting from table in streaming mode.   -- Syntax DELETE FROM table_identifier WHERE conditions; -- The following SQL is an example: -- table definition CREATE TABLE my_table ( id BIGINT NOT NULL, currency STRING, rate BIGINT, dt String, PRIMARY KEY (id, dt) NOT ENFORCED ) PARTITIONED BY (dt) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;deduplicate\u0026#39; ); -- you can use DELETE FROM my_table WHERE currency = \u0026#39;UNKNOWN\u0026#39;;  Partition Mark Done #  For partitioned tables, each partition may need to be scheduled to trigger downstream batch computation. Therefore, it is necessary to choose this timing to indicate that it is ready for scheduling and to minimize the amount of data drift during scheduling. We call this process: \u0026ldquo;Partition Mark Done\u0026rdquo;.\nExample to mark done:\nCREATE TABLE my_partitioned_table ( f0 INT, f1 INT, f2 INT, ... dt STRING ) PARTITIONED BY (dt) WITH ( \u0026#39;partition.timestamp-formatter\u0026#39;=\u0026#39;yyyyMMdd\u0026#39;, \u0026#39;partition.timestamp-pattern\u0026#39;=\u0026#39;$dt\u0026#39;, \u0026#39;partition.time-interval\u0026#39;=\u0026#39;1 d\u0026#39;, \u0026#39;partition.idle-time-to-done\u0026#39;=\u0026#39;15 m\u0026#39; );  Firstly, you need to define the time parser of the partition and the time interval between partitions in order to determine when the partition can be properly marked done. Secondly, you need to define idle-time, which determines how long it takes for the partition to have no new data, and then it will be marked as done. Thirdly, by default, partition mark done will create _SUCCESS file, the content of _SUCCESS file is a json, contains creationTime and modificationTime, they can help you understand if there is any delayed data. You can also configure other actions.  "});index.add({'id':29,'href':'/docs/0.9/spark/sql-write/','title':"SQL Write",'section':"Engine Spark",'content':"SQL Write #  Syntax #  INSERT { INTO | OVERWRITE } table_identifier [ part_spec ] [ column_list ] { value_expr | query }; For more information, please check the syntax document:\nSpark INSERT Statement\nINSERT INTO #  Use INSERT INTO to apply records and changes to tables.\nINSERT INTO my_table SELECT ... Overwriting the Whole Table #  Use INSERT OVERWRITE to overwrite the whole unpartitioned table.\nINSERT OVERWRITE my_table SELECT ... Overwriting a Partition #  Use INSERT OVERWRITE to overwrite a partition.\nINSERT OVERWRITE my_table PARTITION (key1 = value1, key2 = value2, ...) SELECT ... Dynamic Overwrite #  Spark\u0026rsquo;s default overwrite mode is static partition overwrite. To enable dynamic overwritten you need to set the Spark session configuration spark.sql.sources.partitionOverwriteMode to dynamic\nFor example:\nCREATE TABLE my_table (id INT, pt STRING) PARTITIONED BY (pt); INSERT INTO my_table VALUES (1, \u0026#39;p1\u0026#39;), (2, \u0026#39;p2\u0026#39;); -- Static overwrite (Overwrite the whole table) INSERT OVERWRITE my_table VALUES (3, \u0026#39;p1\u0026#39;); SELECT * FROM my_table; /* +---+---+ | id| pt| +---+---+ | 3| p1| +---+---+ */ -- Dynamic overwrite (Only overwrite pt=\u0026#39;p1\u0026#39;) SET spark.sql.sources.partitionOverwriteMode=dynamic; INSERT OVERWRITE my_table VALUES (3, \u0026#39;p1\u0026#39;); SELECT * FROM my_table; /* +---+---+ | id| pt| +---+---+ | 2| p2| | 3| p1| +---+---+ */ Truncate tables #  TRUNCATE TABLE my_table; Updating tables #  spark supports update PrimitiveType and StructType, for example:\n-- Syntax UPDATE table_identifier SET column1 = value1, column2 = value2, ... WHERE condition; CREATE TABLE t ( id INT, s STRUCT\u0026lt;c1: INT, c2: STRING\u0026gt;, name STRING) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;id\u0026#39;, \u0026#39;merge-engine\u0026#39; = \u0026#39;deduplicate\u0026#39; ); -- you can use UPDATE t SET name = \u0026#39;a_new\u0026#39; WHERE id = 1; UPDATE t SET s.c2 = \u0026#39;a_new\u0026#39; WHERE s.c1 = 1; Deleting from table #  DELETE FROM my_table WHERE currency = \u0026#39;UNKNOWN\u0026#39;; Merging into table #  Paimon currently supports Merge Into syntax in Spark 3+, which allow a set of updates, insertions and deletions based on a source table in a single commit.\n This only work with primary-key table. In update clause, to update primary key columns is not supported. WHEN NOT MATCHED BY SOURCE syntax is not supported.   Example: One\nThis is a simple demo that, if a row exists in the target table update it, else insert it.\n-- Here both source and target tables have the same schema: (a INT, b INT, c STRING), and a is a primary key.  MERGE INTO target USING source ON target.a = source.a WHEN MATCHED THEN UPDATE SET * WHEN NOT MATCHED THEN INSERT * Example: Two\nThis is a demo with multiple, conditional clauses.\n-- Here both source and target tables have the same schema: (a INT, b INT, c STRING), and a is a primary key.  MERGE INTO target USING source ON target.a = source.a WHEN MATCHED AND target.a = 5 THEN UPDATE SET b = source.b + target.b -- when matched and meet the condition 1, then update b; WHEN MATCHED AND source.c \u0026gt; \u0026#39;c2\u0026#39; THEN UPDATE SET * -- when matched and meet the condition 2, then update all the columns; WHEN MATCHED THEN DELETE -- when matched, delete this row in target table; WHEN NOT MATCHED AND c \u0026gt; \u0026#39;c9\u0026#39; THEN INSERT (a, b, c) VALUES (a, b * 1.1, c) -- when not matched but meet the condition 3, then transform and insert this row; WHEN NOT MATCHED THEN INSERT * -- when not matched, insert this row without any transformation;  Streaming Write #  Paimon currently supports Spark 3+ for streaming write.\nPaimon Structured Streaming only supports the two append and complete modes.\n // Create a paimon table if not exists. spark.sql(s\u0026#34;\u0026#34;\u0026#34; |CREATE TABLE T (k INT, v STRING) |TBLPROPERTIES (\u0026#39;primary-key\u0026#39;=\u0026#39;a\u0026#39;, \u0026#39;bucket\u0026#39;=\u0026#39;3\u0026#39;) |\u0026#34;\u0026#34;\u0026#34;.stripMargin) // Here we use MemoryStream to fake a streaming source. val inputData = MemoryStream[(Int, String)] val df = inputData.toDS().toDF(\u0026#34;k\u0026#34;, \u0026#34;v\u0026#34;) // Streaming Write to paimon table. val stream = df .writeStream .outputMode(\u0026#34;append\u0026#34;) .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;/path/to/checkpoint\u0026#34;) .format(\u0026#34;paimon\u0026#34;) .start(\u0026#34;/path/to/paimon/sink/table\u0026#34;) Schema Evolution #  Schema evolution is a feature that allows users to easily modify the current schema of a table to adapt to existing data, or new data that changes over time, while maintaining data integrity and consistency.\nPaimon supports automatic schema merging of source data and current table data while data is being written, and uses the merged schema as the latest schema of the table, and it only requires configuring write.merge-schema.\ndata.write .format(\u0026#34;paimon\u0026#34;) .mode(\u0026#34;append\u0026#34;) .option(\u0026#34;write.merge-schema\u0026#34;, \u0026#34;true\u0026#34;) .save(location) When enable write.merge-schema, Paimon can allow users to perform the following actions on table schema by default:\n Adding columns Up-casting the type of column(e.g. Int -\u0026gt; Long)  Paimon also supports explicit type conversions between certain types (e.g. String -\u0026gt; Date, Long -\u0026gt; Int), it requires an explicit configuration write.merge-schema.explicit-cast.\nSchema evolution can be used in streaming mode at the same time.\nval inputData = MemoryStream[(Int, String)] inputData .toDS() .toDF(\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;) .writeStream .format(\u0026#34;paimon\u0026#34;) .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;/path/to/checkpoint\u0026#34;) .option(\u0026#34;write.merge-schema\u0026#34;, \u0026#34;true\u0026#34;) .option(\u0026#34;write.merge-schema.explicit-cast\u0026#34;, \u0026#34;true\u0026#34;) .start(location) Here list the configurations.\n  Scan Mode Description     write.merge-schema If true, merge the data schema and the table schema automatically before write data.   write.merge-schema.explicit-cast If true, allow to merge data types if the two types meet the rules for explicit casting.    "});index.add({'id':30,'href':'/docs/0.9/engines/starrocks/','title':"StarRocks",'section':"Engine Others",'content':"StarRocks #  This documentation is a guide for using Paimon in StarRocks.\nVersion #  Paimon currently supports StarRocks 3.1 and above. Recommended version is StarRocks 3.2.6 or above.\nCreate Paimon Catalog #  Paimon catalogs are registered by executing a CREATE EXTERNAL CATALOG SQL in StarRocks. For example, you can use the following SQL to create a Paimon catalog named paimon_catalog.\nCREATE EXTERNAL CATALOG paimon_catalog PROPERTIES ( \u0026#34;type\u0026#34; = \u0026#34;paimon\u0026#34;, \u0026#34;paimon.catalog.type\u0026#34; = \u0026#34;filesystem\u0026#34;, \u0026#34;paimon.catalog.warehouse\u0026#34; = \u0026#34;oss://\u0026lt;your_bucket\u0026gt;/user/warehouse/\u0026#34; ); More catalog types and configures can be seen in Paimon catalog.\nQuery #  Suppose there already exists a database named test_db and a table named test_tbl in paimon_catalog, you can query this table using the following SQL:\nSELECT * FROM paimon_catalog.test_db.test_tbl; Query System Tables #  You can access all kinds of Paimon system tables by StarRocks. For example, you can read the ro (read-optimized) system table to improve reading performance of primary-key tables.\nSELECT * FROM paimon_catalog.test_db.test_tbl$ro; For another example, you can query partition files of the table using the following SQL:\nSELECT * FROM paimon_catalog.test_db.partition_tbl$partitions; /* +-----------+--------------+--------------------+------------+----------------------------+ | partition | record_count | file_size_in_bytes | file_count | last_update_time | +-----------+--------------+--------------------+------------+----------------------------+ | [1] | 1 | 645 | 1 | 2024-01-01 00:00:00.000000 | +-----------+--------------+--------------------+------------+----------------------------+ */ StarRocks to Paimon type mapping #  This section lists all supported type conversion between StarRocks and Paimon. All StarRocks’s data types can be found in this doc StarRocks Data type overview.\n  StarRocks Data Type Paimon Data Type Atomic Type     STRUCT RowType false   MAP MapType false   ARRAY ArrayType false   BOOLEAN BooleanType true   TINYINT TinyIntType true   SMALLINT SmallIntType true   INT IntType true   BIGINT BigIntType true   FLOAT FloatType true   DOUBLE DoubleType true   CHAR(length) CharType(length) true   VARCHAR(MAX_VARCHAR_LENGTH) VarCharType(VarCharType.MAX_LENGTH) true   VARCHAR(length) VarCharType(length), length is less than VarCharType.MAX_LENGTH true   DATE DateType true   DATETIME TimestampType true   DECIMAL(precision, scale) DecimalType(precision, scale) true   VARBINARY(length) VarBinaryType(length) true   DATETIME LocalZonedTimestampType true    "});index.add({'id':31,'href':'/docs/0.9/append-table/streaming/','title':"Streaming",'section':"Table w/o PK",'content':"Streaming #  You can streaming write to the Append table in a very flexible way through Flink, or through read the Append table Flink, using it like a queue. The only difference is that its latency is in minutes. Its advantages are very low cost and the ability to push down filters and projection.\nAutomatic small file merging #  In streaming writing job, without bucket definition, there is no compaction in writer, instead, will use Compact Coordinator to scan the small files and pass compaction task to Compact Worker. In streaming mode, if you run insert sql in flink, the topology will be like this:\nDo not worry about backpressure, compaction never backpressure.\nIf you set write-only to true, the Compact Coordinator and Compact Worker will be removed in the topology.\nThe auto compaction is only supported in Flink engine streaming mode. You can also start a compaction job in flink by flink action in paimon and disable all the other compaction by set write-only.\nStreaming Query #  You can stream the Append table and use it like a Message Queue. As with primary key tables, there are two options for streaming reads:\n By default, Streaming read produces the latest snapshot on the table upon first startup, and continue to read the latest incremental records. You can specify scan.mode or scan.snapshot-id or scan.timestamp-millis or scan.file-creation-time-millis to streaming read incremental only.  Similar to flink-kafka, order is not guaranteed by default, if your data has some sort of order requirement, you also need to consider defining a bucket-key, see Bucketed Append\n"});index.add({'id':32,'href':'/docs/0.9/primary-key-table/','title':"Table with PK",'section':"Apache Paimon",'content':""});index.add({'id':33,'href':'/docs/0.9/maintenance/write-performance/','title':"Write Performance",'section':"Maintenance",'content':"Write Performance #  Paimon\u0026rsquo;s write performance is closely related to checkpoint, so if you need greater write throughput:\n Flink Configuration ('flink-conf.yaml' or SET in SQL): Increase the checkpoint interval ('execution.checkpointing.interval'), increase max concurrent checkpoints to 3 ('execution.checkpointing.max-concurrent-checkpoints'), or just use batch mode. Increase write-buffer-size. Enable write-buffer-spillable. Rescale bucket number if you are using Fixed-Bucket mode.  Option 'changelog-producer' = 'lookup' or 'full-compaction', and option 'full-compaction.delta-commits' have a large impact on write performance, if it is a snapshot / full synchronization phase you can unset these options and then enable them again in the incremental phase.\nIf you find that the input of the job shows a jagged pattern in the case of backpressure, it may be imbalanced work nodes. You can consider turning on Asynchronous Compaction to observe if the throughput is increased.\nParallelism #  It is recommended that the parallelism of sink should be less than or equal to the number of buckets, preferably equal. You can control the parallelism of the sink with the sink.parallelism table property.\n  Option Required Default Type Description     sink.parallelism No (none) Integer Defines the parallelism of the sink operator. By default, the parallelism is determined by the framework using the same parallelism of the upstream chained operator.    Local Merging #  If your job suffers from primary key data skew (for example, you want to count the number of views for each page in a website, and some particular pages are very popular among the users), you can set 'local-merge-buffer-size' so that input records will be buffered and merged before they\u0026rsquo;re shuffled by bucket and written into sink. This is particularly useful when the same primary key is updated frequently between snapshots.\nThe buffer will be flushed when it is full. We recommend starting with 64 mb when you are faced with data skew but don\u0026rsquo;t know where to start adjusting buffer size.\n(Currently, Local merging not works for CDC ingestion)\nFile Format #  If you want to achieve ultimate compaction performance, you can consider using row storage file format AVRO.\n The advantage is that you can achieve high write throughput and compaction performance. The disadvantage is that your analysis queries will be slow, and the biggest problem with row storage is that it does not have the query projection. For example, if the table have 100 columns but only query a few columns, the IO of row storage cannot be ignored. Additionally, compression efficiency will decrease and storage costs will increase.  This a tradeoff.\nEnable row storage through the following options:\nfile.format = avro metadata.stats-mode = none The collection of statistical information for row storage is a bit expensive, so I suggest turning off statistical information as well.\nIf you don\u0026rsquo;t want to modify all files to Avro format, at least you can consider modifying the files in the previous layers to Avro format. You can use 'file.format.per.level' = '0:avro,1:avro' to specify the files in the first two layers to be in Avro format.\nFile Compression #  By default, Paimon uses zstd with level 1, you can modify the compression algorithm:\n'file.compression.zstd-level': Default zstd level is 1. For higher compression rates, it can be configured to 9, but the read and write speed will significantly decrease.\nStability #  If there are too few buckets or resources, full-compaction may cause the checkpoint timeout, Flink\u0026rsquo;s default checkpoint timeout is 10 minutes.\nIf you expect stability even in this case, you can turn up the checkpoint timeout, for example:\nexecution.checkpointing.timeout = 60 min Write Initialize #  In the initialization of write, the writer of the bucket needs to read all historical files. If there is a bottleneck here (For example, writing a large number of partitions simultaneously), you can use write-manifest-cache to cache the read manifest data to accelerate initialization.\nWrite Memory #  There are three main places in Paimon writer that takes up memory:\n Writer\u0026rsquo;s memory buffer, shared and preempted by all writers of a single task. This memory value can be adjusted by the write-buffer-size table property. Memory consumed when merging several sorted runs for compaction. Can be adjusted by the num-sorted-run.compaction-trigger option to change the number of sorted runs to be merged. If the row is very large, reading too many lines of data at once will consume a lot of memory when making a compaction. Reducing the read.batch-size option can alleviate the impact of this case. The memory consumed by writing columnar ORC file. Decreasing the orc.write.batch-size option can reduce the consumption of memory for ORC format. If files are automatically compaction in the write task, dictionaries for certain large columns can significantly consume memory during compaction.  To disable dictionary encoding for all fields in Parquet format, set 'parquet.enable.dictionary'= 'false'. To disable dictionary encoding for all fields in ORC format, set orc.dictionary.key.threshold='0'. Additionally,set orc.column.encoding.direct='field1,field2' to disable dictionary encoding for specific columns.    If your Flink job does not rely on state, please avoid using managed memory, which you can control with the following Flink parameter:\ntaskmanager.memory.managed.size=1m Or you can use Flink managed memory for your write buffer to avoid OOM, set table property:\nsink.use-managed-memory-allocator=true Commit Memory #  Committer node may use a large memory if the amount of data written to the table is particularly large, OOM may occur if the memory is too small. In this case, you need to increase the Committer heap memory, but you may not want to increase the memory of Flink\u0026rsquo;s TaskManager uniformly, which may lead to a waste of memory.\nYou can use fine-grained-resource-management of Flink to increase committer heap memory only:\n Configure Flink Configuration cluster.fine-grained-resource-management.enabled: true. (This is default after Flink 1.18) Configure Paimon Table Options: sink.committer-memory, for example 300 MB, depends on your TaskManager. (sink.committer-cpu is also supported)  "});index.add({'id':34,'href':'/docs/0.9/primary-key-table/merge-engine/aggregation/','title':"Aggregation",'section':"Merge Engine",'content':"Aggregation #  NOTE: Always set table.exec.sink.upsert-materialize to NONE in Flink SQL TableConfig.  Sometimes users only care about aggregated results. The aggregation merge engine aggregates each value field with the latest data one by one under the same primary key according to the aggregate function.\nEach field not part of the primary keys can be given an aggregate function, specified by the fields.\u0026lt;field-name\u0026gt;.aggregate-function table property, otherwise it will use last_non_null_value aggregation as default. For example, consider the following table definition.\nFlink CREATE TABLE my_table ( product_id BIGINT, price DOUBLE, sales BIGINT, PRIMARY KEY (product_id) NOT ENFORCED ) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;aggregation\u0026#39;, \u0026#39;fields.price.aggregate-function\u0026#39; = \u0026#39;max\u0026#39;, \u0026#39;fields.sales.aggregate-function\u0026#39; = \u0026#39;sum\u0026#39; );  Field price will be aggregated by the max function, and field sales will be aggregated by the sum function. Given two input records \u0026lt;1, 23.0, 15\u0026gt; and \u0026lt;1, 30.2, 20\u0026gt;, the final result will be \u0026lt;1, 30.2, 35\u0026gt;.\nAggregation Functions #  Current supported aggregate functions and data types are:\nsum #  The sum function aggregates the values across multiple rows. It supports DECIMAL, TINYINT, SMALLINT, INTEGER, BIGINT, FLOAT, and DOUBLE data types.\nproduct #  The product function can compute product values across multiple lines. It supports DECIMAL, TINYINT, SMALLINT, INTEGER, BIGINT, FLOAT, and DOUBLE data types.\ncount #  In scenarios where counting rows that match a specific condition is required, you can use the SUM function to achieve this. By expressing a condition as a Boolean value (TRUE or FALSE) and converting it into a numerical value, you can effectively count the rows. In this approach, TRUE is converted to 1, and FALSE is converted to 0.\nFor example, if you have a table orders and want to count the number of rows that meet a specific condition, you can use the following query:\nSELECT SUM(CASE WHEN condition THEN 1 ELSE 0 END) AS count FROM orders; max #  The max function identifies and retains the maximum value. It supports CHAR, VARCHAR, DECIMAL, TINYINT, SMALLINT, INTEGER, BIGINT, FLOAT, DOUBLE, DATE, TIME, TIMESTAMP, and TIMESTAMP_LTZ data types.\nmin #  The min function identifies and retains the minimum value. It supports CHAR, VARCHAR, DECIMAL, TINYINT, SMALLINT, INTEGER, BIGINT, FLOAT, DOUBLE, DATE, TIME, TIMESTAMP, and TIMESTAMP_LTZ data types.\nlast_value #  The last_value function replaces the previous value with the most recently imported value. It supports all data types.\nlast_non_null_value #  The last_non_null_value function replaces the previous value with the latest non-null value. It supports all data types.\nlistagg #  The listagg function concatenates multiple string values into a single string. It supports STRING data type. Each field not part of the primary keys can be given a list agg delimiter, specified by the fields..list-agg-delimiter table property, otherwise it will use \u0026ldquo;,\u0026rdquo; as default.\nbool_and #  The bool_and function evaluates whether all values in a boolean set are true. It supports BOOLEAN data type.\nbool_or #  The bool_or function checks if at least one value in a boolean set is true. It supports BOOLEAN data type.\nfirst_value #  The first_value function retrieves the first null value from a data set. It supports all data types.\nfirst_non_null_value #  The first_non_null_value function selects the first non-null value in a data set. It supports all data types.\nrbm32 #  The rbm32 function aggregates multiple serialized 32-bit RoaringBitmap into a single RoaringBitmap. It supports VARBINARY data type.\nrbm64 #  The rbm64 function aggregates multiple serialized 64-bit Roaring64Bitmap into a single Roaring64Bitmap. It supports VARBINARY data type.\nnested_update #  The nested_update function collects multiple rows into one array (so-called \u0026lsquo;nested table\u0026rsquo;). It supports ARRAY data types.\nUse fields.\u0026lt;field-name\u0026gt;.nested-key=pk0,pk1,... to specify the primary keys of the nested table. If no keys, row will be appended to array.\nAn example:\nFlink -- orders table CREATE TABLE orders ( order_id BIGINT PRIMARY KEY NOT ENFORCED, user_name STRING, address STRING ); -- sub orders that have the same order_id -- belongs to the same order CREATE TABLE sub_orders ( order_id BIGINT, sub_order_id INT, product_name STRING, price BIGINT, PRIMARY KEY (order_id, sub_order_id) NOT ENFORCED ); -- wide table CREATE TABLE order_wide ( order_id BIGINT PRIMARY KEY NOT ENFORCED, user_name STRING, address STRING, sub_orders ARRAY\u0026lt;ROW\u0026lt;sub_order_id BIGINT, product_name STRING, price BIGINT\u0026gt;\u0026gt; ) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;aggregation\u0026#39;, \u0026#39;fields.sub_orders.aggregate-function\u0026#39; = \u0026#39;nested_update\u0026#39;, \u0026#39;fields.sub_orders.nested-key\u0026#39; = \u0026#39;sub_order_id\u0026#39; ); -- widen INSERT INTO order_wide SELECT order_id, user_name, address, CAST (NULL AS ARRAY\u0026lt;ROW\u0026lt;sub_order_id BIGINT, product_name STRING, price BIGINT\u0026gt;\u0026gt;) FROM orders UNION ALL SELECT order_id, CAST (NULL AS STRING), CAST (NULL AS STRING), ARRAY[ROW(sub_order_id, product_name, price)] FROM sub_orders; -- query using UNNEST SELECT order_id, user_name, address, sub_order_id, product_name, price FROM order_wide, UNNEST(sub_orders) AS so(sub_order_id, product_name, price)  collect #  The collect function collects elements into an Array. You can set fields.\u0026lt;field-name\u0026gt;.distinct=true to deduplicate elements. It only supports ARRAY type.\nmerge_map #  The merge_map function merge input maps. It only supports MAP type.\nTypes of cardinality sketches #  Paimon uses the Apache DataSketches library of stochastic streaming algorithms to implement sketch modules. The DataSketches library includes various types of sketches, each one designed to solve a different sort of problem. Paimon supports HyperLogLog (HLL) and Theta cardinality sketches.\nHyperLogLog #  The HyperLogLog (HLL) sketch aggregator is a very compact sketch algorithm for approximate distinct counting. You can also use the HLL aggregator to calculate a union of HLL sketches.\nTheta #  The Theta sketch is a sketch algorithm for approximate distinct counting with set operations. Theta sketches let you count the overlap between sets, so that you can compute the union, intersection, or set difference between sketch objects.\nChoosing a sketch type #  HLL and Theta sketches both support approximate distinct counting; however, the HLL sketch produces more accurate results and consumes less storage space. Theta sketches are more flexible but require significantly more memory.\nWhen choosing an approximation algorithm for your use case, consider the following:\nIf your use case entails distinct counting and merging sketch objects, use the HLL sketch. If you need to evaluate union, intersection, or difference set operations, use the Theta sketch. You cannot merge HLL sketches with Theta sketches.\nhll_sketch #  The hll_sketch function aggregates multiple serialized Sketch objects into a single Sketch. It supports VARBINARY data type.\nAn example:\nFlink -- source table CREATE TABLE VISITS ( id INT PRIMARY KEY NOT ENFORCED, user_id STRING ); -- agg table CREATE TABLE UV_AGG ( id INT PRIMARY KEY NOT ENFORCED, uv VARBINARY ) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;aggregation\u0026#39;, \u0026#39;fields.f0.aggregate-function\u0026#39; = \u0026#39;hll_sketch\u0026#39; ); -- Register the following class as a Flink function with the name \u0026#34;HLL_SKETCH\u0026#34; -- which is used to transform input to sketch bytes array: -- -- public static class HllSketchFunction extends ScalarFunction { -- public byte[] eval(String user_id) { -- HllSketch hllSketch = new HllSketch(); -- hllSketch.update(id); -- return hllSketch.toCompactByteArray(); -- } -- } -- INSERT INTO UV_AGG SELECT id, HLL_SKETCH(user_id) FROM VISITS; -- Register the following class as a Flink function with the name \u0026#34;HLL_SKETCH_COUNT\u0026#34; -- which is used to get cardinality from sketch bytes array: -- -- public static class HllSketchCountFunction extends ScalarFunction { -- public Double eval(byte[] sketchBytes) { -- if (sketchBytes == null) { -- return 0d; -- } -- return HllSketch.heapify(sketchBytes).getEstimate(); -- } -- } -- -- Then we can get user cardinality based on the aggregated field. SELECT id, HLL_SKETCH_COUNT(UV) as uv FROM UV_AGG;  theta_sketch #  The theta_sketch function aggregates multiple serialized Sketch objects into a single Sketch. It supports VARBINARY data type.\nAn example:\nFlink -- source table CREATE TABLE VISITS ( id INT PRIMARY KEY NOT ENFORCED, user_id STRING ); -- agg table CREATE TABLE UV_AGG ( id INT PRIMARY KEY NOT ENFORCED, uv VARBINARY ) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;aggregation\u0026#39;, \u0026#39;fields.f0.aggregate-function\u0026#39; = \u0026#39;theta_sketch\u0026#39; ); -- Register the following class as a Flink function with the name \u0026#34;THETA_SKETCH\u0026#34; -- which is used to transform input to sketch bytes array: -- -- public static class ThetaSketchFunction extends ScalarFunction { -- public byte[] eval(String user_id) { -- UpdateSketch updateSketch = UpdateSketch.builder().build(); -- updateSketch.update(user_id); -- return updateSketch.compact().toByteArray(); -- } -- } -- INSERT INTO UV_AGG SELECT id, THETA_SKETCH(user_id) FROM VISITS; -- Register the following class as a Flink function with the name \u0026#34;THETA_SKETCH_COUNT\u0026#34; -- which is used to get cardinality from sketch bytes array: -- -- public static class ThetaSketchCountFunction extends ScalarFunction { -- public Double eval(byte[] sketchBytes) { -- if (sketchBytes == null) { -- return 0d; -- } -- return Sketches.wrapCompactSketch(Memory.wrap(sketchBytes)).getEstimate(); -- } -- } -- -- Then we can get user cardinality based on the aggregated field. SELECT id, THETA_SKETCH_COUNT(UV) as uv FROM UV_AGG;  For streaming queries, aggregation merge engine must be used together with lookup or full-compaction changelog producer. (\u0026lsquo;input\u0026rsquo; changelog producer is also supported, but only returns input records.)  Retraction #  Only sum, product, collect, merge_map, nested_update, last_value and last_non_null_value supports retraction (UPDATE_BEFORE and DELETE), others aggregate functions do not support retraction. If you allow some functions to ignore retraction messages, you can configure: 'fields.${field_name}.ignore-retract'='true'.\nThe last_value and last_non_null_value just set field to null when accept retract messages.\nThe collect and merge_map make a best-effort attempt to handle retraction messages, but the results are not guaranteed to be accurate. The following behaviors may occur when processing retraction messages:\n  It might fail to handle retraction messages if records are disordered. For example, the table uses collect, and the upstreams send +I['A', 'B'] and -U['A'] respectively. If the table receives -U['A'] first, it can do nothing; then it receives +I['A', 'B'], the merge result will be +I['A', 'B'] instead of +I['B'].\n  The retract message from one upstream will retract the result merged from multiple upstreams. For example, the table uses merge_map, and one upstream sends +I[1-\u0026gt;A], another upstream sends +I[1-\u0026gt;B], -D[1-\u0026gt;B] later. The table will merge two insert values to +I[1-\u0026gt;B] first, and then the -D[1-\u0026gt;B] will retract the whole result, so the final result is an empty map instead of +I[1-\u0026gt;A]\n  "});index.add({'id':35,'href':'/docs/0.9/migration/clone-tables/','title':"Clone Tables",'section':"Migration",'content':"Clone Tables #  Paimon supports cloning tables for data migration. Currently, only table files used by the latest snapshot will be cloned.\nTo clone a table, run the following command to submit a clone job. If the table you clone is not modified at the same time, it is recommended to submit a Flink batch job for better performance. However, if you want to clone the table while writing it at the same time, submit a Flink streaming job for automatic failure recovery.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  clone \\  --warehouse \u0026lt;source-warehouse-path\u0026gt; \\  [--database \u0026lt;source-database-name\u0026gt;] \\  [--table \u0026lt;source-table-name\u0026gt;] \\  [--catalog_conf \u0026lt;source-paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;source-paimon-catalog-conf\u0026gt; ...]] \\  --target_warehouse \u0026lt;target-warehouse-path\u0026gt; \\  [--target_database \u0026lt;target-database\u0026gt;] \\  [--target_table \u0026lt;target-table-name\u0026gt;] \\  [--target_catalog_conf \u0026lt;target-paimon-catalog-conf\u0026gt; [--target_catalog_conf \u0026lt;target-paimon-catalog-conf\u0026gt; ...]] [--parallelism \u0026lt;parallelism\u0026gt;]  If database is not specified, all tables in all databases of the specified warehouse will be cloned. If table is not specified, all tables of the specified database will be cloned.   Example: Clone test_db.test_table from source warehouse to target warehouse.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  clone \\  --warehouse s3:///path/to/warehouse_source \\  --database test_db \\  --table test_table \\  --catalog_conf s3.endpoint=https://****.com \\  --catalog_conf s3.access-key=***** \\  --catalog_conf s3.secret-key=***** \\  --target_warehouse s3:///path/to/warehouse_target \\  --target_database test_db \\  --target_table test_table \\  --target_catalog_conf s3.endpoint=https://****.com \\  --target_catalog_conf s3.access-key=***** \\  --target_catalog_conf s3.secret-key=***** For more usage of the clone action, see\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  clone --help "});index.add({'id':36,'href':'/docs/0.9/concepts/concurrency-control/','title':"Concurrency Control",'section':"Concepts",'content':"Concurrency Control #  Paimon supports optimistic concurrency for multiple concurrent write jobs.\nEach job writes data at its own pace and generates a new snapshot based on the current snapshot by applying incremental files (deleting or adding files) at the time of committing.\nThere may be two types of commit failures here:\n Snapshot conflict: the snapshot id has been preempted, the table has generated a new snapshot from another job. OK, let\u0026rsquo;s commit again. Files conflict: The file that this job wants to delete has been deleted by another jobs. At this point, the job can only fail. (For streaming jobs, it will fail and restart, intentionally failover once)  Snapshot conflict #  Paimon\u0026rsquo;s snapshot ID is unique, so as long as the job writes its snapshot file to the file system, it is considered successful.\nPaimon uses the file system\u0026rsquo;s renaming mechanism to commit snapshots, which is secure for HDFS as it ensures transactional and atomic renaming.\nBut for object storage such as OSS and S3, their 'RENAME' does not have atomic semantic. We need to configure Hive or jdbc metastore and enable 'lock.enabled' option for the catalog. Otherwise, there may be a chance of losing the snapshot.\nFiles conflict #  When Paimon commits a file deletion (which is only a logical deletion), it checks for conflicts with the latest snapshot. If there are conflicts (which means the file has been logically deleted), it can no longer continue on this commit node, so it can only intentionally trigger a failover to restart, and the job will retrieve the latest status from the filesystem in the hope of resolving this conflict.\nPaimon will ensure that there is no data loss or duplication here, but if two streaming jobs are writing at the same time and there are conflicts, you will see that they are constantly restarting, which is not a good thing.\nThe essence of conflict lies in deleting files (logically), and deleting files is born from compaction, so as long as we close the compaction of the writing job (Set \u0026lsquo;write-only\u0026rsquo; to true) and start a separate job to do the compaction work, everything is very good.\nSee dedicated compaction job for more info.\n"});index.add({'id':37,'href':'/docs/0.9/project/contributing/','title':"Contributing",'section':"Project",'content':"Contributing #  Apache Paimon is developed by an open and friendly community. Everybody is cordially welcome to join the community and contribute to Apache Paimon. There are several ways to interact with the community and contribute to Paimon including asking questions, filing bug reports, proposing new features, joining discussions on the mailing lists, contributing code or documentation, improving website, testing release candidates and writing corresponding blog etc.\nWhat do you want to do? Contributing to Apache Paimon goes beyond writing code for the project. Below, we list different opportunities to help the project:\n  Area Further information      Report Bug To report a problem with Paimon, open Paimon’s issues.  Please give detailed information about the problem you encountered and, if possible, add a description that helps to reproduce the problem.    Contribute Code Read the Code Contribution Guide    Code Reviews Read the Code Review Guide    Release Version Releasing a new Paimon version.    Support Users Reply to questions on the user mailing list, check the latest issues in Issues for tickets which are actually user questions.     Spread the Word About Paimon Organize or attend a Paimon Meetup, contribute to the Paimon blog, share your conference, meetup or blog post on the dev@paimon.apache.org mailing list.     Any other question? Reach out to the dev@paimon.apache.org mailing list to get help!     Code Contribution Guide #  Apache Paimon is maintained, improved, and extended by code contributions of volunteers. We welcome contributions to Paimon.\nPlease feel free to ask questions at any time. Either send a mail to the Dev mailing list or comment on the issue you are working on.\n .contribute-grid { margin-bottom: 10px; display: flex; flex-direction: column; margin-left: -2px; margin-right: -2px; } .contribute-grid .column { margin-top: 4px; padding: 0 2px; } @media only screen and (min-width: 480px) { .contribute-grid { flex-direction: row; flex-wrap: wrap; } .contribute-grid .column { flex: 0 0 50%; } .contribute-grid .column { margin-top: 4px; } } @media only screen and (min-width: 960px) { .contribute-grid { flex-wrap: nowrap; } .contribute-grid .column { flex: 0 0 25%; } } .contribute-grid .panel { height: 100%; margin: 0; } .contribute-grid .panel-body { padding: 10px; } .contribute-grid h2 { margin: 0 0 10px 0; padding: 0; display: flex; align-items: flex-start; } .contribute-grid .number { margin-right: 0.25em; font-size: 1.5em; line-height: 0.9; }  1Discuss Create an Issue or mailing list discussion and reach consensus\nTo request an issue, please note that it is not just a \"please assign it to me\", you need to explain your understanding of the issue, and your design, and if possible, you need to provide your POC code.\n   2Implement Create the Pull Request and the approach agreed upon in the issue.\n1.Only create the PR if you are assigned to the issue. 2.Please associate an issue (if any), e.g. fix #123. 3.Please enable the actions of your own clone project.\n   3Review Work with the reviewer.\n1.Make sure no unrelated or unnecessary reformatting changes are included. 2.Please ensure that the test passing. 3.Please don't resolve conversation.\n   4Merge A committer of Paimon checks if the contribution fulfills the requirements and merges the code to the codebase.\n    Code Review Guide #  Every review needs to check the following six aspects. We encourage to check these aspects in order, to avoid spending time on detailed code quality reviews when formal requirements are not met or there is no consensus in the community to accept the change.\n1. Is the Contribution Well-Described? #  Check whether the contribution is sufficiently well-described to support a good review. Trivial changes and fixes do not need a long description. If the implementation is exactly according to a prior discussion on issue or the development mailing list, only a short reference to that discussion is needed.\nIf the implementation is different from the agreed approach in the consensus discussion, a detailed description of the implementation is required for any further review of the contribution.\n2. Does the Contribution Need Attention from some Specific Committers? #  Some changes require attention and approval from specific committers.\nIf the pull request needs specific attention, one of the tagged committers/contributors should give the final approval.\n3. Is the Overall Code Quality Good, Meeting Standard we Want to Maintain in Paimon? #   Does the code follow the right software engineering practices? Is the code correct, robust, maintainable, testable? Are the changes performance aware, when changing a performance sensitive part? Are the changes sufficiently covered by tests? Are the tests executing fast? If dependencies have been changed, were the NOTICE files updated?  Code guidelines can be found in the Flink Java Code Style and Quality Guide.\n4. Are the documentation updated? #  If the pull request introduces a new feature, the feature should be documented.\nBecome a Committer #  How to become a committer #  There is no strict protocol for becoming a committer. Candidates for new committers are typically people that are active contributors and community members. Candidates are suggested by current committers or PPMC members, and voted upon by the PPMC.\nIf you would like to become a committer, you should engage with the community and start contributing to Apache Paimon in any of the above ways. You might also want to talk to other committers and ask for their advice and guidance.\n  Community contributions include helping to answer user questions on the mailing list, verifying release candidates, giving talks, organizing community events, and other forms of evangelism and community building. The \u0026ldquo;Apache Way\u0026rdquo; has a strong focus on the project community, and committers can be recognized for outstanding community contributions even without any code contributions.\n  Code/technology contributions include contributed pull requests (patches), design discussions, reviews, testing, and other help in identifying and fixing bugs. Especially constructive and high quality design discussions, as well as helping other contributors, are strong indicators.\n  Identify promising candidates #  While the prior points give ways to identify promising candidates, the following are \u0026ldquo;must haves\u0026rdquo; for any committer candidate:\n  Being community minded: The candidate understands the meritocratic principles of community management. They do not always optimize for as much as possible personal contribution, but will help and empower others where it makes sense.\n  We trust that a committer candidate will use their write access to the repositories responsibly, and if in doubt, conservatively. It is important that committers are aware of what they know and what they don\u0026rsquo;t know. In doubt, committers should ask for a second pair of eyes rather than commit to parts that they are not well familiar with.\n  They have shown to be respectful towards other community members and constructive in discussions.\n  "});index.add({'id':38,'href':'/docs/0.9/maintenance/dedicated-compaction/','title':"Dedicated Compaction",'section':"Maintenance",'content':"Dedicated Compaction #  Paimon\u0026rsquo;s snapshot management supports writing with multiple writers.\nFor S3-like object store, its 'RENAME' does not have atomic semantic. We need to configure Hive metastore and enable 'lock.enabled' option for the catalog.  By default, Paimon supports concurrent writing to different partitions. A recommended mode is that streaming job writes records to Paimon\u0026rsquo;s latest partition; Simultaneously batch job (overwrite) writes records to the historical partition.\nSo far, everything works very well, but if you need multiple writers to write records to the same partition, it will be a bit more complicated. For example, you don\u0026rsquo;t want to use UNION ALL, you have multiple streaming jobs to write records to a 'partial-update' table. Please refer to the 'Dedicated Compaction Job' below.\nDedicated Compaction Job #  By default, Paimon writers will perform compaction as needed during writing records. This is sufficient for most use cases.\nCompaction will mark some data files as \u0026ldquo;deleted\u0026rdquo; (not really deleted, see expiring snapshots for more info). If multiple writers mark the same file, a conflict will occur when committing the changes. Paimon will automatically resolve the conflict, but this may result in job restarts.\nTo avoid these downsides, users can also choose to skip compactions in writers, and run a dedicated job only for compaction. As compactions are performed only by the dedicated job, writers can continuously write records without pausing and no conflicts will ever occur.\nTo skip compactions in writers, set the following table property to true.\n  Option Required Default Type Description     write-only No false Boolean If set to true, compactions and snapshot expiration will be skipped. This option is used along with dedicated compact jobs.    To run a dedicated job for compaction, follow these instructions.\nFlink Action Jar Run the following command to submit a compaction job for the table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  compact \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  [--partition \u0026lt;partition-name\u0026gt;] \\  [--table_conf \u0026lt;table_conf\u0026gt;] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] Example: compact table\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  compact \\  --warehouse s3:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --partition dt=20221126,hh=08 \\  --partition dt=20221127,hh=09 \\  --table_conf sink.parallelism=10 \\  --catalog_conf s3.endpoint=https://****.com \\  --catalog_conf s3.access-key=***** \\  --catalog_conf s3.secret-key=***** You can use -D execution.runtime-mode=batch or -yD execution.runtime-mode=batch (for the ON-YARN scenario) to control batch or streaming mode. If you submit a batch job, all current table files will be compacted. If you submit a streaming job, the job will continuously monitor new changes to the table and perform compactions as needed.\nFor more usage of the compact action, see\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  compact --help Flink SQL Run the following sql:\n-- compact table CALL sys.compact(`table` =\u0026gt; \u0026#39;default.T\u0026#39;); -- compact table with options CALL sys.compact(`table` =\u0026gt; \u0026#39;default.T\u0026#39;, `options` =\u0026gt; \u0026#39;sink.parallelism=4\u0026#39;); -- compact table partition CALL sys.compact(`table` =\u0026gt; \u0026#39;default.T\u0026#39;, `partitions` =\u0026gt; \u0026#39;p=0\u0026#39;); -- compact table partition with filter CALL sys.compact(`table` =\u0026gt; \u0026#39;default.T\u0026#39;, `where` =\u0026gt; \u0026#39;dt\u0026gt;10 and h\u0026lt;20\u0026#39;);  Similarly, the default is synchronous compaction, which may cause checkpoint timeouts. You can configure table_conf to use Asynchronous Compaction.  Database Compaction Job #  You can run the following command to submit a compaction job for multiple database.\nFlink Action Jar \u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  compact_database \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --including_databases \u0026lt;database-name|name-regular-expr\u0026gt; \\  [--including_tables \u0026lt;paimon-table-name|name-regular-expr\u0026gt;] \\  [--excluding_tables \u0026lt;paimon-table-name|name-regular-expr\u0026gt;] \\  [--mode \u0026lt;compact-mode\u0026gt;] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table_conf \u0026lt;paimon-table_conf\u0026gt; [--table_conf \u0026lt;paimon-table_conf\u0026gt; ...]]  --including_databases is used to specify which database is to be compacted. In compact mode, you need to specify a database name, in compact_database mode, you could specify multiple database, regular expression is supported. --including_tables is used to specify which source tables are to be compacted, you must use \u0026lsquo;|\u0026rsquo; to separate multiple tables, the format is databaseName.tableName, regular expression is supported. For example, specifying \u0026ldquo;\u0026ndash;including_tables db1.t1|db2.+\u0026rdquo; means to compact table \u0026lsquo;db1.t1\u0026rsquo; and all tables in the db2 database. --excluding_tables is used to specify which source tables are not to be compacted. The usage is same as \u0026ldquo;\u0026ndash;including_tables\u0026rdquo;. \u0026ldquo;\u0026ndash;excluding_tables\u0026rdquo; has higher priority than \u0026ldquo;\u0026ndash;including_tables\u0026rdquo; if you specified both. --mode is used to specify compaction mode. Possible values:  \u0026ldquo;divided\u0026rdquo; (the default mode if you haven\u0026rsquo;t specified one): start a sink for each table, the compaction of the new table requires restarting the job. \u0026ldquo;combined\u0026rdquo;: start a single combined sink for all tables, the new table will be automatically compacted.   --catalog_conf is the configuration for Paimon catalog. Each configuration should be specified in the format key=value. See here for a complete list of catalog configurations. --table_conf is the configuration for compaction. Each configuration should be specified in the format key=value. Pivotal configuration is listed below:     Key Default Type Description     continuous.discovery-interval 10 s Duration The discovery interval of continuous reading.   sink.parallelism (none) Integer Defines a custom parallelism for the sink. By default, if this option is not defined, the planner will derive the parallelism for each statement individually by also considering the global configuration.    You can use -D execution.runtime-mode=batch to control batch or streaming mode. If you submit a batch job, all current table files will be compacted. If you submit a streaming job, the job will continuously monitor new changes to the table and perform compactions as needed.\nIf you only want to submit the compaction job and don\u0026rsquo;t want to wait until the job is done, you should submit in detached mode.  You can set --mode combined to enable compacting newly added tables without restarting job.  Example1: compact database\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  compact_database \\  --warehouse s3:///path/to/warehouse \\  --including_databases test_db \\  --catalog_conf s3.endpoint=https://****.com \\  --catalog_conf s3.access-key=***** \\  --catalog_conf s3.secret-key=***** Example2: compact database in combined mode\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  compact_database \\  --warehouse s3:///path/to/warehouse \\  --including_databases test_db \\  --mode combined \\  --catalog_conf s3.endpoint=https://****.com \\  --catalog_conf s3.access-key=***** \\  --catalog_conf s3.secret-key=***** \\  --table_conf continuous.discovery-interval=***** For more usage of the compact_database action, see\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  compact_database --help Flink SQL Run the following sql:\nCALL sys.compact_database(\u0026#39;includingDatabases\u0026#39;) CALL sys.compact_database(\u0026#39;includingDatabases\u0026#39;, \u0026#39;mode\u0026#39;) CALL sys.compact_database(\u0026#39;includingDatabases\u0026#39;, \u0026#39;mode\u0026#39;, \u0026#39;includingTables\u0026#39;) CALL sys.compact_database(\u0026#39;includingDatabases\u0026#39;, \u0026#39;mode\u0026#39;, \u0026#39;includingTables\u0026#39;, \u0026#39;excludingTables\u0026#39;) CALL sys.compact_database(\u0026#39;includingDatabases\u0026#39;, \u0026#39;mode\u0026#39;, \u0026#39;includingTables\u0026#39;, \u0026#39;excludingTables\u0026#39;, \u0026#39;tableOptions\u0026#39;) -- example CALL sys.compact_database(\u0026#39;db1|db2\u0026#39;, \u0026#39;combined\u0026#39;, \u0026#39;table_.*\u0026#39;, \u0026#39;ignore\u0026#39;, \u0026#39;sink.parallelism=4\u0026#39;)  Sort Compact #  If your table is configured with dynamic bucket primary key table or append table , you can trigger a compact with specified column sort to speed up queries.\nFlink Action Jar \u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  -D execution.runtime-mode=batch \\  /path/to/paimon-flink-action-0.9.0.jar \\  compact \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  --order_strategy \u0026lt;orderType\u0026gt; \\  --order_by \u0026lt;col1,col2,...\u0026gt; \\  [--partition \u0026lt;partition-name\u0026gt;] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table_conf \u0026lt;paimon-table-dynamic-conf\u0026gt; [--table_conf \u0026lt;paimon-table-dynamic-conf\u0026gt;] ...] There are two new configuration in Sort Compact\n  Configuration Description     --order_strategy the order strategy now support \"zorder\" and \"hilbert\" and \"order\". For example: --order_strategy zorder   --order_by Specify the order columns. For example: --order_by col0, col1    The sort parallelism is the same as the sink parallelism, you can dynamically specify it by add conf --table_conf sink.parallelism=\u0026lt;value\u0026gt;.\nFlink SQL Run the following sql:\n-- sort compact table CALL sys.compact(`table` =\u0026gt; \u0026#39;default.T\u0026#39;, order_strategy =\u0026gt; \u0026#39;zorder\u0026#39;, order_by =\u0026gt; \u0026#39;a,b\u0026#39;)  Historical Partition Compact #  You can run the following command to submit a compaction job for partition which has not received any new data for a period of time. Small files in those partitions will be full compacted.\nThis feature now is only used in batch mode.  For Table #  This is for one table. Flink Action Jar \u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  -D execution.runtime-mode=batch \\  /path/to/paimon-flink-action-0.9.0.jar \\  compact \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  --partition_idle_time \u0026lt;partition-idle-time\u0026gt; \\  [--partition \u0026lt;partition-name\u0026gt;] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table_conf \u0026lt;paimon-table-dynamic-conf\u0026gt; [--table_conf \u0026lt;paimon-table-dynamic-conf\u0026gt;] ...] There are one new configuration in Historical Partition Compact\n --partition_idle_time: this is used to do a full compaction for partition which had not received any new data for \u0026lsquo;partition_idle_time\u0026rsquo;. And only these partitions will be compacted.  Flink SQL Run the following sql:\n-- history partition compact table CALL sys.compact(`table` =\u0026gt; \u0026#39;default.T\u0026#39;, \u0026#39;partition_idle_time\u0026#39; =\u0026gt; \u0026#39;1 d\u0026#39;)  For Databases #  This is for multiple tables in different databases. Flink Action Jar \u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  -D execution.runtime-mode=batch \\  /path/to/paimon-flink-action-0.9.0.jar \\  compact_database \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --including_databases \u0026lt;database-name|name-regular-expr\u0026gt; \\  --partition_idle_time \u0026lt;partition-idle-time\u0026gt; \\  [--including_tables \u0026lt;paimon-table-name|name-regular-expr\u0026gt;] \\  [--excluding_tables \u0026lt;paimon-table-name|name-regular-expr\u0026gt;] \\  [--mode \u0026lt;compact-mode\u0026gt;] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table_conf \u0026lt;paimon-table_conf\u0026gt; [--table_conf \u0026lt;paimon-table_conf\u0026gt; ...]] Example: compact historical partitions for tables in database\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  compact_database \\  --warehouse s3:///path/to/warehouse \\  --including_databases test_db \\  --partition_idle_time 1d \\  --catalog_conf s3.endpoint=https://****.com \\  --catalog_conf s3.access-key=***** \\  --catalog_conf s3.secret-key=***** Flink SQL Run the following sql:\n-- history partition compact table CALL sys.compact_database(\u0026#39;includingDatabases\u0026#39;, \u0026#39;mode\u0026#39;, \u0026#39;includingTables\u0026#39;, \u0026#39;excludingTables\u0026#39;, \u0026#39;tableOptions\u0026#39;, \u0026#39;partition_idle_time\u0026#39;) Example: compact historical partitions for tables in database\n-- history partition compact table CALL sys.compact_database(\u0026#39;test_db\u0026#39;, \u0026#39;combined\u0026#39;, \u0026#39;\u0026#39;, \u0026#39;\u0026#39;, \u0026#39;\u0026#39;, \u0026#39;1 d\u0026#39;)  "});index.add({'id':39,'href':'/docs/0.9/engines/doris/','title':"Doris",'section':"Engine Others",'content':"Doris #  This documentation is a guide for using Paimon in Doris.\n More details can be found in Apache Doris Website\n Version #  Paimon currently supports Apache Doris 2.0.6 and above.\nCreate Paimon Catalog #  Use CREATE CATALOG statement in Apache Doris to create Paimon Catalog.\nDoris support multi types of Paimon Catalogs. Here are some examples:\n-- HDFS based Paimon Catalog CREATE CATALOG `paimon_hdfs` PROPERTIES ( \u0026#34;type\u0026#34; = \u0026#34;paimon\u0026#34;, \u0026#34;warehouse\u0026#34; = \u0026#34;hdfs://172.21.0.1:8020/user/paimon\u0026#34;, \u0026#34;hadoop.username\u0026#34; = \u0026#34;hadoop\u0026#34; ); -- Aliyun OSS based Paimon Catalog CREATE CATALOG `paimon_oss` PROPERTIES ( \u0026#34;type\u0026#34; = \u0026#34;paimon\u0026#34;, \u0026#34;warehouse\u0026#34; = \u0026#34;oss://paimon-bucket/paimonoss\u0026#34;, \u0026#34;oss.endpoint\u0026#34; = \u0026#34;oss-cn-beijing.aliyuncs.com\u0026#34;, \u0026#34;oss.access_key\u0026#34; = \u0026#34;ak\u0026#34;, \u0026#34;oss.secret_key\u0026#34; = \u0026#34;sk\u0026#34; ); -- Hive Metastore based Paimon Catalog CREATE CATALOG `paimon_hms` PROPERTIES ( \u0026#34;type\u0026#34; = \u0026#34;paimon\u0026#34;, \u0026#34;paimon.catalog.type\u0026#34; = \u0026#34;hms\u0026#34;, \u0026#34;warehouse\u0026#34; = \u0026#34;hdfs://172.21.0.1:8020/user/zhangdong/paimon2\u0026#34;, \u0026#34;hive.metastore.uris\u0026#34; = \u0026#34;thrift://172.21.0.44:7004\u0026#34;, \u0026#34;hadoop.username\u0026#34; = \u0026#34;hadoop\u0026#34; ); See Apache Doris Website for more examples.\nAccess Paimon Catalog #    Query Paimon table with full qualified name\nSELECT * FROM paimon_hdfs.paimon_db.paimon_table;   Switch to Paimon Catalog and query\nSWITCH paimon_hdfs; USE paimon_db; SELECT * FROM paimon_table;   Query Optimization #    Read optimized for Primary Key Table\nDoris can utilize the Read optimized feature for Primary Key Table(release in Paimon 0.6), by reading base data files using native Parquet/ORC reader and delta file using JNI.\n  Deletion Vectors\nDoris(2.1.4+) natively supports Deletion Vectors(released in Paimon 0.8).\n  Doris to Paimon type mapping #    Doris Data Type Paimon Data Type Atomic Type     Boolean BooleanType true   TinyInt TinyIntType true   SmallInt SmallIntType true   Int IntType true   BigInt BigIntType true   Float FloatType true   Double DoubleType true   Varchar VarCharType true   Char CharType true   Binary VarBinaryType, BinaryType true   Decimal(precision, scale) DecimalType(precision, scale) true   Datetime TimestampType,LocalZonedTimestampType true   Date DateType true   Array ArrayType false   Map MapType false   Struct RowType false    "});index.add({'id':40,'href':'/docs/0.9/flink/cdc-ingestion/kafka-cdc/','title':"Kafka CDC",'section':"CDC Ingestion",'content':"Kafka CDC #  Prepare Kafka Bundled Jar #  flink-sql-connector-kafka-*.jar Supported Formats #  Flink provides several Kafka CDC formats: Canal Json, Debezium Json, Debezium Avro, Ogg Json, Maxwell Json and Normal Json. If a message in a Kafka topic is a change event captured from another database using the Change Data Capture (CDC) tool, then you can use the Paimon Kafka CDC. Write the INSERT, UPDATE, DELETE messages parsed into the paimon table.\n  Formats Supported     Canal CDC True   Debezium CDC True   Maxwell CDC True   OGG CDC True   JSON True    The JSON sources possibly missing some information. For example, Ogg and Maxwell format standards don\u0026rsquo;t contain field types; When you write JSON sources into Flink Kafka sink, it will only reserve data and row type and drop other information. The synchronization job will try best to handle the problem as follows:\n Usually, debezium-json contains \u0026lsquo;schema\u0026rsquo; field, from which Paimon will retrieve data types. Make sure your debezium json has this field, or Paimon will use \u0026lsquo;STRING\u0026rsquo; type. If missing field types, Paimon will use \u0026lsquo;STRING\u0026rsquo; type as default. If missing database name or table name, you cannot do database synchronization, but you can still do table synchronization. If missing primary keys, the job might create non primary key table. You can set primary keys when submit job in table synchronization.   Synchronizing Tables #  By using KafkaSyncTableAction in a Flink DataStream job or directly through flink run, users can synchronize one or multiple tables from Kafka\u0026rsquo;s one topic into one Paimon table.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  kafka_sync_table --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  [--partition_keys \u0026lt;partition_keys\u0026gt;] \\  [--primary_keys \u0026lt;primary-keys\u0026gt;] \\  [--type_mapping to-string] \\  [--computed_column \u0026lt;\u0026#39;column-name=expr-name(args[, ...])\u0026#39;\u0026gt; [--computed_column ...]] \\  [--kafka_conf \u0026lt;kafka-source-conf\u0026gt; [--kafka_conf \u0026lt;kafka-source-conf\u0026gt; ...]] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; ...]]    Configuration Description     --warehouse The path to Paimon warehouse.   --database The database name in Paimon catalog.   --table The Paimon table name.   --partition_keys The partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example \"dt,hh,mm\".   --primary_keys The primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example \"buyer_id,seller_id\".   --type_mapping It is used to specify how to map MySQL data type to Paimon type.\nSupported options:  \"tinyint1-not-bool\": maps MySQL TINYINT(1) to TINYINT instead of BOOLEAN. \"to-nullable\": ignores all NOT NULL constraints (except for primary keys). This is used to solve the problem that Flink cannot accept the MySQL 'ALTER TABLE ADD COLUMN column type NOT NULL DEFAULT x' operation.  \"to-string\": maps all MySQL types to STRING. \"char-to-string\": maps MySQL CHAR(length)/VARCHAR(length) types to STRING. \"longtext-to-bytes\": maps MySQL LONGTEXT types to BYTES. \"bigint-unsigned-to-bigint\": maps MySQL BIGINT UNSIGNED, BIGINT UNSIGNED ZEROFILL, SERIAL to BIGINT. You should ensure overflow won't occur when using this option.     --computed_column The definitions of computed columns. The argument field is from Kafka topic's table field name. See here for a complete list of configurations.    --kafka_conf The configuration for Flink Kafka sources. Each configuration should be specified in the format `key=value`. `properties.bootstrap.servers`, `topic/topic-pattern`, `properties.group.id`, and `value.format` are required configurations, others are optional.See its document for a complete list of configurations.   --catalog_conf The configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.   --table_conf The configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.    If the Paimon table you specify does not exist, this action will automatically create the table. Its schema will be derived from all specified Kafka topic\u0026rsquo;s tables,it gets the earliest non-DDL data parsing schema from topic. If the Paimon table already exists, its schema will be compared against the schema of all specified Kafka topic\u0026rsquo;s tables.\nExample 1:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  kafka_sync_table \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --partition_keys pt \\  --primary_keys pt,uid \\  --computed_column \u0026#39;_year=year(age)\u0026#39; \\  --kafka_conf properties.bootstrap.servers=127.0.0.1:9020 \\  --kafka_conf topic=order \\  --kafka_conf properties.group.id=123456 \\  --kafka_conf value.format=canal-json \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --table_conf changelog-producer=input \\  --table_conf sink.parallelism=4 If the kafka topic doesn\u0026rsquo;t contain message when you start the synchronization job, you must manually create the table before submitting the job. You can define the partition keys and primary keys only, and the left columns will be added by the synchronization job.\nNOTE: In this case you shouldn\u0026rsquo;t use \u0026ndash;partition_keys or \u0026ndash;primary_keys, because those keys are defined when creating the table and can not be modified. Additionally, if you specified computed columns, you should also define all the argument columns used for computed columns.\nExample 2: If you want to synchronize a table which has primary key \u0026lsquo;id INT\u0026rsquo;, and you want to compute a partition key \u0026lsquo;part=date_format(create_time,yyyy-MM-dd)\u0026rsquo;, you can create a such table first (the other columns can be omitted):\nCREATE TABLE test_db.test_table ( id INT, -- primary key  create_time TIMESTAMP, -- the argument of computed column part  part STRING, -- partition key  PRIMARY KEY (id, part) NOT ENFORCED ) PARTITIONED BY (part); Then you can submit synchronization job:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  kafka_sync_table \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --computed_column \u0026#39;part=date_format(create_time,yyyy-MM-dd)\u0026#39; \\  ... (other conf) Example 3: For some append data (such as log data), it can be treated as special CDC data with only INSERT operation type, so you can use \u0026lsquo;format=json\u0026rsquo; to synchronize such data to the Paimon table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  kafka_sync_table \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --partition_keys pt \\  --computed_column \u0026#39;pt=date_format(event_tm, yyyyMMdd)\u0026#39; \\  --kafka_conf properties.bootstrap.servers=127.0.0.1:9020 \\  --kafka_conf topic=test_log \\  --kafka_conf properties.group.id=123456 \\  --kafka_conf value.format=json \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf sink.parallelism=4 Synchronizing Databases #  By using KafkaSyncDatabaseAction in a Flink DataStream job or directly through flink run, users can synchronize the multi topic or one topic into one Paimon database.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  kafka_sync_database --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  [--table_prefix \u0026lt;paimon-table-prefix\u0026gt;] \\  [--table_suffix \u0026lt;paimon-table-suffix\u0026gt;] \\  [--including_tables \u0026lt;table-name|name-regular-expr\u0026gt;] \\  [--excluding_tables \u0026lt;table-name|name-regular-expr\u0026gt;] \\  [--type_mapping to-string] \\  [--partition_keys \u0026lt;partition_keys\u0026gt;] \\  [--primary_keys \u0026lt;primary-keys\u0026gt;] \\  [--kafka_conf \u0026lt;kafka-source-conf\u0026gt; [--kafka_conf \u0026lt;kafka-source-conf\u0026gt; ...]] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; ...]]    Configuration Description     --warehouse The path to Paimon warehouse.   --database The database name in Paimon catalog.   --ignore_incompatible It is default false, in this case, if MySQL table name exists in Paimon and their schema is incompatible,an exception will be thrown. You can specify it to true explicitly to ignore the incompatible tables and exception.   --table_prefix The prefix of all Paimon tables to be synchronized. For example, if you want all synchronized tables to have \"ods_\" as prefix, you can specify \"--table_prefix ods_\".   --table_suffix The suffix of all Paimon tables to be synchronized. The usage is same as \"--table_prefix\".   --including_tables It is used to specify which source tables are to be synchronized. You must use '|' to separate multiple tables.Because '|' is a special character, a comma is required, for example: 'a|b|c'.Regular expression is supported, for example, specifying \"--including_tables test|paimon.*\" means to synchronize table 'test' and all tables start with 'paimon'.   --excluding_tables It is used to specify which source tables are not to be synchronized. The usage is same as \"--including_tables\". \"--excluding_tables\" has higher priority than \"--including_tables\" if you specified both.   --type_mapping It is used to specify how to map MySQL data type to Paimon type.\nSupported options:  \"tinyint1-not-bool\": maps MySQL TINYINT(1) to TINYINT instead of BOOLEAN. \"to-nullable\": ignores all NOT NULL constraints (except for primary keys). This is used to solve the problem that Flink cannot accept the MySQL 'ALTER TABLE ADD COLUMN column type NOT NULL DEFAULT x' operation.  \"to-string\": maps all MySQL types to STRING. \"char-to-string\": maps MySQL CHAR(length)/VARCHAR(length) types to STRING. \"longtext-to-bytes\": maps MySQL LONGTEXT types to BYTES. \"bigint-unsigned-to-bigint\": maps MySQL BIGINT UNSIGNED, BIGINT UNSIGNED ZEROFILL, SERIAL to BIGINT. You should ensure overflow won't occur when using this option.     --partition_keys The partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example \"dt,hh,mm\". If the keys are not in source table, the sink table won't set partition keys.   --multiple_table_partition_keys The partition keys for each different Paimon table. If there are multiple partition keys, connect them with comma, for example --multiple_table_partition_keys tableName1=col1,col2.col3 --multiple_table_partition_keys tableName2=col4,col5.col6 --multiple_table_partition_keys tableName3=col7,col8.col9 If the keys are not in source table, the sink table won't set partition keys.   --primary_keys The primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example \"buyer_id,seller_id\". If the keys are not in source table, but the source table has primary keys, the sink table will use source table's primary keys. Otherwise, the sink table won't set primary keys.    --kafka_conf The configuration for Flink Kafka sources. Each configuration should be specified in the format `key=value`. `properties.bootstrap.servers`, `topic/topic-pattern`, `properties.group.id`, and `value.format` are required configurations, others are optional.See its document for a complete list of configurations.   --catalog_conf The configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.   --table_conf The configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.    This action will build a single combined sink for all tables. For each Kafka topic\u0026rsquo;s table to be synchronized, if the corresponding Paimon table does not exist, this action will automatically create the table, and its schema will be derived from all specified Kafka topic\u0026rsquo;s tables. If the Paimon table already exists and its schema is different from that parsed from Kafka record, this action will try to preform schema evolution.\nExample\nSynchronization from one Kafka topic to Paimon database.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  kafka_sync_database \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --kafka_conf properties.bootstrap.servers=127.0.0.1:9020 \\  --kafka_conf topic=order \\  --kafka_conf properties.group.id=123456 \\  --kafka_conf value.format=canal-json \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --table_conf changelog-producer=input \\  --table_conf sink.parallelism=4 Synchronization from multiple Kafka topics to Paimon database.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  kafka_sync_database \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --kafka_conf properties.bootstrap.servers=127.0.0.1:9020 \\  --kafka_conf topic=order\\;logistic_order\\;user \\  --kafka_conf properties.group.id=123456 \\  --kafka_conf value.format=canal-json \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --table_conf changelog-producer=input \\  --table_conf sink.parallelism=4 Additional kafka_config #  There are some useful options to build Flink Kafka Source, but they are not provided by flink-kafka-connector document. They are:\n  Key Default Type Description     schema.registry.url (none) String When configuring \"value.format=debezium-avro\" which requires using the Confluence schema registry model for Apache Avro serialization, you need to provide the schema registry URL.    "});index.add({'id':41,'href':'/docs/0.9/filesystems/oss/','title':"OSS",'section':"Filesystems",'content':"OSS #  Download paimon-oss-0.9.0.jar. Flink If you have already configured oss access through Flink (Via Flink FileSystem), here you can skip the following configuration.  Put paimon-oss-0.9.0.jar into lib directory of your Flink home, and create catalog:\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;oss://\u0026lt;bucket\u0026gt;/\u0026lt;path\u0026gt;\u0026#39;, \u0026#39;fs.oss.endpoint\u0026#39; = \u0026#39;oss-cn-hangzhou.aliyuncs.com\u0026#39;, \u0026#39;fs.oss.accessKeyId\u0026#39; = \u0026#39;xxx\u0026#39;, \u0026#39;fs.oss.accessKeySecret\u0026#39; = \u0026#39;yyy\u0026#39; ); Spark If you have already configured oss access through Spark (Via Hadoop FileSystem), here you can skip the following configuration.  Place paimon-oss-0.9.0.jar together with paimon-spark-0.9.0.jar under Spark\u0026rsquo;s jars directory, and start like\nspark-sql \\  --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\  --conf spark.sql.catalog.paimon.warehouse=oss://\u0026lt;bucket\u0026gt;/\u0026lt;path\u0026gt; \\  --conf spark.sql.catalog.paimon.fs.oss.endpoint=oss-cn-hangzhou.aliyuncs.com \\  --conf spark.sql.catalog.paimon.fs.oss.accessKeyId=xxx \\  --conf spark.sql.catalog.paimon.fs.oss.accessKeySecret=yyy Hive If you have already configured oss access through Hive (Via Hadoop FileSystem), here you can skip the following configuration.  NOTE: You need to ensure that Hive metastore can access oss.\nPlace paimon-oss-0.9.0.jar together with paimon-hive-connector-0.9.0.jar under Hive\u0026rsquo;s auxlib directory, and start like\nSET paimon.fs.oss.endpoint=oss-cn-hangzhou.aliyuncs.com; SET paimon.fs.oss.accessKeyId=xxx; SET paimon.fs.oss.accessKeySecret=yyy; And read table from hive metastore, table can be created by Flink or Spark, see Catalog with Hive Metastore\nSELECT * FROM test_table; SELECT COUNT(1) FROM test_table; Trino From version 0.8, paimon-trino uses trino filesystem as basic file read and write system. We strongly recommend you to use jindo-sdk in trino.\nYou can find How to config jindo sdk on trino here. Please note that:\n Use paimon to replace hive-hadoop2 when you decompress the plugin jar and find location to put in. You can specify the core-site.xml in paimon.properties on configuration hive.config.resources. Presto and Jindo use the same configuration method.   "});index.add({'id':42,'href':'/docs/0.9/append-table/query/','title':"Query",'section':"Table w/o PK",'content':"Query #  Data Skipping By Order #  Paimon by default records the maximum and minimum values of each field in the manifest file.\nIn the query, according to the WHERE condition of the query, according to the statistics in the manifest do files filtering, if the filtering effect is good, the query would have been minutes of the query will be accelerated to milliseconds to complete the execution.\nOften the data distribution is not always effective filtering, so if we can sort the data by the field in WHERE condition? You can take a look to Flink COMPACT Action or Flink COMPACT Procedure or Spark COMPACT Procedure.\nData Skipping By File Index #  You can use file index too, it filters files by index on the read side.\nCREATE TABLE \u0026lt;PAIMON_TABLE\u0026gt; (\u0026lt;COLUMN\u0026gt; \u0026lt;COLUMN_TYPE\u0026gt; , ...) WITH ( \u0026#39;file-index.bloom-filter.columns\u0026#39; = \u0026#39;c1,c2\u0026#39;, \u0026#39;file-index.bloom-filter.c1.items\u0026#39; = \u0026#39;200\u0026#39; ); Define file-index.bloom-filter.columns, Paimon will create its corresponding index file for each file. If the index file is too small, it will be stored directly in the manifest, or in the directory of the data file. Each data file corresponds to an index file, which has a separate file definition and can contain different types of indexes with multiple columns.\nData file index is an external index file corresponding to a certain data file. If the index file is too small, it will be stored directly in the manifest, otherwise in the directory of the data file. Each data file corresponds to an index file, which has a separate file definition and can contain different types of indexes with multiple columns.\nDifferent file index may be efficient in different scenario. For example bloom filter may speed up query in point lookup scenario. Using a bitmap may consume more space but can result in greater accuracy.\nCurrently, file index is only supported in append-only table.\nBloom Filter:\n file-index.bloom-filter.columns: specify the columns that need bloom filter index. file-index.bloom-filter.\u0026lt;column_name\u0026gt;.fpp to config false positive probability. file-index.bloom-filter.\u0026lt;column_name\u0026gt;.items to config the expected distinct items in one data file.  Bitmap:\n file-index.bitmap.columns: specify the columns that need bitmap index.  More filter types will be supported\u0026hellip;\nIf you want to add file index to existing table, without any rewrite, you can use rewrite_file_index procedure. Before we use the procedure, you should config appropriate configurations in target table. You can use ALTER clause to config file-index.\u0026lt;filter-type\u0026gt;.columns to the table.\nHow to invoke: see flink procedures\n"});index.add({'id':43,'href':'/docs/0.9/concepts/spec/snapshot/','title':"Snapshot",'section':"Specification",'content':"Snapshot #  Each commit generates a snapshot file, and the version of the snapshot file starts from 1 and must be continuous. EARLIEST and LATEST are hint files at the beginning and end of the snapshot list, and they can be inaccurate. When hint files are inaccurate, the read will scan all snapshot files to determine the beginning and end.\nwarehouse └── default.db └── my_table ├── snapshot ├── EARLIEST ├── LATEST ├── snapshot-1 ├── snapshot-2 └── snapshot-3 Writing commit will preempt the next snapshot id, and once the snapshot file is successfully written, this commit will be visible.\nSnapshot File is JSON, it includes:\n version: Snapshot file version, current is 3. id: snapshot id, same to file name. schemaId: the corresponding schema version for this commit. baseManifestList: a manifest list recording all changes from the previous snapshots. deltaManifestList: a manifest list recording all new changes occurred in this snapshot. changelogManifestList: a manifest list recording all changelog produced in this snapshot, null if no changelog is produced. indexManifest: a manifest recording all index files of this table, null if no table index file. commitUser: usually generated by UUID, it is used for recovery of streaming writes, one stream write job with one user. commitIdentifier: transaction id corresponding to streaming write, each transaction may result in multiple commits for different commitKinds. commitKind: type of changes in this snapshot, including append, compact, overwrite and analyze. timeMillis: commit time millis. totalRecordCount: record count of all changes occurred in this snapshot. deltaRecordCount: record count of all new changes occurred in this snapshot. changelogRecordCount: record count of all changelog produced in this snapshot. watermark: watermark for input records, from Flink watermark mechanism, Long.MIN_VALUE if there is no watermark. statistics: stats file name for statistics of this table.  "});index.add({'id':44,'href':'/docs/0.9/primary-key-table/table-mode/','title':"Table Mode",'section':"Table with PK",'content':"Table Mode #  The file structure of the primary key table is roughly shown in the above figure. The table or partition contains multiple buckets, and each bucket is a separate LSM tree structure that contains multiple files.\nThe writing process of LSM is roughly as follows: Flink checkpoint flush L0 files, and trigger a compaction as needed to merge the data. According to the different processing ways during writing, there are three modes:\n MOR (Merge On Read): Default mode, only minor compactions are performed, and merging are required for reading. COW (Copy On Write): Using 'full-compaction.delta-commits' = '1', full compaction will be synchronized, which means the merge is completed on write. MOW (Merge On Write): Using 'deletion-vectors.enabled' = 'true', in writing phase, LSM will be queried to generate the deletion vector file for the data file, which directly filters out unnecessary lines during reading.  The Merge On Write mode is recommended for general primary key tables (merge-engine is default deduplicate).\nMerge On Read #  MOR is the default mode of primary key table.\nWhen the mode is MOR, it is necessary to merge all files for reading, as all files are ordered and undergo multi way merging, which includes a comparison calculation of the primary key.\nThere is an obvious issue here, where a single LSM tree can only have a single thread to read, so the read parallelism is limited. If the amount of data in the bucket is too large, it can lead to poor read performance. So in order to read performance, it is recommended to analyze the query requirements table and set the data volume in the bucket to be between 200MB and 1GB. But if the bucket is too small, there will be a lot of small file reads and writes, causing pressure on the file system.\nIn addition, due to the merging process, Filter based data skipping cannot be performed on non primary key columns, otherwise new data will be filtered out, resulting in incorrect old data.\n Write performance: very good. Read performance: not so good.  Copy On Write #  ALTER TABLE orders SET (\u0026#39;full-compaction.delta-commits\u0026#39; = \u0026#39;1\u0026#39;); Set full-compaction.delta-commits to 1, which means that every write will be fully merged, and all data will be merged to the highest level. When reading, merging is not necessary at this time, and the reading performance is the highest. But every write requires full merging, and write amplification is very severe.\n Write performance: very bad. Read performance: very good.  Merge On Write #  ALTER TABLE orders SET (\u0026#39;deletion-vectors.enabled\u0026#39; = \u0026#39;true\u0026#39;); Thanks to Paimon\u0026rsquo;s LSM structure, it has the ability to be queried by primary key. We can generate deletion vectors files when writing, representing which data in the file has been deleted. This directly filters out unnecessary rows during reading, which is equivalent to merging and does not affect reading performance.\nA simple example just like:\nUpdates data by deleting old record first and then adding new one.\n Write performance: good. Read performance: good.  Visibility guarantee: Tables in deletion vectors mode, the files with level 0 will only be visible after compaction. So by default, compaction is synchronous, and if asynchronous is turned on, there may be delays in the data.  MOR Read Optimized #  If you don\u0026rsquo;t want to use Deletion Vectors mode, you want to query fast enough in MOR mode, but can only find older data, you can also:\n Configure \u0026lsquo;compaction.optimization-interval\u0026rsquo; when writing data. Query from read-optimized system table. Reading from results of optimized files avoids merging records with the same key, thus improving reading performance.  You can flexibly balance query performance and data latency when reading.\n"});index.add({'id':45,'href':'/docs/0.9/append-table/','title':"Table w/o PK",'section':"Apache Paimon",'content':""});index.add({'id':46,'href':'/docs/0.9/flink/','title':"Engine Flink",'section':"Apache Paimon",'content':""});index.add({'id':47,'href':'/docs/0.9/primary-key-table/merge-engine/first-row/','title':"First Row",'section':"Merge Engine",'content':"First Row #  By specifying 'merge-engine' = 'first-row', users can keep the first row of the same primary key. It differs from the deduplicate merge engine that in the first-row merge engine, it will generate insert only changelog.\n You can not specify sequence.field. Not accept DELETE and UPDATE_BEFORE message. You can config ignore-delete to ignore these two kinds records. Visibility guarantee: Tables with First Row engine, the files with level 0 will only be visible after compaction. So by default, compaction is synchronous, and if asynchronous is turned on, there may be delays in the data.   This is of great help in replacing log deduplication in streaming computation.\n"});index.add({'id':48,'href':'/docs/0.9/engines/hive/','title':"Hive",'section':"Engine Others",'content':"Hive #  This documentation is a guide for using Paimon in Hive.\nVersion #  Paimon currently supports Hive 3.1, 2.3, 2.2, 2.1 and 2.1-cdh-6.3.\nExecution Engine #  Paimon currently supports MR and Tez execution engine for Hive Read, and MR execution engine for Hive Write. Note If you use beeline, please restart the hive cluster.\nInstallation #  Download the jar file with corresponding version.\n    Jar     Hive 3.1 paimon-hive-connector-3.1-0.9.0.jar   Hive 2.3 paimon-hive-connector-2.3-0.9.0.jar   Hive 2.2 paimon-hive-connector-2.2-0.9.0.jar   Hive 2.1 paimon-hive-connector-2.1-0.9.0.jar   Hive 2.1-cdh-6.3 paimon-hive-connector-2.1-cdh-6.3-0.9.0.jar    You can also manually build bundled jar from the source code.\nTo build from source code, clone the git repository.\nBuild bundled jar with the following command. mvn clean install -DskipTests\nYou can find Hive connector jar in ./paimon-hive/paimon-hive-connector-\u0026lt;hive-version\u0026gt;/target/paimon-hive-connector-\u0026lt;hive-version\u0026gt;-0.9.0.jar.\nThere are several ways to add this jar to Hive.\n You can create an auxlib folder under the root directory of Hive, and copy paimon-hive-connector-0.9.0.jar into auxlib. You can also copy this jar to a path accessible by Hive, then use add jar /path/to/paimon-hive-connector-0.9.0.jar to enable paimon support in Hive. Note that this method is not recommended. If you\u0026rsquo;re using the MR execution engine and running a join statement, you may be faced with the exception org.apache.hive.com.esotericsoftware.kryo.kryoexception: unable to find class.  NOTE:\n If you are using HDFS :  Make sure that the environment variable HADOOP_HOME or HADOOP_CONF_DIR is set. You can set paimon.hadoop-load-default-config =false to disable loading the default value from core-default.xml、hdfs-default.xml, which may lead smaller size for split.   With hive cbo, it may lead to some incorrect query results, such as to query struct type with not null predicate, you can disable the cbo by set hive.cbo.enable=false; command.  Hive SQL: access Paimon Tables already in Hive metastore #  Run the following Hive SQL in Hive CLI to access the created table.\n-- Assume that paimon-hive-connector-\u0026lt;hive-version\u0026gt;-0.9.0.jar is already in auxlib directory. -- List tables in Hive -- (you might need to switch to \u0026#34;default\u0026#34; database if you\u0026#39;re not there by default)  SHOW TABLES; /* OK test_table */ -- Read records from test_table  SELECT a, b FROM test_table ORDER BY a; /* OK 1\tTable 2\tStore */ -- Insert records into test table -- Limitations: -- Only support INSERT INTO, not support INSERT OVERWRITE -- It is recommended to write to a non primary key table -- Writing to a primary key table may result in a large number of small files  INSERT INTO test_table VALUES (3, \u0026#39;Paimon\u0026#39;); SELECT a, b FROM test_table ORDER BY a; /* OK 1\tTable 2\tStore 3\tPaimon */ -- time travel  SET paimon.scan.snapshot-id=1; SELECT a, b FROM test_table ORDER BY a; /* OK 1\tTable 2\tStore 3\tPaimon */ SET paimon.scan.snapshot-id=null; Hive SQL: create new Paimon Tables #  You can create new paimon tables in Hive. Run the following Hive SQL in Hive CLI.\n-- Assume that paimon-hive-connector-0.9.0.jar is already in auxlib directory. -- Let\u0026#39;s create a new paimon table.  SET hive.metastore.warehouse.dir=warehouse_path; CREATE TABLE hive_test_table( a INT COMMENT \u0026#39;The a field\u0026#39;, b STRING COMMENT \u0026#39;The b field\u0026#39; ) STORED BY \u0026#39;org.apache.paimon.hive.PaimonStorageHandler\u0026#39;; Hive SQL: access Paimon Tables by External Table #  To access existing paimon table, you can also register them as external tables in Hive. Run the following Hive SQL in Hive CLI.\n-- Assume that paimon-hive-connector-0.9.0.jar is already in auxlib directory. -- Let\u0026#39;s use the test_table created in the above section. -- To create an external table, you don\u0026#39;t need to specify any column or table properties. -- Pointing the location to the path of table is enough.  CREATE EXTERNAL TABLE external_test_table STORED BY \u0026#39;org.apache.paimon.hive.PaimonStorageHandler\u0026#39; LOCATION \u0026#39;/path/to/table/store/warehouse/default.db/test_table\u0026#39;; -- In addition to the way setting location above, you can also place the location setting in TBProperties -- to avoid Hive accessing Paimon\u0026#39;s location through its own file system when creating tables. -- This method is effective in scenarios using Object storage,such as s3.  CREATE EXTERNAL TABLE external_test_table STORED BY \u0026#39;org.apache.paimon.hive.PaimonStorageHandler\u0026#39; TBLPROPERTIES ( \u0026#39;paimon_location\u0026#39; =\u0026#39;s3://xxxxx/path/to/table/store/warehouse/default.db/test_table\u0026#39; ); -- Read records from external_test_table  SELECT a, b FROM external_test_table ORDER BY a; /* OK 1\tTable 2\tStore */ -- Insert records into test table  INSERT INTO external_test_table VALUES (3, \u0026#39;Paimon\u0026#39;); SELECT a, b FROM external_test_table ORDER BY a; /* OK 1\tTable 2\tStore 3\tPaimon */ Hive Type Conversion #  This section lists all supported type conversion between Hive and Paimon. All Hive\u0026rsquo;s data types are available in package org.apache.hadoop.hive.serde2.typeinfo.\n  Hive Data Type Paimon Data Type Atomic Type     StructTypeInfo RowType false   MapTypeInfo MapType false   ListTypeInfo ArrayType false   PrimitiveTypeInfo(\"boolean\") BooleanType true   PrimitiveTypeInfo(\"tinyint\") TinyIntType true   PrimitiveTypeInfo(\"smallint\") SmallIntType true   PrimitiveTypeInfo(\"int\") IntType true   PrimitiveTypeInfo(\"bigint\") BigIntType true   PrimitiveTypeInfo(\"float\") FloatType true   PrimitiveTypeInfo(\"double\") DoubleType true   CharTypeInfo(length) CharType(length) true   PrimitiveTypeInfo(\"string\") VarCharType(VarCharType.MAX_LENGTH) true   VarcharTypeInfo(length) VarCharType(length), length is less than VarCharType.MAX_LENGTH true   PrimitiveTypeInfo(\"date\") DateType true   PrimitiveTypeInfo(\"timestamp\") TimestampType true   DecimalTypeInfo(precision, scale) DecimalType(precision, scale) true   PrimitiveTypeInfo(\"binary\") VarBinaryType, BinaryType true    "});index.add({'id':49,'href':'/docs/0.9/maintenance/manage-snapshots/','title':"Manage Snapshots",'section':"Maintenance",'content':"Manage Snapshots #  This section will describe the management and behavior related to snapshots.\nExpire Snapshots #  Paimon writers generate one or two snapshot per commit. Each snapshot may add some new data files or mark some old data files as deleted. However, the marked data files are not truly deleted because Paimon also supports time traveling to an earlier snapshot. They are only deleted when the snapshot expires.\nCurrently, expiration is automatically performed by Paimon writers when committing new changes. By expiring old snapshots, old data files and metadata files that are no longer used can be deleted to release disk space.\nSnapshot expiration is controlled by the following table properties.\n  Option Required Default Type Description     snapshot.time-retained No 1 h Duration The maximum time of completed snapshots to retain.   snapshot.num-retained.min No 10 Integer The minimum number of completed snapshots to retain. Should be greater than or equal to 1.   snapshot.num-retained.max No Integer.MAX_VALUE Integer The maximum number of completed snapshots to retain. Should be greater than or equal to the minimum number.   snapshot.expire.execution-mode No sync Enum Specifies the execution mode of expire.   snapshot.expire.limit No 10 Integer The maximum number of snapshots allowed to expire at a time.    When the number of snapshots is less than snapshot.num-retained.min, no snapshots will be expired(even the condition snapshot.time-retained meet), after which snapshot.num-retained.max and snapshot.time-retained will be used to control the snapshot expiration until the remaining snapshot meets the condition.\nThe following example show more details(snapshot.num-retained.min is 2, snapshot.time-retained is 1h, snapshot.num-retained.max is 5):\n snapshot item is described using tuple (snapshotId, corresponding time)\n    New Snapshots All snapshots after expiration check explanation      (snapshots-1, 2023-07-06 10:00)   (snapshots-1, 2023-07-06 10:00)   No snapshot expired     (snapshots-2, 2023-07-06 10:20)   (snapshots-1, 2023-07-06 10:00) (snapshots-2, 2023-07-06 10:20)   No snapshot expired     (snapshots-3, 2023-07-06 10:40)   (snapshots-1, 2023-07-06 10:00) (snapshots-2, 2023-07-06 10:20) (snapshots-3, 2023-07-06 10:40)   No snapshot expired     (snapshots-4, 2023-07-06 11:00)   (snapshots-1, 2023-07-06 10:00) (snapshots-2, 2023-07-06 10:20) (snapshots-3, 2023-07-06 10:40) (snapshots-4, 2023-07-06 11:00)   No snapshot expired     (snapshots-5, 2023-07-06 11:20)   (snapshots-2, 2023-07-06 10:20) (snapshots-3, 2023-07-06 10:40) (snapshots-4, 2023-07-06 11:00) (snapshots-5, 2023-07-06 11:20)   snapshot-1 was expired because the condition `snapshot.time-retained` is not met     (snapshots-6, 2023-07-06 11:30)   (snapshots-3, 2023-07-06 10:40) (snapshots-4, 2023-07-06 11:00) (snapshots-5, 2023-07-06 11:20) (snapshots-6, 2023-07-06 11:30)   snapshot-2 was expired because the condition `snapshot.time-retained` is not met     (snapshots-7, 2023-07-06 11:35)   (snapshots-3, 2023-07-06 10:40) (snapshots-4, 2023-07-06 11:00) (snapshots-5, 2023-07-06 11:20) (snapshots-6, 2023-07-06 11:30) (snapshots-7, 2023-07-06 11:35)   No snapshot expired     (snapshots-8, 2023-07-06 11:36)   (snapshots-4, 2023-07-06 11:00) (snapshots-5, 2023-07-06 11:20) (snapshots-6, 2023-07-06 11:30) (snapshots-7, 2023-07-06 11:35) (snapshots-8, 2023-07-06 11:36)   snapshot-3 was expired because the condition `snapshot.num-retained.max` is not met     Please note that too short retain time or too small retain number may result in:\n Batch queries cannot find the file. For example, the table is relatively large and the batch query takes 10 minutes to read, but the snapshot from 10 minutes ago expires, at which point the batch query will read a deleted snapshot. Streaming reading jobs on table files fail to restart. When the job restarts, the snapshot it recorded may have expired. (You can use Consumer Id to protect streaming reading in a small retain time of snapshot expiration).  By default, paimon will delete expired snapshots synchronously. When there are too many files that need to be deleted, they may not be deleted quickly and back-pressured to the upstream operator. To avoid this situation, users can use asynchronous expiration mode by setting snapshot.expire.execution-mode to async.\nRollback to Snapshot #  Rollback a table to a specific snapshot ID.\nFlink Run the following command:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  rollback_to \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  --version \u0026lt;snapshot-id\u0026gt; \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] Java API import org.apache.paimon.table.Table; public class RollbackTo { public static void main(String[] args) { // before rollback:  // snapshot-3  // snapshot-4  // snapshot-5  // snapshot-6  // snapshot-7  table.rollbackTo(5); // after rollback:  // snapshot-3  // snapshot-4  // snapshot-5  } } Spark Run the following sql:\nCALL rollback(table =\u0026gt; \u0026#39;test.T\u0026#39;, version =\u0026gt; \u0026#39;2\u0026#39;);  Remove Orphan Files #  Paimon files are deleted physically only when expiring snapshots. However, it is possible that some unexpected errors occurred when deleting files, so that there may exist files that are not used by Paimon snapshots (so-called \u0026ldquo;orphan files\u0026rdquo;). You can submit a remove_orphan_files job to clean them:\nSpark SQL/Flink SQL CALL sys.remove_orphan_files(table =\u0026gt; \u0026#34;my_db.my_table\u0026#34;, [older_than =\u0026gt; \u0026#34;2023-10-31 12:00:00\u0026#34;]) CALL sys.remove_orphan_files(table =\u0026gt; \u0026#34;my_db.*\u0026#34;, [older_than =\u0026gt; \u0026#34;2023-10-31 12:00:00\u0026#34;]) Flink Action \u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  remove_orphan_files \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  [--older_than \u0026lt;timestamp\u0026gt;] \\  [--dry_run \u0026lt;false/true\u0026gt;] To avoid deleting files that are newly added by other writing jobs, this action only deletes orphan files older than 1 day by default. The interval can be modified by --older_than. For example:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  remove_orphan_files \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table T \\  --older_than \u0026#39;2023-10-31 12:00:00\u0026#39; The table can be * to clean all tables in the database.\n "});index.add({'id':50,'href':'/docs/0.9/concepts/spec/manifest/','title':"Manifest",'section':"Specification",'content':"Manifest #  Manifest List #  ├── manifest └── manifest-list-51c16f7b-421c-4bc0-80a0-17677f343358-1 Manifest List includes meta of several manifest files. Its name contains UUID, it is a avro file, the schema is:\n fileName: manifest file name. fileSize: manifest file size. numAddedFiles: number added files in manifest. numDeletedFiles: number deleted files in manifest. partitionStats: partition stats, the minimum and maximum values of partition fields in this manifest are beneficial for skipping certain manifest files during queries, it is a SimpleStats. schemaId: schema id when writing this manifest file.  Manifest #  Manifest includes meta of several data files or changelog files or table-index files. Its name contains UUID, it is an avro file.\nThe changes of the file are saved in the manifest, and the file can be added or deleted. Manifests should be in an orderly manner, and the same file may be added or deleted multiple times. The last version should be read. This design can make commit lighter to support file deletion generated by compaction.\nData Manifest #  Data Manifest includes meta of several data files or changelog files.\n├── manifest └── manifest-6758823b-2010-4d06-aef0-3b1b597723d6-0 The schema is:\n kind: ADD or DELETE, partition: partition spec, a BinaryRow. bucket: bucket of this file. totalBuckets: total buckets when write this file, it is used for verification after bucket changes. file: data file meta.  The data file meta is:\n fileName: file name. fileSize: file size. rowCount: total number of rows (including add \u0026amp; delete) in this file. minKey: the minimum key of this file. maxKey: the maximum key of this file. keyStats: the statistics of the key. valueStats: the statistics of the value. minSequenceNumber: the minimum sequence number. maxSequenceNumber: the maximum sequence number. schemaId: schema id when write this file. level: level of this file, in LSM. extraFiles: extra files for this file, for example, data file index file. creationTime: creation time of this file. deleteRowCount: rowCount = addRowCount + deleteRowCount. embeddedIndex: if data file index is too small, store the index in manifest.  Index Manifest #  Index Manifest includes meta of several table-index files.\n├── manifest └── index-manifest-5d670043-da25-4265-9a26-e31affc98039-0 The schema is:\n kind: ADD or DELETE, partition: partition spec, a BinaryRow. bucket: bucket of this file. indexFile: index file meta.  The index file meta is:\n indexType: string, \u0026ldquo;HASH\u0026rdquo; or \u0026ldquo;DELETION_VECTORS\u0026rdquo;. fileName: file name. fileSize: file size. rowCount: total number of rows. deletionVectorsRanges: Metadata only used by \u0026ldquo;DELETION_VECTORS\u0026rdquo;, Stores offset and length of each data file, The schema is ARRAY\u0026lt;ROW\u0026lt;f0: STRING, f1: INT, f2: INT\u0026gt;\u0026gt;.  "});index.add({'id':51,'href':'/docs/0.9/primary-key-table/merge-engine/','title':"Merge Engine",'section':"Table with PK",'content':""});index.add({'id':52,'href':'/docs/0.9/flink/cdc-ingestion/mongo-cdc/','title':"Mongo CDC",'section':"CDC Ingestion",'content':"Mongo CDC #  Prepare MongoDB Bundled Jar #  flink-sql-connector-mongodb-cdc-*.jar only cdc 3.1+ is supported\nSynchronizing Tables #  By using MongoDBSyncTableAction in a Flink DataStream job or directly through flink run, users can synchronize one collection from MongoDB into one Paimon table.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  mongodb_sync_table --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  [--partition_keys \u0026lt;partition_keys\u0026gt;] \\  [--computed_column \u0026lt;\u0026#39;column-name=expr-name(args[, ...])\u0026#39;\u0026gt; [--computed_column ...]] \\  [--mongodb_conf \u0026lt;mongodb-cdc-source-conf\u0026gt; [--mongodb_conf \u0026lt;mongodb-cdc-source-conf\u0026gt; ...]] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; ...]]    Configuration Description     --warehouse The path to Paimon warehouse.   --database The database name in Paimon catalog.   --table The Paimon table name.   --partition_keys The partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example \"dt,hh,mm\".   --computed_column The definitions of computed columns. The argument field is from MongoDB collection field name. See here for a complete list of configurations.    --mongodb_conf The configuration for Flink CDC MongoDB sources. Each configuration should be specified in the format \"key=value\". hosts, username, password, database and collection are required configurations, others are optional. See its document for a complete list of configurations.   --catalog_conf The configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.   --table_conf The configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.    Here are a few points to take note of:\n The mongodb_conf introduces the schema.start.mode parameter on top of the MongoDB CDC source configuration.schema.start.mode provides two modes: dynamic (default) and specified. In dynamic mode, MongoDB schema information is parsed at one level, which forms the basis for schema change evolution. In specified mode, synchronization takes place according to specified criteria. This can be done by configuring field.name to specify the synchronization fields and parser.path to specify the JSON parsing path for those fields. The difference between the two is that the specify mode requires the user to explicitly identify the fields to be used and create a mapping table based on those fields. Dynamic mode, on the other hand, ensures that Paimon and MongoDB always keep the top-level fields consistent, eliminating the need to focus on specific fields. Further processing of the data table is required when using values from nested fields. The mongodb_conf introduces the default.id.generation parameter as an enhancement to the MongoDB CDC source configuration. The default.id.generation setting offers two distinct behaviors: when set to true and when set to false. When default.id.generation is set to true, the MongoDB CDC source adheres to the default _id generation strategy, which involves stripping the outer $oid nesting to provide a more straightforward identifier. This mode simplifies the _id representation, making it more direct and user-friendly. On the contrary, when default.id.generation is set to false, the MongoDB CDC source retains the original _id structure, without any additional processing. This mode offers users the flexibility to work with the raw _id format as provided by MongoDB, preserving any nested elements like $oid. The choice between the two hinges on the user\u0026rsquo;s preference: the former for a cleaner, simplified _id and the latter for a direct representation of MongoDB\u0026rsquo;s _id structure.    Operator Description     $ The root element to query. This starts all path expressions.   @ The current node being processed by a filter predicate.   * Wildcard. Available anywhere a name or numeric are required.   .. Deep scan. Available anywhere a name is required.   . Dot-notated child.   ['{name}' (, '{name}')] Bracket-notated child or children.   [{number} (, {number})] Bracket-notated child or children.   [start:end] Array index or indexes.   [?({expression})] Filter expression. Expression must evaluate to a boolean value.    Functions can be invoked at the tail end of a path - the input to a function is the output of the path expression. The function output is dictated by the function itself.\n  Function Description Output type     min() Provides the min value of an array of numbers. Double   max() Provides the max value of an array of numbers. Double   avg() Provides the average value of an array of numbers. Double   stddev() Provides the standard deviation value of an array of numbers Double   length() Provides the length of an array Integer   sum() Provides the sum value of an array of numbers. Double   keys() Provides the property keys (An alternative for terminal tilde ~) Set   concat(X) Provides a concatinated version of the path output with a new item. like input   append(X) add an item to the json path output array like input   append(X) add an item to the json path output array like input   first() Provides the first item of an array Depends on the array   last() Provides the last item of an array Depends on the array   index(X) Provides the item of an array of index: X, if the X is negative, take from backwards Depends on the array    Path Examples\n{ \u0026#34;store\u0026#34;: { \u0026#34;book\u0026#34;: [ { \u0026#34;category\u0026#34;: \u0026#34;reference\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Nigel Rees\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Sayings of the Century\u0026#34;, \u0026#34;price\u0026#34;: 8.95 }, { \u0026#34;category\u0026#34;: \u0026#34;fiction\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Evelyn Waugh\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Sword of Honour\u0026#34;, \u0026#34;price\u0026#34;: 12.99 }, { \u0026#34;category\u0026#34;: \u0026#34;fiction\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Herman Melville\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Moby Dick\u0026#34;, \u0026#34;isbn\u0026#34;: \u0026#34;0-553-21311-3\u0026#34;, \u0026#34;price\u0026#34;: 8.99 }, { \u0026#34;category\u0026#34;: \u0026#34;fiction\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;J. R. R. Tolkien\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;The Lord of the Rings\u0026#34;, \u0026#34;isbn\u0026#34;: \u0026#34;0-395-19395-8\u0026#34;, \u0026#34;price\u0026#34;: 22.99 } ], \u0026#34;bicycle\u0026#34;: { \u0026#34;color\u0026#34;: \u0026#34;red\u0026#34;, \u0026#34;price\u0026#34;: 19.95 } }, \u0026#34;expensive\u0026#34;: 10 }    JsonPath Result     $.store.book[*].author Provides the min value of an array of numbers.   $..author All authors.   $.store.* All things, both books and bicycles.   $.store..price Provides the standard deviation value of an array of numbers.   $..book[2] The third book.   $..book[-2] The second to last book.   $..book[0,1] The first two books.   $..book[:2] All books from index 0 (inclusive) until index 2 (exclusive).   $..book[1:2] All books from index 1 (inclusive) until index 2 (exclusive)   $..book[-2:] Last two books   $..book[2:] All books from index 2 (inclusive) to last   $..book[?(@.isbn)] All books with an ISBN number   $.store.book[?(@.price \u0026lt; 10)] All books in store cheaper than 10   $..book[?(@.price \u0026lt;= $['expensive'])] All books in store that are not \"expensive\"   $..book[?(@.author =~ /.*REES/i)] All books matching regex (ignore case)   $..* Give me every thing   $..book.length() The number of books      The synchronized table is required to have its primary key set as _id. This is because MongoDB\u0026rsquo;s change events are recorded before updates in messages. Consequently, we can only convert them into Flink\u0026rsquo;s UPSERT change log stream. The upstart stream demands a unique key, which is why we must declare _id as the primary key. Declaring other columns as primary keys is not feasible, as delete operations only encompass the _id and sharding key, excluding other keys and values.\n  MongoDB Change Streams are designed to return simple JSON documents without any data type definitions. This is because MongoDB is a document-oriented database, and one of its core features is the dynamic schema, where documents can contain different fields, and the data types of fields can be flexible. Therefore, the absence of data type definitions in Change Streams is to maintain this flexibility and extensibility. For this reason, we have set all field data types for synchronizing MongoDB to Paimon as String to address the issue of not being able to obtain data types.\n  If the Paimon table you specify does not exist, this action will automatically create the table. Its schema will be derived from MongoDB collection.\nExample 1: synchronize collection into one Paimon table\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  mongodb_sync_table \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --partition_keys pt \\  --computed_column \u0026#39;_year=year(age)\u0026#39; \\  --mongodb_conf hosts=127.0.0.1:27017 \\  --mongodb_conf username=root \\  --mongodb_conf password=123456 \\  --mongodb_conf database=source_db \\  --mongodb_conf collection=source_table1 \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --table_conf changelog-producer=input \\  --table_conf sink.parallelism=4 Example 2: Synchronize collection into a Paimon table according to the specified field mapping.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  mongodb_sync_table \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --partition_keys pt \\  --mongodb_conf hosts=127.0.0.1:27017 \\  --mongodb_conf username=root \\  --mongodb_conf password=123456 \\  --mongodb_conf database=source_db \\  --mongodb_conf collection=source_table1 \\  --mongodb_conf schema.start.mode=specified \\  --mongodb_conf field.name=_id,name,description \\  --mongodb_conf parser.path=$._id,$.name,$.description \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --table_conf changelog-producer=input \\  --table_conf sink.parallelism=4 Synchronizing Databases #  By using MongoDBSyncDatabaseAction in a Flink DataStream job or directly through flink run, users can synchronize the whole MongoDB database into one Paimon database.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  mongodb_sync_database --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  [--table_prefix \u0026lt;paimon-table-prefix\u0026gt;] \\  [--table_suffix \u0026lt;paimon-table-suffix\u0026gt;] \\  [--including_tables \u0026lt;mongodb-table-name|name-regular-expr\u0026gt;] \\  [--excluding_tables \u0026lt;mongodb-table-name|name-regular-expr\u0026gt;] \\  [--partition_keys \u0026lt;partition_keys\u0026gt;] \\  [--primary_keys \u0026lt;primary-keys\u0026gt;] \\  [--mongodb_conf \u0026lt;mongodb-cdc-source-conf\u0026gt; [--mongodb_conf \u0026lt;mongodb-cdc-source-conf\u0026gt; ...]] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; ...]]    Configuration Description     --warehouse The path to Paimon warehouse.   --database The database name in Paimon catalog.   --table_prefix The prefix of all Paimon tables to be synchronized. For example, if you want all synchronized tables to have \"ods_\" as prefix, you can specify \"--table_prefix ods_\".   --table_suffix The suffix of all Paimon tables to be synchronized. The usage is same as \"--table_prefix\".   --including_tables It is used to specify which source tables are to be synchronized. You must use '|' to separate multiple tables.Because '|' is a special character, a comma is required, for example: 'a|b|c'.Regular expression is supported, for example, specifying \"--including_tables test|paimon.*\" means to synchronize table 'test' and all tables start with 'paimon'.   --excluding_tables It is used to specify which source tables are not to be synchronized. The usage is same as \"--including_tables\". \"--excluding_tables\" has higher priority than \"--including_tables\" if you specified both.   --partition_keys The partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example \"dt,hh,mm\". If the keys are not in source table, the sink table won't set partition keys.   --primary_keys The primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example \"buyer_id,seller_id\". If the keys are not in source table, but the source table has primary keys, the sink table will use source table's primary keys. Otherwise, the sink table won't set primary keys.   --mongodb_conf The configuration for Flink CDC MongoDB sources. Each configuration should be specified in the format \"key=value\". hosts, username, password, database are required configurations, others are optional. See its document for a complete list of configurations.   --catalog_conf The configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.   --table_conf The configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.    All collections to be synchronized need to set _id as the primary key. For each MongoDB collection to be synchronized, if the corresponding Paimon table does not exist, this action will automatically create the table. Its schema will be derived from all specified MongoDB collection. If the Paimon table already exists, its schema will be compared against the schema of all specified MongoDB collection. Any MongoDB tables created after the commencement of the task will automatically be included.\nExample 1: synchronize entire database\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  mongodb_sync_database \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --mongodb_conf hosts=127.0.0.1:27017 \\  --mongodb_conf username=root \\  --mongodb_conf password=123456 \\  --mongodb_conf database=source_db \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --table_conf changelog-producer=input \\  --table_conf sink.parallelism=4 Example 2: Synchronize the specified table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ --fromSavepoint savepointPath \\ /path/to/paimon-flink-action-0.9.0.jar \\ mongodb_sync_database \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --mongodb_conf hosts=127.0.0.1:27017 \\ --mongodb_conf username=root \\ --mongodb_conf password=123456 \\ --mongodb_conf database=source_db \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --including_tables \u0026#39;product|user|address|order|custom\u0026#39; "});index.add({'id':53,'href':'/docs/0.9/filesystems/s3/','title':"S3",'section':"Filesystems",'content':"S3 #  Download paimon-s3-0.9.0.jar. Flink If you have already configured s3 access through Flink (Via Flink FileSystem), here you can skip the following configuration.  Put paimon-s3-0.9.0.jar into lib directory of your Flink home, and create catalog:\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;s3://\u0026lt;bucket\u0026gt;/\u0026lt;path\u0026gt;\u0026#39;, \u0026#39;s3.endpoint\u0026#39; = \u0026#39;your-endpoint-hostname\u0026#39;, \u0026#39;s3.access-key\u0026#39; = \u0026#39;xxx\u0026#39;, \u0026#39;s3.secret-key\u0026#39; = \u0026#39;yyy\u0026#39; ); Spark If you have already configured s3 access through Spark (Via Hadoop FileSystem), here you can skip the following configuration.  Place paimon-s3-0.9.0.jar together with paimon-spark-0.9.0.jar under Spark\u0026rsquo;s jars directory, and start like\nspark-sql \\  --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\  --conf spark.sql.catalog.paimon.warehouse=s3://\u0026lt;bucket\u0026gt;/\u0026lt;path\u0026gt; \\  --conf spark.sql.catalog.paimon.s3.endpoint=your-endpoint-hostname \\  --conf spark.sql.catalog.paimon.s3.access-key=xxx \\  --conf spark.sql.catalog.paimon.s3.secret-key=yyy Hive If you have already configured s3 access through Hive ((Via Hadoop FileSystem)), here you can skip the following configuration.  NOTE: You need to ensure that Hive metastore can access s3.\nPlace paimon-s3-0.9.0.jar together with paimon-hive-connector-0.9.0.jar under Hive\u0026rsquo;s auxlib directory, and start like\nSET paimon.s3.endpoint=your-endpoint-hostname; SET paimon.s3.access-key=xxx; SET paimon.s3.secret-key=yyy; And read table from hive metastore, table can be created by Flink or Spark, see Catalog with Hive Metastore\nSELECT * FROM test_table; SELECT COUNT(1) FROM test_table; Trino Paimon use shared trino filesystem as basic read and write system.\nPlease refer to Trino S3 to config s3 filesystem in trino.\n S3 Complaint Object Stores #  The S3 Filesystem also support using S3 compliant object stores such as MinIO, Tencent\u0026rsquo;s COS and IBM’s Cloud Object Storage. Just configure your endpoint to the provider of the object store service.\ns3.endpoint:your-endpoint-hostnameConfigure Path Style Access #  Some S3 compliant object stores might not have virtual host style addressing enabled by default, for example when using Standalone MinIO for testing purpose. In such cases, you will have to provide the property to enable path style access.\ns3.path.style.access:trueS3A Performance #  Tune Performance for S3AFileSystem.\nIf you encounter the following exception:\nCaused by: org.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool. Try to configure this in catalog options: fs.s3a.connection.maximum=1000.\n"});index.add({'id':54,'href':'/docs/0.9/concepts/spec/','title':"Specification",'section':"Concepts",'content':""});index.add({'id':55,'href':'/docs/0.9/flink/sql-query/','title':"SQL Query",'section':"Engine Flink",'content':"SQL Query #  Just like all other tables, Paimon tables can be queried with SELECT statement.\nBatch Query #  Paimon\u0026rsquo;s batch read returns all the data in a snapshot of the table. By default, batch reads return the latest snapshot.\n-- Flink SQL SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;batch\u0026#39;; Batch Time Travel #  Paimon batch reads with time travel can specify a snapshot or a tag and read the corresponding data.\nFlink (dynamic option) -- read the snapshot with id 1L SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.snapshot-id\u0026#39; = \u0026#39;1\u0026#39;) */; -- read the snapshot from specified timestamp in unix milliseconds SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.timestamp-millis\u0026#39; = \u0026#39;1678883047356\u0026#39;) */; -- read the snapshot from specified timestamp string ,it will be automatically converted to timestamp in unix milliseconds -- Supported formats include：yyyy-MM-dd, yyyy-MM-dd HH:mm:ss, yyyy-MM-dd HH:mm:ss.SSS, use default local time zone SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.timestamp\u0026#39; = \u0026#39;2023-12-09 23:09:12\u0026#39;) */; -- read tag \u0026#39;my-tag\u0026#39; SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.tag-name\u0026#39; = \u0026#39;my-tag\u0026#39;) */; -- read the snapshot from watermark, will match the first snapshot after the watermark SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.watermark\u0026#39; = \u0026#39;1678883047356\u0026#39;) */; Flink 1.18\u0026#43; Flink SQL supports time travel syntax after 1.18.\n-- read the snapshot from specified timestamp SELECT * FROM t FOR SYSTEM_TIME AS OF TIMESTAMP \u0026#39;2023-01-01 00:00:00\u0026#39;; -- you can also use some simple expressions (see flink document to get supported functions) SELECT * FROM t FOR SYSTEM_TIME AS OF TIMESTAMP \u0026#39;2023-01-01 00:00:00\u0026#39; + INTERVAL \u0026#39;1\u0026#39; DAY  Batch Incremental #  Read incremental changes between start snapshot (exclusive) and end snapshot.\nFor example:\n \u0026lsquo;5,10\u0026rsquo; means changes between snapshot 5 and snapshot 10. \u0026lsquo;TAG1,TAG3\u0026rsquo; means changes between TAG1 and TAG3.  -- incremental between snapshot ids SELECT * FROM t /*+ OPTIONS(\u0026#39;incremental-between\u0026#39; = \u0026#39;12,20\u0026#39;) */; -- incremental between snapshot time mills SELECT * FROM t /*+ OPTIONS(\u0026#39;incremental-between-timestamp\u0026#39; = \u0026#39;1692169000000,1692169900000\u0026#39;) */; By default, will scan changelog files for the table which produces changelog files. Otherwise, scan newly changed files. You can also force specifying 'incremental-between-scan-mode'.\nIn Batch SQL, the DELETE records are not allowed to be returned, so records of -D will be dropped. If you want see DELETE records, you can use audit_log table:\nSELECT * FROM t$audit_log /*+ OPTIONS(\u0026#39;incremental-between\u0026#39; = \u0026#39;12,20\u0026#39;) */; Streaming Query #  By default, Streaming read produces the latest snapshot on the table upon first startup, and continue to read the latest changes.\nPaimon by default ensures that your startup is properly processed with all data included.\nPaimon Source in Streaming mode is unbounded, like a queue that never ends.  -- Flink SQL SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;streaming\u0026#39;; You can also do streaming read without the snapshot data, you can use latest scan mode:\n-- Continuously reads latest changes without producing a snapshot at the beginning. SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.mode\u0026#39; = \u0026#39;latest\u0026#39;) */; Streaming Time Travel #  If you only want to process data for today and beyond, you can do so with partitioned filters:\nSELECT * FROM t WHERE dt \u0026gt; \u0026#39;2023-06-26\u0026#39;; If it\u0026rsquo;s not a partitioned table, or you can\u0026rsquo;t filter by partition, you can use Time travel\u0026rsquo;s stream read.\nFlink (dynamic option) -- read changes from snapshot id 1L SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.snapshot-id\u0026#39; = \u0026#39;1\u0026#39;) */; -- read changes from snapshot specified timestamp SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.timestamp-millis\u0026#39; = \u0026#39;1678883047356\u0026#39;) */; -- read snapshot id 1L upon first startup, and continue to read the changes SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.mode\u0026#39;=\u0026#39;from-snapshot-full\u0026#39;,\u0026#39;scan.snapshot-id\u0026#39; = \u0026#39;1\u0026#39;) */; Flink 1.18\u0026#43; Flink SQL supports time travel syntax after 1.18.\n-- read the snapshot from specified timestamp SELECT * FROM t FOR SYSTEM_TIME AS OF TIMESTAMP \u0026#39;2023-01-01 00:00:00\u0026#39;; -- you can also use some simple expressions (see flink document to get supported functions) SELECT * FROM t FOR SYSTEM_TIME AS OF TIMESTAMP \u0026#39;2023-01-01 00:00:00\u0026#39; + INTERVAL \u0026#39;1\u0026#39; DAY  Time travel\u0026rsquo;s stream read rely on snapshots, but by default, snapshot only retains data within 1 hour, which can prevent you from reading older incremental data. So, Paimon also provides another mode for streaming reads, scan.file-creation-time-millis, which provides a rough filtering to retain files generated after timeMillis.\nSELECT * FROM t /*+ OPTIONS(\u0026#39;scan.file-creation-time-millis\u0026#39; = \u0026#39;1678883047356\u0026#39;) */; Consumer ID #  You can specify the consumer-id when streaming read table:\nSELECT * FROM t /*+ OPTIONS(\u0026#39;consumer-id\u0026#39; = \u0026#39;myid\u0026#39;, \u0026#39;consumer.expiration-time\u0026#39; = \u0026#39;1 d\u0026#39;, \u0026#39;consumer.mode\u0026#39; = \u0026#39;at-least-once\u0026#39;) */; When stream read Paimon tables, the next snapshot id to be recorded into the file system. This has several advantages:\n When previous job is stopped, the newly started job can continue to consume from the previous progress without resuming from the state. The newly reading will start reading from next snapshot id found in consumer files. If you don\u0026rsquo;t want this behavior, you can set 'consumer.ignore-progress' to true. When deciding whether a snapshot has expired, Paimon looks at all the consumers of the table in the file system, and if there are consumers that still depend on this snapshot, then this snapshot will not be deleted by expiration.  NOTE 1: The consumer will prevent expiration of the snapshot. You can specify 'consumer.expiration-time' to manage the lifetime of consumers.\nNOTE 2: If you don\u0026rsquo;t want to affect the checkpoint time, you need to configure 'consumer.mode' = 'at-least-once'. This mode allow readers consume snapshots at different rates and record the slowest snapshot-id among all readers into the consumer. This mode can provide more capabilities, such as watermark alignment.\nNOTE 3: About 'consumer.mode', since the implementation of exactly-once mode and at-least-once mode are completely different, the state of flink is incompatible and cannot be restored from the state when switching modes.\n You can reset a consumer with a given consumer ID and next snapshot ID and delete a consumer with a given consumer ID. First, you need to stop the streaming task using this consumer ID, and then execute the reset consumer action job.\nRun the following command:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  reset-consumer \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  --consumer_id \u0026lt;consumer-id\u0026gt; \\  [--next_snapshot \u0026lt;next-snapshot-id\u0026gt;] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] please don\u0026rsquo;t specify \u0026ndash;next_snapshot parameter if you want to delete the consumer.\nRead Overwrite #  Streaming reading will ignore the commits generated by INSERT OVERWRITE by default. If you want to read the commits of OVERWRITE, you can configure streaming-read-overwrite.\nRead Parallelism #  By default, the parallelism of batch reads is the same as the number of splits, while the parallelism of stream reads is the same as the number of buckets, but not greater than scan.infer-parallelism.max.\nDisable scan.infer-parallelism, global parallelism will be used for reads.\nYou can also manually specify the parallelism from scan.parallelism.\n  Key Default Type Description     scan.infer-parallelism true Boolean If it is false, parallelism of source are set by global parallelism. Otherwise, source parallelism is inferred from splits number (batch mode) or bucket number(streaming mode).   scan.infer-parallelism.max 1024 Integer If scan.infer-parallelism is true, limit the parallelism of source through this option.   scan.parallelism (none) Integer Define a custom parallelism for the scan source. By default, if this option is not defined, the planner will derive the parallelism for each statement individually by also considering the global configuration. If user enable the scan.infer-parallelism, the planner will derive the parallelism by inferred parallelism.    Query Optimization #   Batch Streaming It is highly recommended to specify partition and primary key filters along with the query, which will speed up the data skipping of the query.\nThe filter functions that can accelerate data skipping are:\n = \u0026lt; \u0026lt;= \u0026gt; \u0026gt;= IN (...) LIKE 'abc%' IS NULL  Paimon will sort the data by primary key, which speeds up the point queries and range queries. When using a composite primary key, it is best for the query filters to form a leftmost prefix of the primary key for good acceleration.\nSuppose that a table has the following specification:\nCREATE TABLE orders ( catalog_id BIGINT, order_id BIGINT, ....., PRIMARY KEY (catalog_id, order_id) NOT ENFORCED -- composite primary key ); The query obtains a good acceleration by specifying a range filter for the leftmost prefix of the primary key.\nSELECT * FROM orders WHERE catalog_id=1025; SELECT * FROM orders WHERE catalog_id=1025 AND order_id=29495; SELECT * FROM orders WHERE catalog_id=1025 AND order_id\u0026gt;2035 AND order_id\u0026lt;6000; However, the following filter cannot accelerate the query well.\nSELECT * FROM orders WHERE order_id=29495; SELECT * FROM orders WHERE catalog_id=1025 OR order_id=29495; "});index.add({'id':56,'href':'/docs/0.9/spark/sql-query/','title':"SQL Query",'section':"Engine Spark",'content':"SQL Query #  Just like all other tables, Paimon tables can be queried with SELECT statement.\nBatch Query #  Paimon\u0026rsquo;s batch read returns all the data in a snapshot of the table. By default, batch reads return the latest snapshot.\nBatch Time Travel #  Paimon batch reads with time travel can specify a snapshot or a tag and read the corresponding data.\nRequires Spark 3.3+.\nyou can use VERSION AS OF and TIMESTAMP AS OF in query to do time travel:\n-- read the snapshot with id 1L (use snapshot id as version) SELECT * FROM t VERSION AS OF 1; -- read the snapshot from specified timestamp SELECT * FROM t TIMESTAMP AS OF \u0026#39;2023-06-01 00:00:00.123\u0026#39;; -- read the snapshot from specified timestamp in unix seconds SELECT * FROM t TIMESTAMP AS OF 1678883047; -- read tag \u0026#39;my-tag\u0026#39; SELECT * FROM t VERSION AS OF \u0026#39;my-tag\u0026#39;; -- read the snapshot from specified watermark. will match the first snapshot after the watermark SELECT * FROM t VERSION AS OF \u0026#39;watermark-1678883047356\u0026#39;; If tag\u0026rsquo;s name is a number and equals to a snapshot id, the VERSION AS OF syntax will consider tag first. For example, if you have a tag named \u0026lsquo;1\u0026rsquo; based on snapshot 2, the statement SELECT * FROM t VERSION AS OF '1' actually queries snapshot 2 instead of snapshot 1.  Batch Incremental #  Read incremental changes between start snapshot (exclusive) and end snapshot.\nFor example:\n \u0026lsquo;5,10\u0026rsquo; means changes between snapshot 5 and snapshot 10. \u0026lsquo;TAG1,TAG3\u0026rsquo; means changes between TAG1 and TAG3.  By default, will scan changelog files for the table which produces changelog files. Otherwise, scan newly changed files. You can also force specifying 'incremental-between-scan-mode'.\nRequires Spark 3.2+.\nPaimon supports that use Spark SQL to do the incremental query that implemented by Spark Table Valued Function.\nyou can use paimon_incremental_query in query to extract the incremental data:\n-- read the incremental data between snapshot id 12 and snapshot id 20. SELECT * FROM paimon_incremental_query(\u0026#39;tableName\u0026#39;, 12, 20); In Batch SQL, the DELETE records are not allowed to be returned, so records of -D will be dropped. If you want see DELETE records, you can query audit_log table.\nStreaming Query #  Paimon currently supports Spark 3.3+ for streaming read.  Paimon supports rich scan mode for streaming read. There is a list:\n  Scan Mode Description     latest For streaming sources, continuously reads latest changes without producing a snapshot at the beginning.    latest-full For streaming sources, produces the latest snapshot on the table upon first startup, and continue to read the latest changes.   from-timestamp For streaming sources, continuously reads changes starting from timestamp specified by \"scan.timestamp-millis\", without producing a snapshot at the beginning.    from-snapshot For streaming sources, continuously reads changes starting from snapshot specified by \"scan.snapshot-id\", without producing a snapshot at the beginning.    from-snapshot-full For streaming sources, produces from snapshot specified by \"scan.snapshot-id\" on the table upon first startup, and continuously reads changes.   default It is equivalent to from-snapshot if \"scan.snapshot-id\" is specified. It is equivalent to from-timestamp if \"timestamp-millis\" is specified. Or, It is equivalent to latest-full.    A simple example with default scan mode:\n// no any scan-related configs are provided, that will use latest-full scan mode. val query = spark.readStream .format(\u0026#34;paimon\u0026#34;) .load(\u0026#34;/path/to/paimon/source/table\u0026#34;) .writeStream .format(\u0026#34;console\u0026#34;) .start() Paimon Structured Streaming also supports a variety of streaming read modes, it can support many triggers and many read limits.\nThese read limits are supported:\n  Key Default Type Description     read.stream.maxFilesPerTrigger (none) Integer The maximum number of files returned in a single batch.   read.stream.maxBytesPerTrigger (none) Long The maximum number of bytes returned in a single batch.   read.stream.maxRowsPerTrigger (none) Long The maximum number of rows returned in a single batch.   read.stream.minRowsPerTrigger (none) Long The minimum number of rows returned in a single batch, which used to create MinRowsReadLimit with read.stream.maxTriggerDelayMs together.   read.stream.maxTriggerDelayMs (none) Long The maximum delay between two adjacent batches, which used to create MinRowsReadLimit with read.stream.minRowsPerTrigger together.    Example: One\nUse org.apache.spark.sql.streaming.Trigger.AvailableNow() and maxBytesPerTrigger defined by paimon.\n// Trigger.AvailableNow()) processes all available data at the start // of the query in one or multiple batches, then terminates the query. // That set read.stream.maxBytesPerTrigger to 128M means that each // batch processes a maximum of 128 MB of data. val query = spark.readStream .format(\u0026#34;paimon\u0026#34;) .option(\u0026#34;read.stream.maxBytesPerTrigger\u0026#34;, \u0026#34;134217728\u0026#34;) .load(\u0026#34;/path/to/paimon/source/table\u0026#34;) .writeStream .format(\u0026#34;console\u0026#34;) .trigger(Trigger.AvailableNow()) .start() Example: Two\nUse org.apache.spark.sql.connector.read.streaming.ReadMinRows.\n// It will not trigger a batch until there are more than 5,000 pieces of data, // unless the interval between the two batches is more than 300 seconds. val query = spark.readStream .format(\u0026#34;paimon\u0026#34;) .option(\u0026#34;read.stream.minRowsPerTrigger\u0026#34;, \u0026#34;5000\u0026#34;) .option(\u0026#34;read.stream.maxTriggerDelayMs\u0026#34;, \u0026#34;300000\u0026#34;) .load(\u0026#34;/path/to/paimon/source/table\u0026#34;) .writeStream .format(\u0026#34;console\u0026#34;) .start() Paimon Structured Streaming supports read row in the form of changelog (add rowkind column in row to represent its change type) in two ways:\n Direct streaming read with the system audit_log table Set read.changelog to true (default is false), then streaming read with table location  Example:\n// Option 1 val query1 = spark.readStream .format(\u0026#34;paimon\u0026#34;) .table(\u0026#34;`table_name$audit_log`\u0026#34;) .writeStream .format(\u0026#34;console\u0026#34;) .start() // Option 2 val query2 = spark.readStream .format(\u0026#34;paimon\u0026#34;) .option(\u0026#34;read.changelog\u0026#34;, \u0026#34;true\u0026#34;) .load(\u0026#34;/path/to/paimon/source/table\u0026#34;) .writeStream .format(\u0026#34;console\u0026#34;) .start() /* +I 1 Hi +I 2 Hello */ Query Optimization #  It is highly recommended to specify partition and primary key filters along with the query, which will speed up the data skipping of the query.\nThe filter functions that can accelerate data skipping are:\n = \u0026lt; \u0026lt;= \u0026gt; \u0026gt;= IN (...) LIKE 'abc%' IS NULL  Paimon will sort the data by primary key, which speeds up the point queries and range queries. When using a composite primary key, it is best for the query filters to form a leftmost prefix of the primary key for good acceleration.\nSuppose that a table has the following specification:\nCREATE TABLE orders ( catalog_id BIGINT, order_id BIGINT, ....., ) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;catalog_id,order_id\u0026#39; ); The query obtains a good acceleration by specifying a range filter for the leftmost prefix of the primary key.\nSELECT * FROM orders WHERE catalog_id=1025; SELECT * FROM orders WHERE catalog_id=1025 AND order_id=29495; SELECT * FROM orders WHERE catalog_id=1025 AND order_id\u0026gt;2035 AND order_id\u0026lt;6000; However, the following filter cannot accelerate the query well.\nSELECT * FROM orders WHERE order_id=29495; SELECT * FROM orders WHERE catalog_id=1025 OR order_id=29495; "});index.add({'id':57,'href':'/docs/0.9/append-table/update/','title':"Update",'section':"Table w/o PK",'content':"Update #  Now, only Spark SQL supports DELETE \u0026amp; UPDATE, you can take a look to Spark Write.\nExample:\nDELETE FROM my_table WHERE currency = \u0026#39;UNKNOWN\u0026#39;; Update append table has two modes:\n COW (Copy on Write): search for the hit files and then rewrite each file to remove the data that needs to be deleted from the files. This operation is costly. MOW (Merge on Write): By specifying 'deletion-vectors.enabled' = 'true', the Deletion Vectors mode can be enabled. Only marks certain records of the corresponding file for deletion and writes the deletion file, without rewriting the entire file.  "});index.add({'id':58,'href':'/docs/0.9/append-table/bucketed/','title':"Bucketed",'section':"Table w/o PK",'content':"Bucketed Append #  You can define the bucket and bucket-key to get a bucketed append table.\nExample to create bucketed append table:\nFlink CREATE TABLE my_table ( product_id BIGINT, price DOUBLE, sales BIGINT ) WITH ( \u0026#39;bucket\u0026#39; = \u0026#39;8\u0026#39;, \u0026#39;bucket-key\u0026#39; = \u0026#39;product_id\u0026#39; );  Streaming #  An ordinary Append table has no strict ordering guarantees for its streaming writes and reads, but there are some cases where you need to define a key similar to Kafka\u0026rsquo;s.\nEvery record in the same bucket is ordered strictly, streaming read will transfer the record to down-stream exactly in the order of writing. To use this mode, you do not need to config special configurations, all the data will go into one bucket as a queue.\nCompaction in Bucket #  By default, the sink node will automatically perform compaction to control the number of files. The following options control the strategy of compaction:\n  Key Default Type Description     write-only false Boolean If set to true, compactions and snapshot expiration will be skipped. This option is used along with dedicated compact jobs.   compaction.min.file-num 5 Integer For file set [f_0,...,f_N], the minimum file number which satisfies sum(size(f_i)) \u0026gt;= targetFileSize to trigger a compaction for append table. This value avoids almost-full-file to be compacted, which is not cost-effective.   compaction.max.file-num 5 Integer For file set [f_0,...,f_N], the maximum file number to trigger a compaction for append table, even if sum(size(f_i)) \u0026lt; targetFileSize. This value avoids pending too much small files, which slows down the performance.   full-compaction.delta-commits (none) Integer Full compaction will be constantly triggered after delta commits.    Streaming Read Order #  For streaming reads, records are produced in the following order:\n For any two records from two different partitions  If scan.plan-sort-partition is set to true, the record with a smaller partition value will be produced first. Otherwise, the record with an earlier partition creation time will be produced first.   For any two records from the same partition and the same bucket, the first written record will be produced first. For any two records from the same partition but two different buckets, different buckets are processed by different tasks, there is no order guarantee between them.  Watermark Definition #  You can define watermark for reading Paimon tables:\nCREATE TABLE t ( `user` BIGINT, product STRING, order_time TIMESTAMP(3), WATERMARK FOR order_time AS order_time - INTERVAL \u0026#39;5\u0026#39; SECOND ) WITH (...); -- launch a bounded streaming job to read paimon_table SELECT window_start, window_end, COUNT(`user`) FROM TABLE( TUMBLE(TABLE t, DESCRIPTOR(order_time), INTERVAL \u0026#39;10\u0026#39; MINUTES)) GROUP BY window_start, window_end; You can also enable Flink Watermark alignment, which will make sure no sources/splits/shards/partitions increase their watermarks too far ahead of the rest:\n  Key Default Type Description     scan.watermark.alignment.group (none) String A group of sources to align watermarks.   scan.watermark.alignment.max-drift (none) Duration Maximal drift to align watermarks, before we pause consuming from the source/task/partition.    Bounded Stream #  Streaming Source can also be bounded, you can specify \u0026lsquo;scan.bounded.watermark\u0026rsquo; to define the end condition for bounded streaming mode, stream reading will end until a larger watermark snapshot is encountered.\nWatermark in snapshot is generated by writer, for example, you can specify a kafka source and declare the definition of watermark. When using this kafka source to write to Paimon table, the snapshots of Paimon table will generate the corresponding watermark, so that you can use the feature of bounded watermark when streaming reads of this Paimon table.\nCREATE TABLE kafka_table ( `user` BIGINT, product STRING, order_time TIMESTAMP(3), WATERMARK FOR order_time AS order_time - INTERVAL \u0026#39;5\u0026#39; SECOND ) WITH (\u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;...); -- launch a streaming insert job INSERT INTO paimon_table SELECT * FROM kakfa_table; -- launch a bounded streaming job to read paimon_table SELECT * FROM paimon_table /*+ OPTIONS(\u0026#39;scan.bounded.watermark\u0026#39;=\u0026#39;...\u0026#39;) */; Batch #  Bucketed table can be used to avoid shuffle if necessary in batch query, for example, you can use the following Spark SQL to read a Paimon table:\nSET spark.sql.sources.v2.bucketing.enabled = true; CREATE TABLE FACT_TABLE (order_id INT, f1 STRING) TBLPROPERTIES (\u0026#39;bucket\u0026#39;=\u0026#39;10\u0026#39;, \u0026#39;bucket-key\u0026#39; = \u0026#39;order_id\u0026#39;); CREATE TABLE DIM_TABLE (order_id INT, f2 STRING) TBLPROPERTIES (\u0026#39;bucket\u0026#39;=\u0026#39;10\u0026#39;, \u0026#39;primary-key\u0026#39; = \u0026#39;order_id\u0026#39;); SELECT * FROM FACT_TABLE JOIN DIM_TABLE on t1.order_id = t4.order_id; The spark.sql.sources.v2.bucketing.enabled config is used to enable bucketing for V2 data sources. When turned on, Spark will recognize the specific distribution reported by a V2 data source through SupportsReportPartitioning, and will try to avoid shuffle if necessary.\nThe costly join shuffle will be avoided if two tables have same bucketing strategy and same number of buckets.\n"});index.add({'id':59,'href':'/docs/0.9/primary-key-table/changelog-producer/','title':"Changelog Producer",'section':"Table with PK",'content':"Changelog Producer #  Streaming write can continuously produce the latest changes for streaming read.\nBy specifying the changelog-producer table property when creating the table, users can choose the pattern of changes produced from table files.\nchangelog-producer may significantly reduce compaction performance, please do not enable it unless necessary.  None #  By default, no extra changelog producer will be applied to the writer of table. Paimon source can only see the merged changes across snapshots, like what keys are removed and what are the new values of some keys.\nHowever, these merged changes cannot form a complete changelog, because we can\u0026rsquo;t read the old values of the keys directly from them. Merged changes require the consumers to \u0026ldquo;remember\u0026rdquo; the values of each key and to rewrite the values without seeing the old ones. Some consumers, however, need the old values to ensure correctness or efficiency.\nConsider a consumer which calculates the sum on some grouping keys (might not be equal to the primary keys). If the consumer only sees a new value 5, it cannot determine what values should be added to the summing result. For example, if the old value is 4, it should add 1 to the result. But if the old value is 6, it should in turn subtract 1 from the result. Old values are important for these types of consumers.\nTo conclude, none changelog producers are best suited for consumers such as a database system. Flink also has a built-in \u0026ldquo;normalize\u0026rdquo; operator which persists the values of each key in states. As one can easily tell, this operator will be very costly and should be avoided. (You can force removing \u0026ldquo;normalize\u0026rdquo; operator via 'scan.remove-normalize'.)\nInput #  By specifying 'changelog-producer' = 'input', Paimon writers rely on their inputs as a source of complete changelog. All input records will be saved in separated changelog files and will be given to the consumers by Paimon sources.\ninput changelog producer can be used when Paimon writers' inputs are complete changelog, such as from a database CDC, or generated by Flink stateful computation.\nLookup #  If your input can’t produce a complete changelog but you still want to get rid of the costly normalized operator, you may consider using the 'lookup' changelog producer.\nBy specifying 'changelog-producer' = 'lookup', Paimon will generate changelog through 'lookup' before committing the data writing.\nLookup will cache data on the memory and local disk, you can use the following options to tune performance:\n  Option Default Type Description     lookup.cache-file-retention 1 h Duration The cached files retention time for lookup. After the file expires, if there is a need for access, it will be re-read from the DFS to build an index on the local disk.   lookup.cache-max-disk-size unlimited MemorySize Max disk size for lookup cache, you can use this option to limit the use of local disks.   lookup.cache-max-memory-size 256 mb MemorySize Max memory size for lookup cache.    Lookup changelog-producer supports changelog-producer.row-deduplicate to avoid generating -U, +U changelog for the same record.\n(Note: Please increase 'execution.checkpointing.max-concurrent-checkpoints' Flink configuration, this is very important for performance).\nFull Compaction #  If you think the resource consumption of \u0026lsquo;lookup\u0026rsquo; is too large, you can consider using \u0026lsquo;full-compaction\u0026rsquo; changelog producer, which can decouple data writing and changelog generation, and is more suitable for scenarios with high latency (For example, 10 minutes).\nBy specifying 'changelog-producer' = 'full-compaction', Paimon will compare the results between full compactions and produce the differences as changelog. The latency of changelog is affected by the frequency of full compactions.\nBy specifying full-compaction.delta-commits table property, full compaction will be constantly triggered after delta commits (checkpoints). This is set to 1 by default, so each checkpoint will have a full compression and generate a change log.\nFull compaction changelog producer can produce complete changelog for any type of source. However it is not as efficient as the input changelog producer and the latency to produce changelog might be high.  Full-compaction changelog-producer supports changelog-producer.row-deduplicate to avoid generating -U, +U changelog for the same record.\n(Note: Please increase 'execution.checkpointing.max-concurrent-checkpoints' Flink configuration, this is very important for performance).\n"});index.add({'id':60,'href':'/docs/0.9/concepts/spec/datafile/','title':"DataFile",'section':"Specification",'content':"DataFile #  Partition #  Consider a Partition table via Flink SQL:\nCREATE TABLE part_t ( f0 INT, f1 STRING, dt STRING ) PARTITIONED BY (dt); INSERT INTO part_t VALUES (1, \u0026#39;11\u0026#39;, \u0026#39;20240514\u0026#39;); The file system will be:\npart_t ├── dt=20240514 │ └── bucket-0 │ └── data-ca1c3c38-dc8d-4533-949b-82e195b41bd4-0.orc ├── manifest │ ├── manifest-08995fe5-c2ac-4f54-9a5f-d3af1fcde41d-0 │ ├── manifest-list-51c16f7b-421c-4bc0-80a0-17677f343358-0 │ └── manifest-list-51c16f7b-421c-4bc0-80a0-17677f343358-1 ├── schema │ └── schema-0 └── snapshot ├── EARLIEST ├── LATEST └── snapshot-1 Paimon adopts the same partitioning concept as Apache Hive to separate data. The files of the partition will be placed in a separate partition directory.\nBucket #  The storage of all Paimon tables relies on buckets, and data files are stored in the bucket directory. The relationship between various table types and buckets in Paimon:\n Primary Key Table:  bucket = -1: Default mode, the dynamic bucket mode records which bucket the key corresponds to through the index files. The index records the correspondence between the hash value of the primary-key and the bucket. bucket = 10: The data is distributed to the corresponding buckets according to the hash value of bucket key ( default is primary key).   Append Table:  bucket = -1: Default mode, ignoring bucket concept, although all data is written to bucket-0, the parallelism of reads and writes is unrestricted. bucket = 10: You need to define bucket-key too, the data is distributed to the corresponding buckets according to the hash value of bucket key.    Data File #  The name of data file is data-${uuid}-${id}.${format}. For the append table, the file stores the data of the table without adding any new columns. But for the primary key table, each row of data stores additional system columns:\n _VALUE_KIND: row is deleted or added. Similar to RocksDB, each row of data can be deleted or added, which will be used for updating the primary key table. _SEQUENCE_NUMBER: this number is used for comparison during updates, determining which data came first and which data came later. _KEY_ prefix to key columns, this is to avoid conflicts with columns of the table.  Changelog File #  Changelog file and Data file are exactly the same, it only takes effect on the primary key table. It is similar to the Binlog in a database, recording changes to the data in the table.\n"});index.add({'id':61,'href':'/docs/0.9/spark/','title':"Engine Spark",'section':"Apache Paimon",'content':""});index.add({'id':62,'href':'/docs/0.9/flink/cdc-ingestion/pulsar-cdc/','title':"Pulsar CDC",'section':"CDC Ingestion",'content':"Pulsar CDC #  Prepare Pulsar Bundled Jar #  flink-connector-pulsar-*.jar Supported Formats #  Flink provides several Pulsar CDC formats: Canal Json, Debezium Json, Debezium Avro, Ogg Json, Maxwell Json and Normal Json. If a message in a pulsar topic is a change event captured from another database using the Change Data Capture (CDC) tool, then you can use the Paimon Pulsar CDC. Write the INSERT, UPDATE, DELETE messages parsed into the paimon table.\n  Formats Supported     Canal CDC True   Debezium CDC True   Maxwell CDC True   OGG CDC True   JSON True    The JSON sources possibly missing some information. For example, Ogg and Maxwell format standards don\u0026rsquo;t contain field types; When you write JSON sources into Flink Pulsar sink, it will only reserve data and row type and drop other information. The synchronization job will try best to handle the problem as follows:\n If missing field types, Paimon will use \u0026lsquo;STRING\u0026rsquo; type as default. If missing database name or table name, you cannot do database synchronization, but you can still do table synchronization. If missing primary keys, the job might create non primary key table. You can set primary keys when submit job in table synchronization.   Synchronizing Tables #  By using PulsarSyncTableAction in a Flink DataStream job or directly through flink run, users can synchronize one or multiple tables from Pulsar\u0026rsquo;s one topic into one Paimon table.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  pulsar_sync_table --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  [--partition_keys \u0026lt;partition_keys\u0026gt;] \\  [--primary_keys \u0026lt;primary-keys\u0026gt;] \\  [--type_mapping to-string] \\  [--computed_column \u0026lt;\u0026#39;column-name=expr-name(args[, ...])\u0026#39;\u0026gt; [--computed_column ...]] \\  [--pulsar_conf \u0026lt;pulsar-source-conf\u0026gt; [--pulsar_conf \u0026lt;pulsar-source-conf\u0026gt; ...]] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; ...]]    Configuration Description     --warehouse The path to Paimon warehouse.   --database The database name in Paimon catalog.   --table The Paimon table name.   --partition_keys The partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example \"dt,hh,mm\".   --primary_keys The primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example \"buyer_id,seller_id\".   --type_mapping It is used to specify how to map MySQL data type to Paimon type.\nSupported options:  \"tinyint1-not-bool\": maps MySQL TINYINT(1) to TINYINT instead of BOOLEAN. \"to-nullable\": ignores all NOT NULL constraints (except for primary keys). This is used to solve the problem that Flink cannot accept the MySQL 'ALTER TABLE ADD COLUMN column type NOT NULL DEFAULT x' operation.  \"to-string\": maps all MySQL types to STRING. \"char-to-string\": maps MySQL CHAR(length)/VARCHAR(length) types to STRING. \"longtext-to-bytes\": maps MySQL LONGTEXT types to BYTES. \"bigint-unsigned-to-bigint\": maps MySQL BIGINT UNSIGNED, BIGINT UNSIGNED ZEROFILL, SERIAL to BIGINT. You should ensure overflow won't occur when using this option.     --computed_column The definitions of computed columns. The argument field is from Pulsar topic's table field name. See here for a complete list of configurations.    --pulsar_conf The configuration for Flink Pulsar sources. Each configuration should be specified in the format `key=value`. `topic/topic-pattern`, `value.format`, `pulsar.client.serviceUrl`, `pulsar.admin.adminUrl`, and `pulsar.consumer.subscriptionName` are required configurations, others are optional.See its document for a complete list of configurations.   --catalog_conf The configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.   --table_conf The configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.    If the Paimon table you specify does not exist, this action will automatically create the table. Its schema will be derived from all specified Pulsar topic\u0026rsquo;s tables,it gets the earliest non-DDL data parsing schema from topic. If the Paimon table already exists, its schema will be compared against the schema of all specified Pulsar topic\u0026rsquo;s tables.\nExample 1:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  pulsar_sync_table \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --partition_keys pt \\  --primary_keys pt,uid \\  --computed_column \u0026#39;_year=year(age)\u0026#39; \\  --pulsar_conf topic=order \\  --pulsar_conf value.format=canal-json \\  --pulsar_conf pulsar.client.serviceUrl=pulsar://127.0.0.1:6650 \\  --pulsar_conf pulsar.admin.adminUrl=http://127.0.0.1:8080 \\  --pulsar_conf pulsar.consumer.subscriptionName=paimon-tests \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --table_conf changelog-producer=input \\  --table_conf sink.parallelism=4 If the Pulsar topic doesn\u0026rsquo;t contain message when you start the synchronization job, you must manually create the table before submitting the job. You can define the partition keys and primary keys only, and the left columns will be added by the synchronization job.\nNOTE: In this case you shouldn\u0026rsquo;t use \u0026ndash;partition_keys or \u0026ndash;primary_keys, because those keys are defined when creating the table and can not be modified. Additionally, if you specified computed columns, you should also define all the argument columns used for computed columns.\nExample 2: If you want to synchronize a table which has primary key \u0026lsquo;id INT\u0026rsquo;, and you want to compute a partition key \u0026lsquo;part=date_format(create_time,yyyy-MM-dd)\u0026rsquo;, you can create a such table first (the other columns can be omitted):\nCREATE TABLE test_db.test_table ( id INT, -- primary key  create_time TIMESTAMP, -- the argument of computed column part  part STRING, -- partition key  PRIMARY KEY (id, part) NOT ENFORCED ) PARTITIONED BY (part); Then you can submit synchronization job:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  pulsar_sync_table \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --computed_column \u0026#39;part=date_format(create_time,yyyy-MM-dd)\u0026#39; \\  ... (other conf) Example 3: For some append data (such as log data), it can be treated as special CDC data with only INSERT operation type, so you can use \u0026lsquo;format=json\u0026rsquo; to synchronize such data to the Paimon table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  kafka_sync_table \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --partition_keys pt \\  --computed_column \u0026#39;pt=date_format(event_tm, yyyyMMdd)\u0026#39; \\  --kafka_conf properties.bootstrap.servers=127.0.0.1:9020 \\  --kafka_conf topic=test_log \\  --kafka_conf properties.group.id=123456 \\  --kafka_conf value.format=json \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf sink.parallelism=4 Synchronizing Databases #  By using PulsarSyncDatabaseAction in a Flink DataStream job or directly through flink run, users can synchronize the multi topic or one topic into one Paimon database.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  pulsar_sync_database --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  [--table_prefix \u0026lt;paimon-table-prefix\u0026gt;] \\  [--table_suffix \u0026lt;paimon-table-suffix\u0026gt;] \\  [--including_tables \u0026lt;table-name|name-regular-expr\u0026gt;] \\  [--excluding_tables \u0026lt;table-name|name-regular-expr\u0026gt;] \\  [--type_mapping to-string] \\  [--partition_keys \u0026lt;partition_keys\u0026gt;] \\  [--primary_keys \u0026lt;primary-keys\u0026gt;] \\  [--pulsar_conf \u0026lt;pulsar-source-conf\u0026gt; [--pulsar_conf \u0026lt;pulsar-source-conf\u0026gt; ...]] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; ...]]    Configuration Description     --warehouse The path to Paimon warehouse.   --database The database name in Paimon catalog.   --ignore_incompatible It is default false, in this case, if MySQL table name exists in Paimon and their schema is incompatible,an exception will be thrown. You can specify it to true explicitly to ignore the incompatible tables and exception.   --table_prefix The prefix of all Paimon tables to be synchronized. For example, if you want all synchronized tables to have \"ods_\" as prefix, you can specify \"--table_prefix ods_\".   --table_suffix The suffix of all Paimon tables to be synchronized. The usage is same as \"--table_prefix\".   --including_tables It is used to specify which source tables are to be synchronized. You must use '|' to separate multiple tables.Because '|' is a special character, a comma is required, for example: 'a|b|c'.Regular expression is supported, for example, specifying \"--including_tables test|paimon.*\" means to synchronize table 'test' and all tables start with 'paimon'.   --excluding_tables It is used to specify which source tables are not to be synchronized. The usage is same as \"--including_tables\". \"--excluding_tables\" has higher priority than \"--including_tables\" if you specified both.   --type_mapping It is used to specify how to map MySQL data type to Paimon type.\nSupported options:  \"tinyint1-not-bool\": maps MySQL TINYINT(1) to TINYINT instead of BOOLEAN. \"to-nullable\": ignores all NOT NULL constraints (except for primary keys). This is used to solve the problem that Flink cannot accept the MySQL 'ALTER TABLE ADD COLUMN column type NOT NULL DEFAULT x' operation.  \"to-string\": maps all MySQL types to STRING. \"char-to-string\": maps MySQL CHAR(length)/VARCHAR(length) types to STRING. \"longtext-to-bytes\": maps MySQL LONGTEXT types to BYTES. \"bigint-unsigned-to-bigint\": maps MySQL BIGINT UNSIGNED, BIGINT UNSIGNED ZEROFILL, SERIAL to BIGINT. You should ensure overflow won't occur when using this option.     --partition_keys The partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example \"dt,hh,mm\". If the keys are not in source table, the sink table won't set partition keys.   --primary_keys The primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example \"buyer_id,seller_id\". If the keys are not in source table, but the source table has primary keys, the sink table will use source table's primary keys. Otherwise, the sink table won't set primary keys.   --pulsar_conf The configuration for Flink Pulsar sources. Each configuration should be specified in the format `key=value`. `topic/topic-pattern`, `value.format`, `pulsar.client.serviceUrl`, `pulsar.admin.adminUrl`, and `pulsar.consumer.subscriptionName` are required configurations, others are optional.See its document for a complete list of configurations.   --catalog_conf The configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.   --table_conf The configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.    Only tables with primary keys will be synchronized.\nThis action will build a single combined sink for all tables. For each Pulsar topic\u0026rsquo;s table to be synchronized, if the corresponding Paimon table does not exist, this action will automatically create the table, and its schema will be derived from all specified Pulsar topic\u0026rsquo;s tables. If the Paimon table already exists and its schema is different from that parsed from Pulsar record, this action will try to preform schema evolution.\nExample\nSynchronization from one Pulsar topic to Paimon database.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  pulsar_sync_database \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --pulsar_conf topic=order \\  --pulsar_conf value.format=canal-json \\  --pulsar_conf pulsar.client.serviceUrl=pulsar://127.0.0.1:6650 \\  --pulsar_conf pulsar.admin.adminUrl=http://127.0.0.1:8080 \\  --pulsar_conf pulsar.consumer.subscriptionName=paimon-tests \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --table_conf changelog-producer=input \\  --table_conf sink.parallelism=4 Synchronization from multiple Pulsar topics to Paimon database.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  pulsar_sync_database \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --pulsar_conf topic=order,logistic_order,user \\  --pulsar_conf value.format=canal-json \\  --pulsar_conf pulsar.client.serviceUrl=pulsar://127.0.0.1:6650 \\  --pulsar_conf pulsar.admin.adminUrl=http://127.0.0.1:8080 \\  --pulsar_conf pulsar.consumer.subscriptionName=paimon-tests \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --table_conf changelog-producer=input \\  --table_conf sink.parallelism=4 Additional pulsar_config #  There are some useful options to build Flink Pulsar Source, but they are not provided by flink-pulsar-connector document. They are:\n  Key Default Type Description     value.format (none) String Defines the format identifier for encoding value data.   topic (none) String Topic name(s) from which the data is read. It also supports topic list by separating topic by semicolon like 'topic-1;topic-2'. Note, only one of \"topic-pattern\" and \"topic\" can be specified.    topic-pattern (none) String The regular expression for a pattern of topic names to read from. All topics with names that match the specified regular expression will be subscribed by the consumer when the job starts running. Note, only one of \"topic-pattern\" and \"topic\" can be specified.    pulsar.startCursor.fromMessageId EARLIEST Sting Using a unique identifier of a single message to seek the start position. The common format is a triple '\u0026ltlong\u0026gtledgerId,\u0026ltlong\u0026gtentryId,\u0026ltint\u0026gtpartitionIndex'. Specially, you can set it to EARLIEST (-1, -1, -1) or LATEST (Long.MAX_VALUE, Long.MAX_VALUE, -1).    pulsar.startCursor.fromPublishTime (none) Long Using the message publish time to seek the start position.   pulsar.startCursor.fromMessageIdInclusive true Boolean Whether to include the given message id. This option only works when the message id is not EARLIEST or LATEST.   pulsar.stopCursor.atMessageId (none) String Stop consuming when the message id is equal or greater than the specified message id. Message that is equal to the specified message id will not be consumed. The common format is a triple '\u0026ltlong\u0026gtledgerId,\u0026ltlong\u0026gtentryId,\u0026ltint\u0026gtpartitionIndex'. Specially, you can set it to LATEST (Long.MAX_VALUE, Long.MAX_VALUE, -1).  pulsar.stopCursor.afterMessageId (none) String Stop consuming when the message id is greater than the specified message id. Message that is equal to the specified message id will be consumed. The common format is a triple '\u0026ltlong\u0026gtledgerId,\u0026ltlong\u0026gtentryId,\u0026ltint\u0026gtpartitionIndex'. Specially, you can set it to LATEST (Long.MAX_VALUE, Long.MAX_VALUE, -1).    pulsar.stopCursor.atEventTime (none) Long Stop consuming when message event time is greater than or equals the specified timestamp. Message that even time is equal to the specified timestamp will not be consumed.    pulsar.stopCursor.afterEventTime (none) Long Stop consuming when message event time is greater than the specified timestamp. Message that even time is equal to the specified timestamp will be consumed.    pulsar.source.unbounded true Boolean To specify the boundedness of a stream.   schema.registry.url (none) String When configuring \"value.format=debezium-avro\" which requires using the Confluence schema registry model for Apache Avro serialization, you need to provide the schema registry URL.    "});index.add({'id':63,'href':'/docs/0.9/flink/sql-lookup/','title':"SQL Lookup",'section':"Engine Flink",'content':"Lookup Joins #  Lookup Joins are a type of join in streaming queries. It is used to enrich a table with data that is queried from Paimon. The join requires one table to have a processing time attribute and the other table to be backed by a lookup source connector.\nPaimon supports lookup joins on tables with primary keys and append tables in Flink. The following example illustrates this feature.\nPrepare #  First, let\u0026rsquo;s create a Paimon table and update it in real-time.\n-- Create a paimon catalog CREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39;=\u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39;=\u0026#39;hdfs://nn:8020/warehouse/path\u0026#39; -- or \u0026#39;file://tmp/foo/bar\u0026#39; ); USE CATALOG my_catalog; -- Create a table in paimon catalog CREATE TABLE customers ( id INT PRIMARY KEY NOT ENFORCED, name STRING, country STRING, zip STRING ); -- Launch a streaming job to update customers table INSERT INTO customers ... -- Create a temporary left table, like from kafka CREATE TEMPORARY TABLE orders ( order_id INT, total INT, customer_id INT, proc_time AS PROCTIME() ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ... ); Normal Lookup #  You can now use customers in a lookup join query.\n-- enrich each order with customer information SELECT o.order_id, o.total, c.country, c.zip FROM orders AS o JOIN customers FOR SYSTEM_TIME AS OF o.proc_time AS c ON o.customer_id = c.id; Retry Lookup #  If the records of orders (main table) join missing because the corresponding data of customers (lookup table) is not ready. You can consider using Flink\u0026rsquo;s Delayed Retry Strategy For Lookup. Only for Flink 1.16+.\n-- enrich each order with customer information SELECT /*+ LOOKUP(\u0026#39;table\u0026#39;=\u0026#39;c\u0026#39;, \u0026#39;retry-predicate\u0026#39;=\u0026#39;lookup_miss\u0026#39;, \u0026#39;retry-strategy\u0026#39;=\u0026#39;fixed_delay\u0026#39;, \u0026#39;fixed-delay\u0026#39;=\u0026#39;1s\u0026#39;, \u0026#39;max-attempts\u0026#39;=\u0026#39;600\u0026#39;) */ o.order_id, o.total, c.country, c.zip FROM orders AS o JOIN customers FOR SYSTEM_TIME AS OF o.proc_time AS c ON o.customer_id = c.id; Async Retry Lookup #  The problem with synchronous retry is that one record will block subsequent records, causing the entire job to be blocked. You can consider using async + allow_unordered to avoid blocking, the records that join missing will no longer block other records.\n-- enrich each order with customer information SELECT /*+ LOOKUP(\u0026#39;table\u0026#39;=\u0026#39;c\u0026#39;, \u0026#39;retry-predicate\u0026#39;=\u0026#39;lookup_miss\u0026#39;, \u0026#39;output-mode\u0026#39;=\u0026#39;allow_unordered\u0026#39;, \u0026#39;retry-strategy\u0026#39;=\u0026#39;fixed_delay\u0026#39;, \u0026#39;fixed-delay\u0026#39;=\u0026#39;1s\u0026#39;, \u0026#39;max-attempts\u0026#39;=\u0026#39;600\u0026#39;) */ o.order_id, o.total, c.country, c.zip FROM orders AS o JOIN customers /*+ OPTIONS(\u0026#39;lookup.async\u0026#39;=\u0026#39;true\u0026#39;, \u0026#39;lookup.async-thread-number\u0026#39;=\u0026#39;16\u0026#39;) */ FOR SYSTEM_TIME AS OF o.proc_time AS c ON o.customer_id = c.id; If the main table (orders) is CDC stream, allow_unordered will be ignored by Flink SQL (only supports append stream), your streaming job may be blocked. You can try to use audit_log system table feature of Paimon to walk around (convert CDC stream to append stream).  Dynamic Partition #  In traditional data warehouses, each partition often maintains the latest full data, so this partition table only needs to join the latest partition. Paimon has specifically developed the max_pt feature for this scenario.\nCreate Paimon Partitioned Table\nCREATE TABLE customers ( id INT, name STRING, country STRING, zip STRING, dt STRING, PRIMARY KEY (id, dt) NOT ENFORCED ) PARTITIONED BY (dt); Lookup Join\nSELECT o.order_id, o.total, c.country, c.zip FROM orders AS o JOIN customers /*+ OPTIONS(\u0026#39;lookup.dynamic-partition\u0026#39;=\u0026#39;max_pt()\u0026#39;, \u0026#39;lookup.dynamic-partition.refresh-interval\u0026#39;=\u0026#39;1 h\u0026#39;) */ FOR SYSTEM_TIME AS OF o.proc_time AS c ON o.customer_id = c.id; The Lookup node will automatically refresh the latest partition and query the data of the latest partition.\nQuery Service #  You can run a Flink Streaming Job to start query service for the table. When QueryService exists, Flink Lookup Join will prioritize obtaining data from it, which will effectively improve query performance.\nFlink SQL CALL sys.query_service(\u0026#39;database_name.table_name\u0026#39;, parallelism); Flink Action \u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  query_service \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  [--parallelism \u0026lt;parallelism\u0026gt;] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]]  "});index.add({'id':64,'href':'/docs/0.9/engines/trino/','title':"Trino",'section':"Engine Others",'content':"Trino #  This documentation is a guide for using Paimon in Trino.\nVersion #  Paimon currently supports Trino 420 and above.\nFilesystem #  From version 0.8, paimon share trino filesystem for all actions, which means, you should config trino filesystem before using trino-paimon. You can find information about how to config filesystems for trino on trino official website.\nPreparing Paimon Jar File #  Download from master: https://paimon.apache.org/docs/master/project/download/ You can also manually build a bundled jar from the source code. However, there are a few preliminary steps that need to be taken before compiling:\n To build from the source code, clone the git repository. Install JDK17 locally, and configure JDK17 as a global environment variable;  Then,you can build bundled jar with the following command:\nmvn clean install -DskipTests You can find Trino connector jar in ./paimon-trino-\u0026lt;trino-version\u0026gt;/target/paimon-trino-\u0026lt;trino-version\u0026gt;-0.9.0-plugin.tar.gz.\nWe use hadoop-apache as a dependency for Hadoop, and the default Hadoop dependency typically supports both Hadoop 2 and Hadoop 3. If you encounter an unsupported scenario, you can specify the corresponding Apache Hadoop version.\nFor example, if you want to use Hadoop 3.3.5-1, you can use the following command to build the jar:\nmvn clean install -DskipTests -Dhadoop.apache.version=3.3.5-1 Tmp Dir #  Paimon will unzip some jars to the tmp directory for codegen. By default, Trino will use '/tmp' as the temporary directory, but '/tmp' may be periodically deleted.\nYou can configure this environment variable when Trino starts:\n-Djava.io.tmpdir=/path/to/other/tmpdir Let Paimon use a secure temporary directory.\nConfigure Paimon Catalog #  Install Paimon Connector #  tar -zxf paimon-trino-\u0026lt;trino-version\u0026gt;-0.9.0-plugin.tar.gz -C ${TRINO_HOME}/plugin the variable trino-version is module name, must be one of 420, 427.\n NOTE: For JDK 17, when Deploying Trino, should add jvm options: --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED\n Configure #  Catalogs are registered by creating a catalog properties file in the etc/catalog directory. For example, create etc/catalog/paimon.properties with the following contents to mount the paimon connector as the paimon catalog:\nconnector.name=paimon warehouse=file:/tmp/warehouse If you are using HDFS, choose one of the following ways to configure your HDFS:\n set environment variable HADOOP_HOME. set environment variable HADOOP_CONF_DIR. configure hadoop-conf-dir in the properties.  If you are using a hadoop filesystem, you can still use trino-hdfs and trino-hive to config it. For example, if you use oss as a storage, you can write in paimon.properties according to Trino Reference:\nhive.config.resources=/path/to/core-site.xml Then, config core-site.xml according to Jindo Reference\nKerberos #  You can configure kerberos keytab file when using KERBEROS authentication in the properties.\nsecurity.kerberos.login.principal=hadoop-user security.kerberos.login.keytab=/etc/trino/hdfs.keytab Keytab files must be distributed to every node in the cluster that runs Trino.\nCreate Schema #  CREATE SCHEMA paimon.test_db; Create Table #  CREATE TABLE paimon.test_db.orders ( order_key bigint, orders_tatus varchar, total_price decimal(18,4), order_date date ) WITH ( file_format = 'ORC', primary_key = ARRAY['order_key','order_date'], partitioned_by = ARRAY['order_date'], bucket = '2', bucket_key = 'order_key', changelog_producer = 'input' ) Add Column #  CREATE TABLE paimon.test_db.orders ( order_key bigint, orders_tatus varchar, total_price decimal(18,4), order_date date ) WITH ( file_format = 'ORC', primary_key = ARRAY['order_key','order_date'], partitioned_by = ARRAY['order_date'], bucket = '2', bucket_key = 'order_key', changelog_producer = 'input' ) ALTER TABLE paimon.test_db.orders ADD COLUMN shipping_address varchar; Query #  SELECT * FROM paimon.test_db.orders Query with Time Traveling #  version \u0026gt;=420 -- read the snapshot from specified timestamp SELECT * FROM t FOR TIMESTAMP AS OF TIMESTAMP \u0026#39;2023-01-01 00:00:00 Asia/Shanghai\u0026#39;; -- read the snapshot with id 1L (use snapshot id as version) SELECT * FROM t FOR VERSION AS OF 1;  Trino to Paimon type mapping #  This section lists all supported type conversion between Trino and Paimon. All Trino\u0026rsquo;s data types are available in package io.trino.spi.type.\n  Trino Data Type Paimon Data Type Atomic Type     RowType RowType false   MapType MapType false   ArrayType ArrayType false   BooleanType BooleanType true   TinyintType TinyIntType true   SmallintType SmallIntType true   IntegerType IntType true   BigintType BigIntType true   RealType FloatType true   DoubleType DoubleType true   CharType(length) CharType(length) true   VarCharType(VarCharType.MAX_LENGTH) VarCharType(VarCharType.MAX_LENGTH) true   VarCharType(length) VarCharType(length), length is less than VarCharType.MAX_LENGTH true   DateType DateType true   TimestampType TimestampType true   DecimalType(precision, scale) DecimalType(precision, scale) true   VarBinaryType(length) VarBinaryType(length) true   TimestampWithTimeZoneType LocalZonedTimestampType true    "});index.add({'id':65,'href':'/docs/0.9/engines/presto/','title':"Presto",'section':"Engine Others",'content':"Presto #  This documentation is a guide for using Paimon in Presto.\nVersion #  Paimon currently supports Presto 0.236 and above.\nPreparing Paimon Jar File #  Download from master: https://paimon.apache.org/docs/master/project/download/ You can also manually build a bundled jar from the source code.\nTo build from the source code, clone the git repository.\nBuild presto connector plugin with the following command.\nmvn clean install -DskipTests After the packaging is complete, you can choose the corresponding connector based on your own Presto version:\n   Version Package     [0.236, 0.268) ./paimon-presto-0.236/target/paimon-presto-0.236-0.9.0-plugin.tar.gz   [0.268, 0.273) ./paimon-presto-0.268/target/paimon-presto-0.268-0.9.0-plugin.tar.gz   [0.273, latest] ./paimon-presto-0.273/target/paimon-presto-0.273-0.9.0-plugin.tar.gz    Of course, we also support different versions of Hive and Hadoop. But note that we utilize Presto-shaded versions of Hive and Hadoop packages to address dependency conflicts. You can check the following two links to select the appropriate versions of Hive and Hadoop:\nhadoop-apache2\nhive-apache\nBoth Hive 2 and 3, as well as Hadoop 2 and 3, are supported.\nFor example, if your presto version is 0.274, hive and hadoop version is 2.x, you could run:\nmvn clean install -DskipTests -am -pl paimon-presto-0.273 -Dpresto.version=0.274 -Dhadoop.apache2.version=2.7.4-9 -Dhive.apache.version=1.2.2-2 Tmp Dir #  Paimon will unzip some jars to the tmp directory for codegen. By default, Presto will use '/tmp' as the temporary directory, but '/tmp' may be periodically deleted.\nYou can configure this environment variable when Presto starts:\n-Djava.io.tmpdir=/path/to/other/tmpdir Let Paimon use a secure temporary directory.\nConfigure Paimon Catalog #  Install Paimon Connector #  tar -zxf paimon-presto-${PRESTO_VERSION}/target/paimon-presto-${PRESTO_VERSION}-${PAIMON_VERSION}-plugin.tar.gz -C ${PRESTO_HOME}/plugin Note that, the variable PRESTO_VERSION is module name, must be one of 0.236, 0.268, 0.273.\nConfiguration #  cd ${PRESTO_HOME} mkdir -p etc/catalog connector.name=paimon # set your filesystem path, such as hdfs://namenode01:8020/path and s3://${YOUR_S3_BUCKET}/path warehouse=${YOUR_FS_PATH} If you are using HDFS FileSystem, you will also need to do one more thing: choose one of the following ways to configure your HDFS:\n set environment variable HADOOP_HOME. set environment variable HADOOP_CONF_DIR. configure hadoop-conf-dir in the properties.  If you are using S3 FileSystem, you need to add paimon-s3-${PAIMON_VERSION}.jar in ${PRESTO_HOME}/plugin/paimon and additionally configure the following properties in paimon.properties:\ns3.endpoint=${YOUR_ENDPOINTS} s3.access-key=${YOUR_AK} s3.secret-key=${YOUR_SK} Query HiveCatalog table:\nvim etc/catalog/paimon.properties and set the following config:\nconnector.name=paimon # set your filesystem path, such as hdfs://namenode01:8020/path and s3://${YOUR_S3_BUCKET}/path warehouse=${YOUR_FS_PATH} metastore=hive uri=thrift://${YOUR_HIVE_METASTORE}:9083 Kerberos #  You can configure kerberos keytab file when using KERBEROS authentication in the properties.\nsecurity.kerberos.login.principal=hadoop-user security.kerberos.login.keytab=/etc/presto/hdfs.keytab Keytab files must be distributed to every node in the cluster that runs Presto.\nCreate Schema #  CREATE SCHEMA paimon.test_db; Create Table #  CREATE TABLE paimon.test_db.orders ( order_key bigint, order_status varchar, total_price decimal(18,4), order_date date ) WITH ( file_format = 'ORC', primary_key = ARRAY['order_key','order_date'], partitioned_by = ARRAY['order_date'], bucket = '2', bucket_key = 'order_key', changelog_producer = 'input' ) Add Column #  CREATE TABLE paimon.test_db.orders ( order_key bigint, orders_tatus varchar, total_price decimal(18,4), order_date date ) WITH ( file_format = 'ORC', primary_key = ARRAY['order_key','order_date'], partitioned_by = ARRAY['order_date'], bucket = '2', bucket_key = 'order_key', changelog_producer = 'input' ) ALTER TABLE paimon.test_db.orders ADD COLUMN \u0026quot;shipping_address varchar; Query #  SELECT * FROM paimon.default.MyTable Presto to Paimon type mapping #  This section lists all supported type conversion between Presto and Paimon. All Presto\u0026rsquo;s data types are available in package  com.facebook.presto.common.type.\n  Presto Data Type Paimon Data Type Atomic Type     RowType RowType false   MapType MapType false   ArrayType ArrayType false   BooleanType BooleanType true   TinyintType TinyIntType true   SmallintType SmallIntType true   IntegerType IntType true   BigintType BigIntType true   RealType FloatType true   DoubleType DoubleType true   CharType(length) CharType(length) true   VarCharType(VarCharType.MAX_LENGTH) VarCharType(VarCharType.MAX_LENGTH) true   VarCharType(length) VarCharType(length), length is less than VarCharType.MAX_LENGTH true   DateType DateType true   TimestampType TimestampType true   DecimalType(precision, scale) DecimalType(precision, scale) true   VarBinaryType(length) VarBinaryType(length) true   TimestampWithTimeZoneType LocalZonedTimestampType true    "});index.add({'id':66,'href':'/docs/0.9/primary-key-table/sequence-rowkind/','title':"Sequence \u0026 Rowkind",'section':"Table with PK",'content':"Sequence and Rowkind #  When creating a table, you can specify the 'sequence.field' by specifying fields to determine the order of updates, or you can specify the 'rowkind.field' to determine the changelog kind of record.\nSequence Field #  By default, the primary key table determines the merge order according to the input order (the last input record will be the last to merge). However, in distributed computing, there will be some cases that lead to data disorder. At this time, you can use a time field as sequence.field, for example:\nFlink CREATE TABLE my_table ( pk BIGINT PRIMARY KEY NOT ENFORCED, v1 DOUBLE, v2 BIGINT, update_time TIMESTAMP ) WITH ( \u0026#39;sequence.field\u0026#39; = \u0026#39;update_time\u0026#39; );  The record with the largest sequence.field value will be the last to merge, if the values are the same, the input order will be used to determine which one is the last one. sequence.field supports fields of all data types.\nYou can define multiple fields for sequence.field, for example 'update_time,flag', multiple fields will be compared in order.\nUser defined sequence fields conflict with features such as first_row and first_value, which may result in unexpected results.  Row Kind Field #  By default, the primary key table determines the row kind according to the input row. You can also define the 'rowkind.field' to use a field to extract row kind.\nThe valid row kind string should be '+I', '-U', '+U' or '-D'.\n"});index.add({'id':67,'href':'/docs/0.9/flink/sql-alter/','title':"SQL Alter",'section':"Engine Flink",'content':"Altering Tables #  Changing/Adding Table Properties #  The following SQL sets write-buffer-size table property to 256 MB.\nALTER TABLE my_table SET ( \u0026#39;write-buffer-size\u0026#39; = \u0026#39;256 MB\u0026#39; ); Removing Table Properties #  The following SQL removes write-buffer-size table property.\nALTER TABLE my_table RESET (\u0026#39;write-buffer-size\u0026#39;); Changing/Adding Table Comment #  The following SQL changes comment of table my_table to table comment.\nALTER TABLE my_table SET ( \u0026#39;comment\u0026#39; = \u0026#39;table comment\u0026#39; ); Removing Table Comment #  The following SQL removes table comment.\nALTER TABLE my_table RESET (\u0026#39;comment\u0026#39;); Rename Table Name #  The following SQL rename the table name to new name.\nALTER TABLE my_table RENAME TO my_table_new; If you use object storage, such as S3 or OSS, please use this syntax carefully, because the renaming of object storage is not atomic, and only partial files may be moved in case of failure.  Adding New Columns #  The following SQL adds two columns c1 and c2 to table my_table.\nALTER TABLE my_table ADD (c1 INT, c2 STRING); Renaming Column Name #  The following SQL renames column c0 in table my_table to c1.\nALTER TABLE my_table RENAME c0 TO c1; Dropping Columns #  The following SQL drops two columns c1 and c2 from table my_table. In hive catalog, you need to ensure disable hive.metastore.disallow.incompatible.col.type.changes in your hive server, otherwise this operation may fail, throws an exception like The following columns have types incompatible with the existing columns in their respective positions.\nALTER TABLE my_table DROP (c1, c2); Dropping Partitions #  The following SQL drops the partitions of the paimon table.\nFor flink sql, you can specify the partial columns of partition columns, and you can also specify multiple partition values at the same time.\nALTER TABLE my_table DROP PARTITION (`id` = 1); ALTER TABLE my_table DROP PARTITION (`id` = 1, `name` = \u0026#39;paimon\u0026#39;); ALTER TABLE my_table DROP PARTITION (`id` = 1), PARTITION (`id` = 2); Changing Column Nullability #  The following SQL changes nullability of column coupon_info.\nCREATE TABLE my_table (id INT PRIMARY KEY NOT ENFORCED, coupon_info FLOAT NOT NULL); -- Change column `coupon_info` from NOT NULL to nullable ALTER TABLE my_table MODIFY coupon_info FLOAT; -- Change column `coupon_info` from nullable to NOT NULL -- If there are NULL values already, set table option as below to drop those records silently before altering table. SET \u0026#39;table.exec.sink.not-null-enforcer\u0026#39; = \u0026#39;DROP\u0026#39;; ALTER TABLE my_table MODIFY coupon_info FLOAT NOT NULL; Changing nullable column to NOT NULL is only supported by Flink currently.  Changing Column Comment #  The following SQL changes comment of column buy_count to buy count.\nALTER TABLE my_table MODIFY buy_count BIGINT COMMENT \u0026#39;buy count\u0026#39;; Adding Column Position #  To add a new column with specified position, use FIRST or AFTER col_name.\nALTER TABLE my_table ADD c INT FIRST; ALTER TABLE my_table ADD c INT AFTER b; Changing Column Position #  To modify an existent column to a new position, use FIRST or AFTER col_name.\nALTER TABLE my_table MODIFY col_a DOUBLE FIRST; ALTER TABLE my_table MODIFY col_a DOUBLE AFTER col_b; Changing Column Type #  The following SQL changes type of column col_a to DOUBLE.\nALTER TABLE my_table MODIFY col_a DOUBLE; Adding watermark #  The following SQL adds a computed column ts from existing column log_ts, and a watermark with strategy ts - INTERVAL '1' HOUR on column ts which is marked as event time attribute of table my_table.\nALTER TABLE my_table ADD ( ts AS TO_TIMESTAMP(log_ts) AFTER log_ts, WATERMARK FOR ts AS ts - INTERVAL \u0026#39;1\u0026#39; HOUR ); Dropping watermark #  The following SQL drops the watermark of table my_table.\nALTER TABLE my_table DROP WATERMARK; Changing watermark #  The following SQL modifies the watermark strategy to ts - INTERVAL '2' HOUR.\nALTER TABLE my_table MODIFY WATERMARK FOR ts AS ts - INTERVAL \u0026#39;2\u0026#39; HOUR "});index.add({'id':68,'href':'/docs/0.9/spark/sql-alter/','title':"SQL Alter",'section':"Engine Spark",'content':"Altering Tables #  Changing/Adding Table Properties #  The following SQL sets write-buffer-size table property to 256 MB.\nALTER TABLE my_table SET TBLPROPERTIES ( \u0026#39;write-buffer-size\u0026#39; = \u0026#39;256 MB\u0026#39; ); Removing Table Properties #  The following SQL removes write-buffer-size table property.\nALTER TABLE my_table UNSET TBLPROPERTIES (\u0026#39;write-buffer-size\u0026#39;); Changing/Adding Table Comment #  The following SQL changes comment of table my_table to table comment.\nALTER TABLE my_table SET TBLPROPERTIES ( \u0026#39;comment\u0026#39; = \u0026#39;table comment\u0026#39; ); Removing Table Comment #  The following SQL removes table comment.\nALTER TABLE my_table UNSET TBLPROPERTIES (\u0026#39;comment\u0026#39;); Rename Table Name #  The following SQL rename the table name to new name.\nThe simplest sql to call is:\nALTER TABLE my_table RENAME TO my_table_new; Note that: we can rename paimon table in spark this way:\nALTER TABLE [catalog.[database.]]test1 RENAME to [database.]test2; But we can\u0026rsquo;t put catalog name before the renamed-to table, it will throw an error if we write sql like this:\nALTER TABLE catalog.database.test1 RENAME to catalog.database.test2; If you use object storage, such as S3 or OSS, please use this syntax carefully, because the renaming of object storage is not atomic, and only partial files may be moved in case of failure.  Adding New Columns #  The following SQL adds two columns c1 and c2 to table my_table.\nALTER TABLE my_table ADD COLUMNS ( c1 INT, c2 STRING ); Renaming Column Name #  The following SQL renames column c0 in table my_table to c1.\nALTER TABLE my_table RENAME COLUMN c0 TO c1; Dropping Columns #  The following SQL drops two columns c1 and c2 from table my_table.\nALTER TABLE my_table DROP COLUMNS (c1, c2); Dropping Partitions #  The following SQL drops the partitions of the paimon table. For spark sql, you need to specify all the partition columns.\nALTER TABLE my_table DROP PARTITION (`id` = 1, `name` = \u0026#39;paimon\u0026#39;); Changing Column Comment #  The following SQL changes comment of column buy_count to buy count.\nALTER TABLE my_table ALTER COLUMN buy_count COMMENT \u0026#39;buy count\u0026#39;; Adding Column Position #  ALTER TABLE my_table ADD COLUMN c INT FIRST; ALTER TABLE my_table ADD COLUMN c INT AFTER b; Changing Column Position #  ALTER TABLE my_table ALTER COLUMN col_a FIRST; ALTER TABLE my_table ALTER COLUMN col_a AFTER col_b; Changing Column Type #  ALTER TABLE my_table ALTER COLUMN col_a TYPE DOUBLE; "});index.add({'id':69,'href':'/docs/0.9/concepts/spec/tableindex/','title':"Table Index",'section':"Specification",'content':"Table index #  Table Index files is in the index directory.\nDynamic Bucket Index #  Dynamic bucket index is used to store the correspondence between the hash value of the primary-key and the bucket.\nIts structure is very simple, only storing hash values in the file:\nHASH_VALUE | HASH_VALUE | HASH_VALUE | HASH_VALUE | \u0026hellip;\nHASH_VALUE is the hash value of the primary-key. 4 bytes, BIT_ENDIAN.\nDeletion Vectors #  Deletion file is used to store the deleted records position for each data file. Each bucket has one deletion file for primary key table.\nThe deletion file is a binary file, and the format is as follows:\n First, record version by a byte. Current version is 1. Then, record \u0026lt;size of serialized bin, serialized bin, checksum of serialized bin\u0026gt; in sequence. Size and checksum are BIT_ENDIAN Integer.  For each serialized bin:\n First, record a const magic number by an int (BIT_ENDIAN). Current the magic number is 1581511376. Then, record serialized bitmap. Which is a RoaringBitmap (org.roaringbitmap.RoaringBitmap).  "});index.add({'id':70,'href':'/docs/0.9/spark/auxiliary/','title':"Auxiliary",'section':"Engine Spark",'content':"Auxiliary Statements #  Set / Reset #  The SET command sets a property, returns the value of an existing property or returns all SQLConf properties with value and meaning. The RESET command resets runtime configurations specific to the current session which were set via the SET command to their default values. To set paimon configs specifically, you need add the spark.paimon. prefix.\n-- set spark conf SET spark.sql.sources.partitionOverwriteMode=dynamic; -- set paimon conf SET spark.paimon.file.block-size=512M; -- reset conf RESET spark.paimon.file.block-size; Describe table #  DESCRIBE TABLE statement returns the basic metadata information of a table. The metadata information includes column name, column type and column comment.\n-- describe table DESCRIBE TABLE my_table; -- describe table with additional metadata DESCRIBE TABLE EXTENDED my_table; Show create table #  SHOW CREATE TABLE returns the CREATE TABLE statement that was used to create a given table.\nSHOW CREATE TABLE my_table; Show columns #  Returns the list of columns in a table. If the table does not exist, an exception is thrown.\nSHOW COLUMNS FROM my_table; Show partitions #  The SHOW PARTITIONS statement is used to list partitions of a table. An optional partition spec may be specified to return the partitions matching the supplied partition spec.\n-- Lists all partitions for my_table SHOW PARTITIONS my_table; -- Lists partitions matching the supplied partition spec for my_table SHOW PARTITIONS my_table PARTITION (dt=20230817); Analyze table #  The ANALYZE TABLE statement collects statistics about the table, that are to be used by the query optimizer to find a better query execution plan. Paimon supports collecting table-level statistics and column statistics through analyze.\n-- collect table-level statistics ANALYZE TABLE my_table COMPUTE STATISTICS; -- collect table-level statistics and column statistics for col1 ANALYZE TABLE my_table COMPUTE STATISTICS FOR COLUMNS col1; -- collect table-level statistics and column statistics for all columns ANALYZE TABLE my_table COMPUTE STATISTICS FOR ALL COLUMNS; "});index.add({'id':71,'href':'/docs/0.9/primary-key-table/compaction/','title':"Compaction",'section':"Table with PK",'content':"Compaction #  When more and more records are written into the LSM tree, the number of sorted runs will increase. Because querying an LSM tree requires all sorted runs to be combined, too many sorted runs will result in a poor query performance, or even out of memory.\nTo limit the number of sorted runs, we have to merge several sorted runs into one big sorted run once in a while. This procedure is called compaction.\nHowever, compaction is a resource intensive procedure which consumes a certain amount of CPU time and disk IO, so too frequent compaction may in turn result in slower writes. It is a trade-off between query and write performance. Paimon currently adapts a compaction strategy similar to Rocksdb\u0026rsquo;s universal compaction.\nCompaction solves:\n Reduce Level 0 files to avoid poor query performance. Produce changelog via changelog-producer. Produce deletion vectors for MOW mode. Snapshot Expiration, Tag Expiration, Partitions Expiration.  Limitation:\n There can only be one job working on the same partition\u0026rsquo;s compaction, otherwise it will cause conflicts and one side will throw an exception failure.  Writing performance is almost always affected by compaction, so its tuning is crucial.\nAsynchronous Compaction #  Compaction is inherently asynchronous, but if you want it to be completely asynchronous and not blocking writing, expect a mode to have maximum writing throughput, the compaction can be done slowly and not in a hurry. You can use the following strategies for your table:\nnum-sorted-run.stop-trigger = 2147483647 sort-spill-threshold = 10 lookup-wait = false This configuration will generate more files during peak write periods and gradually merge into optimal read performance during low write periods.\nDedicated compaction job #  In general, if you expect multiple jobs to be written to the same table, you need to separate the compaction. You can use dedicated compaction job.\nRecord-Level expire #  In compaction, you can configure record-Level expire time to expire records, you should configure:\n 'record-level.expire-time': time retain for records. 'record-level.time-field': time field for record level expire. 'record-level.time-field-type': time field type for record level expire, it can be seconds-int or millis-long.  Expiration happens in compaction, and there is no strong guarantee to expire records in time.\nFull Compaction #  Paimon Compaction uses Universal-Compaction. By default, when there is too much incremental data, Full Compaction will be automatically performed. You don\u0026rsquo;t usually have to worry about it.\nPaimon also provides a configuration that allows for regular execution of Full Compaction.\n \u0026lsquo;compaction.optimization-interval\u0026rsquo;: Implying how often to perform an optimization full compaction, this configuration is used to ensure the query timeliness of the read-optimized system table. \u0026lsquo;full-compaction.delta-commits\u0026rsquo;: Full compaction will be constantly triggered after delta commits. its disadvantage is that it can only perform compaction synchronously, which will affect writing efficiency.  Compaction Options #  Number of Sorted Runs to Pause Writing #  When the number of sorted runs is small, Paimon writers will perform compaction asynchronously in separated threads, so records can be continuously written into the table. However, to avoid unbounded growth of sorted runs, writers will pause writing when the number of sorted runs hits the threshold. The following table property determines the threshold.\n  Option Required Default Type Description     num-sorted-run.stop-trigger No (none) Integer The number of sorted runs that trigger the stopping of writes, the default value is 'num-sorted-run.compaction-trigger' + 3.    Write stalls will become less frequent when num-sorted-run.stop-trigger becomes larger, thus improving writing performance. However, if this value becomes too large, more memory and CPU time will be needed when querying the table. If you are concerned about the OOM problem, please configure the following option. Its value depends on your memory size.\n  Option Required Default Type Description     sort-spill-threshold No (none) Integer If the maximum number of sort readers exceeds this value, a spill will be attempted. This prevents too many readers from consuming too much memory and causing OOM.    Number of Sorted Runs to Trigger Compaction #  Paimon uses LSM tree which supports a large number of updates. LSM organizes files in several sorted runs. When querying records from an LSM tree, all sorted runs must be combined to produce a complete view of all records.\nOne can easily see that too many sorted runs will result in poor query performance. To keep the number of sorted runs in a reasonable range, Paimon writers will automatically perform compactions. The following table property determines the minimum number of sorted runs to trigger a compaction.\n  Option Required Default Type Description     num-sorted-run.compaction-trigger No 5 Integer The sorted run number to trigger compaction. Includes level0 files (one file one sorted run) and high-level runs (one level one sorted run).    Compaction will become less frequent when num-sorted-run.compaction-trigger becomes larger, thus improving writing performance. However, if this value becomes too large, more memory and CPU time will be needed when querying the table. This is a trade-off between writing and query performance.\n"});index.add({'id':72,'href':'/docs/0.9/concepts/spec/fileindex/','title':"File Index",'section':"Specification",'content':"File index #  Define file-index.${index_type}.columns, Paimon will create its corresponding index file for each file. If the index file is too small, it will be stored directly in the manifest, or in the directory of the data file. Each data file corresponds to an index file, which has a separate file definition and can contain different types of indexes with multiple columns.\nIndex File #  File index file format. Put all column and offset in the header.\n _____________________________________ _____________________ ｜ magic ｜version｜head length ｜ ｜-------------------------------------｜ ｜ column number ｜ ｜-------------------------------------｜ ｜ column 1 ｜ index number ｜ ｜-------------------------------------｜ ｜ index name 1 ｜start pos ｜length ｜ ｜-------------------------------------｜ ｜ index name 2 ｜start pos ｜length ｜ ｜-------------------------------------｜ ｜ index name 3 ｜start pos ｜length ｜ ｜-------------------------------------｜ HEAD ｜ column 2 ｜ index number ｜ ｜-------------------------------------｜ ｜ index name 1 ｜start pos ｜length ｜ ｜-------------------------------------｜ ｜ index name 2 ｜start pos ｜length ｜ ｜-------------------------------------｜ ｜ index name 3 ｜start pos ｜length ｜ ｜-------------------------------------｜ ｜ ... ｜ ｜-------------------------------------｜ ｜ ... ｜ ｜-------------------------------------｜ ｜ redundant length ｜redundant bytes ｜ ｜-------------------------------------｜ --------------------- ｜ BODY ｜ ｜ BODY ｜ ｜ BODY ｜ BODY ｜ BODY ｜ ｜_____________________________________｜ _____________________ * magic: 8 bytes long, value is 1493475289347502L, BIT_ENDIAN version: 4 bytes int, BIT_ENDIAN head length: 4 bytes int, BIT_ENDIAN column number: 4 bytes int, BIT_ENDIAN column x name: 2 bytes short BIT_ENDIAN and Java modified-utf-8 index number: 4 bytes int (how many column items below), BIT_ENDIAN index name x: 2 bytes short BIT_ENDIAN and Java modified-utf-8 start pos: 4 bytes int, BIT_ENDIAN length: 4 bytes int, BIT_ENDIAN redundant length: 4 bytes int (for compatibility with later versions, in this version, content is zero) redundant bytes: var bytes (for compatibility with later version, in this version, is empty) BODY: column index bytes + column index bytes + column index bytes + .......  Column Index Bytes: BloomFilter #  Define 'file-index.bloom-filter.columns'.\nContent of bloom filter index is simple:\n numHashFunctions 4 bytes int, BIT_ENDIAN bloom filter bytes  This class use (64-bits) long hash. Store the num hash function (one integer) and bit set bytes only. Hash bytes type (like varchar, binary, etc.) using xx hash, hash numeric type by specified number hash.\nColumn Index Bytes: Bitmap #  Define 'file-index.bitmap.columns'.\nBitmap file index format (V1):\n Bitmap file index format (V1) +-------------------------------------------------+----------------- ｜ version (1 byte) ｜ +-------------------------------------------------+ ｜ row count (4 bytes int) ｜ +-------------------------------------------------+ ｜ non-null value bitmap number (4 bytes int) ｜ +-------------------------------------------------+ ｜ has null value (1 byte) ｜ +-------------------------------------------------+ ｜ null value offset (4 bytes if has null value) ｜ HEAD +-------------------------------------------------+ ｜ value 1 | offset 1 ｜ +-------------------------------------------------+ ｜ value 2 | offset 2 ｜ +-------------------------------------------------+ ｜ value 3 | offset 3 ｜ +-------------------------------------------------+ ｜ ... ｜ +-------------------------------------------------+----------------- ｜ serialized bitmap 1 ｜ +-------------------------------------------------+ ｜ serialized bitmap 2 ｜ +-------------------------------------------------+ BODY ｜ serialized bitmap 3 ｜ +-------------------------------------------------+ ｜ ... ｜ +-------------------------------------------------+----------------- * value x: var bytes for any data type (as bitmap identifier) offset: 4 bytes int (when it is negative, it represents that there is only one value and its position is the inverse of the negative value)  Integer are all BIT_ENDIAN.\n"});index.add({'id':73,'href':'/docs/0.9/maintenance/rescale-bucket/','title':"Rescale Bucket",'section':"Maintenance",'content':"Rescale Bucket #  Since the number of total buckets dramatically influences the performance, Paimon allows users to tune bucket numbers by ALTER TABLE command and reorganize data layout by INSERT OVERWRITE without recreating the table/partition. When executing overwrite jobs, the framework will automatically scan the data with the old bucket number and hash the record according to the current bucket number.\nRescale Overwrite #  -- rescale number of total buckets ALTER TABLE table_identifier SET (\u0026#39;bucket\u0026#39; = \u0026#39;...\u0026#39;); -- reorganize data layout of table/partition INSERT OVERWRITE table_identifier [PARTITION (part_spec)] SELECT ... FROM table_identifier [WHERE part_spec]; Please note that\n ALTER TABLE only modifies the table\u0026rsquo;s metadata and will NOT reorganize or reformat existing data. Reorganize existing data must be achieved by INSERT OVERWRITE. Rescale bucket number does not influence the read and running write jobs. Once the bucket number is changed, any newly scheduled INSERT INTO jobs which write to without-reorganized existing table/partition will throw a TableException with message like Try to write table/partition ... with a new bucket num ..., but the previous bucket num is ... Please switch to batch mode, and perform INSERT OVERWRITE to rescale current data layout first.  For partitioned table, it is possible to have different bucket number for different partitions. E.g. ALTER TABLE my_table SET (\u0026#39;bucket\u0026#39; = \u0026#39;4\u0026#39;); INSERT OVERWRITE my_table PARTITION (dt = \u0026#39;2022-01-01\u0026#39;) SELECT * FROM ...; ALTER TABLE my_table SET (\u0026#39;bucket\u0026#39; = \u0026#39;8\u0026#39;); INSERT OVERWRITE my_table PARTITION (dt = \u0026#39;2022-01-02\u0026#39;) SELECT * FROM ...;  During overwrite period, make sure there are no other jobs writing the same table/partition.  Note: For the table which enables log system(e.g. Kafka), please rescale the topic\u0026rsquo;s partition as well to keep consistency.  Use Case #  Rescale bucket helps to handle sudden spikes in throughput. Suppose there is a daily streaming ETL task to sync transaction data. The table\u0026rsquo;s DDL and pipeline are listed as follows.\n-- table DDL CREATE TABLE verified_orders ( trade_order_id BIGINT, item_id BIGINT, item_price DOUBLE, dt STRING, PRIMARY KEY (dt, trade_order_id, item_id) NOT ENFORCED ) PARTITIONED BY (dt) WITH ( \u0026#39;bucket\u0026#39; = \u0026#39;16\u0026#39; ); -- like from a kafka table CREATE temporary TABLE raw_orders( trade_order_id BIGINT, item_id BIGINT, item_price BIGINT, gmt_create STRING, order_status STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ... ); -- streaming insert as bucket num = 16 INSERT INTO verified_orders SELECT trade_order_id, item_id, item_price, DATE_FORMAT(gmt_create, \u0026#39;yyyy-MM-dd\u0026#39;) AS dt FROM raw_orders WHERE order_status = \u0026#39;verified\u0026#39;; The pipeline has been running well for the past few weeks. However, the data volume has grown fast recently, and the job\u0026rsquo;s latency keeps increasing. To improve the data freshness, users can\n Suspend the streaming job with a savepoint ( see Suspended State and Stopping a Job Gracefully Creating a Final Savepoint ) $ ./bin/flink stop \\  --savepointPath /tmp/flink-savepoints \\  $JOB_ID  Increase the bucket number -- scaling out ALTER TABLE verified_orders SET (\u0026#39;bucket\u0026#39; = \u0026#39;32\u0026#39;);  Switch to the batch mode and overwrite the current partition(s) to which the streaming job is writing SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;batch\u0026#39;; -- suppose today is 2022-06-22 -- case 1: there is no late event which updates the historical partitions, thus overwrite today\u0026#39;s partition is enough INSERT OVERWRITE verified_orders PARTITION (dt = \u0026#39;2022-06-22\u0026#39;) SELECT trade_order_id, item_id, item_price FROM verified_orders WHERE dt = \u0026#39;2022-06-22\u0026#39;; -- case 2: there are late events updating the historical partitions, but the range does not exceed 3 days INSERT OVERWRITE verified_orders SELECT trade_order_id, item_id, item_price, dt FROM verified_orders WHERE dt IN (\u0026#39;2022-06-20\u0026#39;, \u0026#39;2022-06-21\u0026#39;, \u0026#39;2022-06-22\u0026#39;);  After overwrite job has finished, switch back to streaming mode. And now, the parallelism can be increased alongside with bucket number to restore the streaming job from the savepoint ( see Start a SQL Job from a savepoint ) SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;streaming\u0026#39;; SET \u0026#39;execution.savepoint.path\u0026#39; = \u0026lt;savepointPath\u0026gt;; INSERT INTO verified_orders SELECT trade_order_id, item_id, item_price, DATE_FORMAT(gmt_create, \u0026#39;yyyy-MM-dd\u0026#39;) AS dt FROM raw_orders WHERE order_status = \u0026#39;verified\u0026#39;;   "});index.add({'id':74,'href':'/docs/0.9/maintenance/manage-tags/','title':"Manage Tags",'section':"Maintenance",'content':"Manage Tags #  Paimon\u0026rsquo;s snapshots can provide an easy way to query historical data. But in most scenarios, a job will generate too many snapshots and table will expire old snapshots according to table configuration. Snapshot expiration will also delete old data files, and the historical data of expired snapshots cannot be queried anymore.\nTo solve this problem, you can create a tag based on a snapshot. The tag will maintain the manifests and data files of the snapshot. A typical usage is creating tags daily, then you can maintain the historical data of each day for batch reading.\nAutomatic Creation #  Paimon supports automatic creation of tags in writing job.\nStep 1: Choose Creation Mode\nYou can set creation mode by table option 'tag.automatic-creation'. Supported values are:\n process-time: Create TAG based on the time of the machine. watermark: Create TAG based on the watermark of the Sink input. batch: In a batch processing scenario, a tag is generated after the current task is completed.  If you choose Watermark, you may need to specify the time zone of watermark, if watermark is not in the UTC time zone, please configure 'sink.watermark-time-zone'.  Step 2: Choose Creation Period\nWhat frequency is used to generate tags. You can choose 'daily', 'hourly' and 'two-hours' for 'tag.creation-period'.\nIf you need to wait for late data, you can configure a delay time: 'tag.creation-delay'.\nStep 3: Automatic deletion of tags\nYou can configure 'tag.num-retained-max' or tag.default-time-retained to delete tags automatically.\nExample, configure table to create a tag at 0:10 every day, with a maximum retention time of 3 months:\n-- Flink SQL CREATE TABLE t ( k INT PRIMARY KEY NOT ENFORCED, f0 INT, ... ) WITH ( \u0026#39;tag.automatic-creation\u0026#39; = \u0026#39;process-time\u0026#39;, \u0026#39;tag.creation-period\u0026#39; = \u0026#39;daily\u0026#39;, \u0026#39;tag.creation-delay\u0026#39; = \u0026#39;10 m\u0026#39;, \u0026#39;tag.num-retained-max\u0026#39; = \u0026#39;90\u0026#39; ); INSERT INTO t SELECT ...; -- Spark SQL  -- Read latest snapshot SELECT * FROM t; -- Read Tag snapshot SELECT * FROM t VERSION AS OF \u0026#39;2023-07-26\u0026#39;; -- Read Incremental between Tags SELECT * FROM paimon_incremental_query(\u0026#39;t\u0026#39;, \u0026#39;2023-07-25\u0026#39;, \u0026#39;2023-07-26\u0026#39;); See Query Tables to see more query for Spark.\nCreate Tags #  You can create a tag with given name and snapshot ID.\nFlink Run the following command:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  create_tag \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  --tag_name \u0026lt;tag-name\u0026gt; \\  [--snapshot \u0026lt;snapshot_id\u0026gt;] \\  [--time_retained \u0026lt;time-retained\u0026gt;] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] If snapshot unset, snapshot_id defaults to the latest.\nJava API import org.apache.paimon.table.Table; public class CreateTag { public static void main(String[] args) { Table table = ...; table.createTag(\u0026#34;my-tag\u0026#34;, 1); table.createTag(\u0026#34;my-tag-retained-12-hours\u0026#34;, 1, Duration.ofHours(12)); } } Spark Run the following sql:\nCALL sys.create_tag(table =\u0026gt; \u0026#39;test.t\u0026#39;, tag =\u0026gt; \u0026#39;test_tag\u0026#39;, snapshot =\u0026gt; 2); To create a tag with retained 1 day, run the following sql:\nCALL sys.create_tag(table =\u0026gt; \u0026#39;test.t\u0026#39;, tag =\u0026gt; \u0026#39;test_tag\u0026#39;, snapshot =\u0026gt; 2, time_retained =\u0026gt; \u0026#39;1 d\u0026#39;); To create a tag based on the latest snapshot id, run the following sql:\nCALL sys.create_tag(table =\u0026gt; \u0026#39;test.t\u0026#39;, tag =\u0026gt; \u0026#39;test_tag\u0026#39;);  Delete Tags #  You can delete a tag by its name.\nFlink Run the following command:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  delete_tag \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  --tag_name \u0026lt;tag-name\u0026gt; \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] Java API import org.apache.paimon.table.Table; public class DeleteTag { public static void main(String[] args) { Table table = ...; table.deleteTag(\u0026#34;my-tag\u0026#34;); } } Spark Run the following sql:\nCALL sys.delete_tag(table =\u0026gt; \u0026#39;test.t\u0026#39;, tag =\u0026gt; \u0026#39;test_tag\u0026#39;);  Rollback to Tag #  Rollback table to a specific tag. All snapshots and tags whose snapshot id is larger than the tag will be deleted (and the data will be deleted too).\nFlink Run the following command:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  rollback_to \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  --version \u0026lt;tag-name\u0026gt; \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] Java API import org.apache.paimon.table.Table; public class RollbackTo { public static void main(String[] args) { // before rollback:  // snapshot-3 [expired] -\u0026gt; tag3  // snapshot-4 [expired]  // snapshot-5 -\u0026gt; tag5  // snapshot-6  // snapshot-7  table.rollbackTo(\u0026#34;tag3\u0026#34;); // after rollback:  // snapshot-3 -\u0026gt; tag3  } } Spark Run the following sql:\nCALL sys.rollback(table =\u0026gt; \u0026#39;test.t\u0026#39;, version =\u0026gt; \u0026#39;2\u0026#39;);  "});index.add({'id':75,'href':'/docs/0.9/maintenance/metrics/','title':"Metrics",'section':"Maintenance",'content':"Paimon Metrics #  Paimon has built a metrics system to measure the behaviours of reading and writing, like how many manifest files it scanned in the last planning, how long it took in the last commit operation, how many files it deleted in the last compact operation.\nIn Paimon\u0026rsquo;s metrics system, metrics are updated and reported at table granularity.\nThere are three types of metrics provided in the Paimon metric system, Gauge, Counter, Histogram.\n Gauge: Provides a value of any type at a point in time. Counter: Used to count values by incrementing and decrementing. Histogram: Measure the statistical distribution of a set of values including the min, max, mean, standard deviation and percentile.  Paimon has supported built-in metrics to measure operations of commits, scans, writes and compactions, which can be bridged to any computing engine that supports, like Flink, Spark etc.\nMetrics List #  Below is lists of Paimon built-in metrics. They are summarized into types of scan metrics, commit metrics, write metrics, write buffer metrics and compaction metrics.\nScan Metrics #    Metrics Name Type Description     lastScanDuration Gauge The time it took to complete the last scan.   scanDuration Histogram Distributions of the time taken by the last few scans.   lastScannedManifests Gauge Number of scanned manifest files in the last scan.   lastSkippedByPartitionAndStats Gauge Skipped table files by partition filter and value / key stats information in the last scan.   lastSkippedByBucketAndLevelFilter Gauge Skipped table files by bucket, bucket key and level filter in the last scan.   lastSkippedByWholeBucketFilesFilter Gauge Skipped table files by bucket level value filter (only primary key table) in the last scan.   lastScanSkippedTableFiles Gauge Total skipped table files in the last scan.   lastScanResultedTableFiles Gauge Resulted table files in the last scan.    Commit Metrics #    Metrics Name Type Description     lastCommitDuration Gauge The time it took to complete the last commit.   commitDuration Histogram Distributions of the time taken by the last few commits.   lastCommitAttempts Gauge The number of attempts the last commit made.   lastTableFilesAdded Gauge Number of added table files in the last commit, including newly created data files and compacted after.   lastTableFilesDeleted Gauge Number of deleted table files in the last commit, which comes from compacted before.   lastTableFilesAppended Gauge Number of appended table files in the last commit, which means the newly created data files.   lastTableFilesCommitCompacted Gauge Number of compacted table files in the last commit, including compacted before and after.   lastChangelogFilesAppended Gauge Number of appended changelog files in last commit.   lastChangelogFileCommitCompacted Gauge Number of compacted changelog files in last commit.   lastGeneratedSnapshots Gauge Number of snapshot files generated in the last commit, maybe 1 snapshot or 2 snapshots.   lastDeltaRecordsAppended Gauge Delta records count in last commit with APPEND commit kind.   lastChangelogRecordsAppended Gauge Changelog records count in last commit with APPEND commit kind.   lastDeltaRecordsCommitCompacted Gauge Delta records count in last commit with COMPACT commit kind.   lastChangelogRecordsCommitCompacted Gauge Changelog records count in last commit with COMPACT commit kind.   lastPartitionsWritten Gauge Number of partitions written in the last commit.   lastBucketsWritten Gauge Number of buckets written in the last commit.    Write Buffer Metrics #    Metrics Name Type Description     numWriters Gauge Number of writers in this parallelism.   bufferPreemptCount Gauge The total number of memory preempted.   usedWriteBufferSizeByte Gauge Current used write buffer size in byte.   totalWriteBufferSizeByte Gauge The total write buffer size configured in byte.    Compaction Metrics #    Metrics Name Type Description     maxLevel0FileCount Gauge The maximum number of level 0 files currently handled by this writer. This value will become larger if asynchronous compaction cannot be done in time.   avgLevel0FileCount Gauge The average number of level 0 files currently handled by this writer. This value will become larger if asynchronous compaction cannot be done in time.   compactionThreadBusy Gauge The maximum business of compaction threads in this parallelism. Currently, there is only one compaction thread in each parallelism, so value of business ranges from 0 (idle) to 100 (compaction running all the time).   avgCompactionTime Gauge The average runtime of compaction threads, calculated based on recorded compaction time data in milliseconds. The value represents the average duration of compaction operations. Higher values indicate longer average compaction times, which may suggest the need for performance optimization.    Bridging To Flink #  Paimon has implemented bridging metrics to Flink\u0026rsquo;s metrics system, which can be reported by Flink, and the lifecycle of metric groups are managed by Flink.\nPlease join the \u0026lt;scope\u0026gt;.\u0026lt;infix\u0026gt;.\u0026lt;metric_name\u0026gt; to get the complete metric identifier when using Flink to access Paimon, metric_name can be got from Metric List.\nFor example, the identifier of metric lastPartitionsWritten for table word_count in Flink job named insert_word_count is:\nlocalhost.taskmanager.localhost:60340-775a20.insert_word_count.Global Committer : word_count.0.paimon.table.word_count.commit.lastPartitionsWritten.\nFrom Flink Web-UI, go to the committer operator\u0026rsquo;s metrics, it\u0026rsquo;s shown as:\n0.Global_Committer___word_count.paimon.table.word_count.commit.lastPartitionsWritten.\n Please refer to System Scope to understand Flink scope Scan metrics are only supported by Flink versions \u0026gt;= 1.18      Scope Infix     Scan Metrics \u0026lt;host\u0026gt;.jobmanager.\u0026lt;job_name\u0026gt; \u0026lt;source_operator_name\u0026gt;.coordinator. enumerator.paimon.table.\u0026lt;table_name\u0026gt;.scan   Commit Metrics \u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt;.\u0026lt;job_name\u0026gt;.\u0026lt;committer_operator_name\u0026gt;.\u0026lt;subtask_index\u0026gt; paimon.table.\u0026lt;table_name\u0026gt;.commit   Write Metrics \u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt;.\u0026lt;job_name\u0026gt;.\u0026lt;writer_operator_name\u0026gt;.\u0026lt;subtask_index\u0026gt; paimon.table.\u0026lt;table_name\u0026gt;.partition.\u0026lt;partition_string\u0026gt;.bucket.\u0026lt;bucket_index\u0026gt;.writer   Write Buffer Metrics \u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt;.\u0026lt;job_name\u0026gt;.\u0026lt;writer_operator_name\u0026gt;.\u0026lt;subtask_index\u0026gt; paimon.table.\u0026lt;table_name\u0026gt;.writeBuffer   Compaction Metrics \u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt;.\u0026lt;job_name\u0026gt;.\u0026lt;writer_operator_name\u0026gt;.\u0026lt;subtask_index\u0026gt; paimon.table.\u0026lt;table_name\u0026gt;.partition.\u0026lt;partition_string\u0026gt;.bucket.\u0026lt;bucket_index\u0026gt;.compaction   Flink Source Metrics \u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt;.\u0026lt;job_name\u0026gt;.\u0026lt;source_operator_name\u0026gt;.\u0026lt;subtask_index\u0026gt; -   Flink Sink Metrics \u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt;.\u0026lt;job_name\u0026gt;.\u0026lt;committer_operator_name\u0026gt;.\u0026lt;subtask_index\u0026gt; -    Flink Connector Standard Metrics #  When using Flink to read and write, Paimon has implemented some key standard Flink connector metrics to measure the source latency and output of sink, see FLIP-33: Standardize Connector Metrics. Flink source / sink metrics implemented are listed here.\nSource Metrics (Flink) #    Metrics Name Level Type Description     currentEmitEventTimeLag Flink Source Operator Gauge Time difference between sending the record out of source and file creation.   currentFetchEventTimeLag Flink Source Operator Gauge Time difference between reading the data file and file creation.    Please note that if you specified consumer-id in your streaming query, the level of source metrics should turn into the reader operator, which is behind the Monitor operator.  Sink Metrics (Flink) #    Metrics Name Level Type Description     numBytesOut Table Counter The total number of output bytes.   numBytesOutPerSecond Table Meter The output bytes per second.   numRecordsOut Table Counter The total number of output records.   numRecordsOutPerSecond Table Meter The output records per second.    "});index.add({'id':76,'href':'/docs/0.9/maintenance/manage-privileges/','title':"Manage Privileges",'section':"Maintenance",'content':"Manage Privileges #  Paimon provides a privilege system on catalogs. Privileges determine which users can perform which operations on which objects, so that you can manage table access in a fine-grained manner.\nCurrently, Paimon adopts the identity-based access control (IBAC) privilege model. That is, privileges are directly assigned to users.\nThis privilege system only prevents unwanted users from accessing tables through catalogs. It does not block access through temporary table (by specifying table path on filesystem), nor does it prevent user from directly modifying data files on filesystem. If you need more serious protection, use a filesystem with access management instead.  Basic Concepts #  We now introduce the basic concepts of the privilege system.\nObject #  An object is an entity to which access can be granted. Unless allowed by a grant, access is denied.\nCurrently, the privilege system in Paimon has three types of objects: CATALOG, DATABASE and TABLE. Objects have a logical hierarchy, which is related to the concept they represent. For example:\n If a user is granted a privilege on the catalog, he will also have this privilege on all databases and all tables in the catalog. If a user is granted a privilege on the database, he will also have this privilege on all tables in that database. If a user is revoked a privilege from the catalog, he will also lose this privilege on all databases and all tables in the catalog. If a user is revoked a privilege from the database, he will also lose this privilege on all tables in that database.  Privilege #  A privilege is a defined level of access to an object. Multiple privileges can be used to control the granularity of access granted on an object. Privileges are object-specific. Different objects may have different privileges.\nCurrently, we support the following privileges.\n   Privilege Description Can be Granted on     SELECT Queries data in a table. TABLE, DATABASE, CATALOG   INSERT Inserts, updates or drops data in a table. Creates or drops tags and branches in a table. TABLE, DATABASE, CATALOG   ALTER_TABLE Alters metadata of a table, including table name, column names, table options, etc. TABLE, DATABASE, CATALOG   DROP_TABLE Drops a table. TABLE, DATABASE, CATALOG   CREATE_TABLE Creates a table in a database. DATABASE, CATALOG   DROP_DATABASE Drops a database. DATABASE, CATALOG   CREATE_DATABASE Creates a database in the catalog. CATALOG   ADMIN Creates or drops privileged users, grants or revokes privileges from users in a catalog. CATALOG    User #  The entity to which privileges can be granted. Users are authenticated by their password.\nWhen the privilege system is enabled, two special users will be created automatically.\n The root user, which is identified by the provided root password when enabling the privilege system. This user always has all privileges in the catalog. The anonymous user. This is the default user if no username and password is provided when creating the catalog.  Enable Privileges #  Paimon currently only supports file-based privilege system. Only catalogs with 'metastore' = 'filesystem' (the default value) or 'metastore' = 'hive' support such privilege system.\nTo enable the privilege system on a filesystem / Hive catalog, do the following steps.\nFlink 1.18\u0026#43; Run the following Flink SQL.\n-- use the catalog where you want to enable the privilege system USE CATALOG `my-catalog`; -- initialize privilege system by providing a root password -- change \u0026#39;root-password\u0026#39; to the password you want CALL sys.init_file_based_privilege(\u0026#39;root-password\u0026#39;);  After the privilege system is enabled, please re-create the catalog and authenticate as root to create other users and grant them privileges.\nPrivilege system does not affect existing catalogs. That is, these catalogs can still access and modify the tables freely. Please drop and re-create all catalogs with the desired warehouse path if you want to use the privilege system in these catalogs.  Accessing Privileged Catalogs #  To access a privileged catalog and to be authenticated as a user, you need to define user and password catalog options when creating the catalog. For example, the following SQL creates a catalog while trying to be authenticated as root, whose password is mypassword.\nFlink CREATE CATALOG `my-catalog` WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, -- ...  \u0026#39;user\u0026#39; = \u0026#39;root\u0026#39;, \u0026#39;password\u0026#39; = \u0026#39;mypassword\u0026#39; );  Creating Users #  You must be authenticated as a user with ADMIN privilege (for example, root) to perform this operation.\nDo the following steps to create a user in the privilege system.\nFlink 1.18\u0026#43; Run the following Flink SQL.\n-- use the catalog where you want to create a user -- you must be authenticated as a user with ADMIN privilege in this catalog USE CATALOG `my-catalog`; -- create a user authenticated by the specified password -- change \u0026#39;user\u0026#39; and \u0026#39;password\u0026#39; to the username and password you want CALL sys.create_privileged_user(\u0026#39;user\u0026#39;, \u0026#39;password\u0026#39;);  Dropping Users #  You must be authenticated as a user with ADMIN privilege (for example, root) to perform this operation.\nDo the following steps to drop a user in the privilege system.\nFlink 1.18\u0026#43; Run the following Flink SQL.\n-- use the catalog where you want to drop a user -- you must be authenticated as a user with ADMIN privilege in this catalog USE CATALOG `my-catalog`; -- change \u0026#39;user\u0026#39; to the username you want to drop CALL sys.drop_privileged_user(\u0026#39;user\u0026#39;);  Granting Privileges to Users #  You must be authenticated as a user with ADMIN privilege (for example, root) to perform this operation.\nDo the following steps to grant a user with privilege in the privilege system.\nFlink 1.18\u0026#43; Run the following Flink SQL.\n-- use the catalog where you want to drop a user -- you must be authenticated as a user with ADMIN privilege in this catalog USE CATALOG `my-catalog`; -- you can change \u0026#39;user\u0026#39; to the username you want, and \u0026#39;SELECT\u0026#39; to other privilege you want -- grant \u0026#39;user\u0026#39; with privilege \u0026#39;SELECT\u0026#39; on the whole catalog CALL sys.grant_privilege_to_user(\u0026#39;user\u0026#39;, \u0026#39;SELECT\u0026#39;); -- grant \u0026#39;user\u0026#39; with privilege \u0026#39;SELECT\u0026#39; on database my_db CALL sys.grant_privilege_to_user(\u0026#39;user\u0026#39;, \u0026#39;SELECT\u0026#39;, \u0026#39;my_db\u0026#39;); -- grant \u0026#39;user\u0026#39; with privilege \u0026#39;SELECT\u0026#39; on table my_db.my_tbl CALL sys.grant_privilege_to_user(\u0026#39;user\u0026#39;, \u0026#39;SELECT\u0026#39;, \u0026#39;my_db\u0026#39;, \u0026#39;my_tbl\u0026#39;);  Revoking Privileges to Users #  You must be authenticated as a user with ADMIN privilege (for example, root) to perform this operation.\nDo the following steps to revoke a privilege from user in the privilege system.\nFlink 1.18\u0026#43; Run the following Flink SQL.\n-- use the catalog where you want to drop a user -- you must be authenticated as a user with ADMIN privilege in this catalog USE CATALOG `my-catalog`; -- you can change \u0026#39;user\u0026#39; to the username you want, and \u0026#39;SELECT\u0026#39; to other privilege you want -- revoke \u0026#39;user\u0026#39; with privilege \u0026#39;SELECT\u0026#39; on the whole catalog CALL sys.revoke_privilege_from_user(\u0026#39;user\u0026#39;, \u0026#39;SELECT\u0026#39;); -- revoke \u0026#39;user\u0026#39; with privilege \u0026#39;SELECT\u0026#39; on database my_db CALL sys.revoke_privilege_from_user(\u0026#39;user\u0026#39;, \u0026#39;SELECT\u0026#39;, \u0026#39;my_db\u0026#39;); -- revoke \u0026#39;user\u0026#39; with privilege \u0026#39;SELECT\u0026#39; on table my_db.my_tbl CALL sys.revoke_privilege_from_user(\u0026#39;user\u0026#39;, \u0026#39;SELECT\u0026#39;, \u0026#39;my_db\u0026#39;, \u0026#39;my_tbl\u0026#39;);  "});index.add({'id':77,'href':'/docs/0.9/maintenance/manage-branches/','title':"Manage Branches",'section':"Maintenance",'content':"Manage Branches #  In streaming data processing, it\u0026rsquo;s difficult to correct data for it may affect the existing data, and users will see the streaming provisional results, which is not expected.\nWe suppose the branch that the existing workflow is processing on is \u0026lsquo;main\u0026rsquo; branch, by creating custom data branch, it can help to do experimental tests and data validating for the new job on the existing table, which doesn\u0026rsquo;t need to stop the existing reading / writing workflows and no need to copy data from the main branch.\nBy merge or replace branch operations, users can complete the correcting of data.\nCreate Branches #  Paimon supports creating branch from a specific tag or snapshot, or just creating an empty branch which means the initial state of the created branch is like an empty table.\nFlink Run the following sql:\n-- create branch named \u0026#39;branch1\u0026#39; from tag \u0026#39;tag1\u0026#39; CALL sys.create_branch(\u0026#39;default.T\u0026#39;, \u0026#39;branch1\u0026#39;, \u0026#39;tag1\u0026#39;); -- create empty branch named \u0026#39;branch1\u0026#39; CALL sys.create_branch(\u0026#39;default.T\u0026#39;, \u0026#39;branch1\u0026#39;); Flink Action Jar Run the following command:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  create_branch \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  --branch_name \u0026lt;branch-name\u0026gt; \\  [--tag_name \u0026lt;tag-name\u0026gt;] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]]  Delete Branches #  You can delete branch by its name.\nFlink Run the following sql:\nCALL sys.delete_branch(\u0026#39;default.T\u0026#39;, \u0026#39;branch1\u0026#39;); Flink Action Jar Run the following command:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  delete_branch \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  --branch_name \u0026lt;branch-name\u0026gt; \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]]  Read / Write With Branch #  You can read or write with branch as below.\nFlink -- read from branch \u0026#39;branch1\u0026#39; SELECT * FROM `t$branch_branch1`; SELECT * FROM `t$branch_branch1` /*+ OPTIONS(\u0026#39;consumer-id\u0026#39; = \u0026#39;myid\u0026#39;) */; -- write to branch \u0026#39;branch1\u0026#39; INSERT INTO `t$branch_branch1` SELECT ...  Fast Forward #  Fast-Forward the custom branch to main will delete all the snapshots, tags and schemas in the main branch that are created after the branch\u0026rsquo;s initial tag. And copy snapshots, tags and schemas from the branch to the main branch.\nFlink CALL sys.fast_forward(\u0026#39;default.T\u0026#39;, \u0026#39;branch1\u0026#39;); Flink Action Jar Run the following command:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  fast_forward \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  --branch_name \u0026lt;branch-name\u0026gt; \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]]  Batch Reading from Fallback Branch #  You can set the table option scan.fallback-branch so that when a batch job reads from the current branch, if a partition does not exist, the reader will try to read this partition from the fallback branch. For streaming read jobs, this feature is currently not supported, and will only produce results from the current branch.\nWhat\u0026rsquo;s the use case of this feature? Say you have created a Paimon table partitioned by date. You have a long-running streaming job which inserts records into Paimon, so that today\u0026rsquo;s data can be queried in time. You also have a batch job which runs at every night to insert corrected records of yesterday into Paimon, so that the preciseness of the data can be promised.\nWhen you query from this Paimon table, you would like to first read from the results of batch job. But if a partition (for example, today\u0026rsquo;s partition) does not exist in its result, then you would like to read from the results of streaming job. In this case, you can create a branch for streaming job, and set scan.fallback-branch to this streaming branch.\nLet\u0026rsquo;s look at an example.\nFlink -- create Paimon table CREATE TABLE T ( dt STRING NOT NULL, name STRING NOT NULL, amount BIGINT ) PARTITIONED BY (dt); -- create a branch for streaming job CALL sys.create_branch(\u0026#39;default.T\u0026#39;, \u0026#39;test\u0026#39;); -- set primary key and bucket number for the branch ALTER TABLE `T$branch_test` SET ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,name\u0026#39;, \u0026#39;bucket\u0026#39; = \u0026#39;2\u0026#39;, \u0026#39;changelog-producer\u0026#39; = \u0026#39;lookup\u0026#39; ); -- set fallback branch ALTER TABLE T SET ( \u0026#39;scan.fallback-branch\u0026#39; = \u0026#39;test\u0026#39; ); -- write records into the streaming branch INSERT INTO `T$branch_test` VALUES (\u0026#39;20240725\u0026#39;, \u0026#39;apple\u0026#39;, 4), (\u0026#39;20240725\u0026#39;, \u0026#39;peach\u0026#39;, 10), (\u0026#39;20240726\u0026#39;, \u0026#39;cherry\u0026#39;, 3), (\u0026#39;20240726\u0026#39;, \u0026#39;pear\u0026#39;, 6); -- write records into the default branch INSERT INTO T VALUES (\u0026#39;20240725\u0026#39;, \u0026#39;apple\u0026#39;, 5), (\u0026#39;20240725\u0026#39;, \u0026#39;banana\u0026#39;, 7); SELECT * FROM T; /* +------------------+------------------+--------+ | dt | name | amount | +------------------+------------------+--------+ | 20240725 | apple | 5 | | 20240725 | banana | 7 | | 20240726 | cherry | 3 | | 20240726 | pear | 6 | +------------------+------------------+--------+ */ -- reset fallback branch ALTER TABLE T RESET ( \u0026#39;scan.fallback-branch\u0026#39; ); -- now it only reads from default branch SELECT * FROM T; /* +------------------+------------------+--------+ | dt | name | amount | +------------------+------------------+--------+ | 20240725 | apple | 5 | | 20240725 | banana | 7 | +------------------+------------------+--------+ */  "});index.add({'id':78,'href':'/docs/0.9/engines/','title':"Engine Others",'section':"Apache Paimon",'content':""});index.add({'id':79,'href':'/docs/0.9/filesystems/','title':"Filesystems",'section':"Apache Paimon",'content':""});index.add({'id':80,'href':'/docs/0.9/flink/cdc-ingestion/','title':"CDC Ingestion",'section':"Engine Flink",'content':""});index.add({'id':81,'href':'/docs/0.9/maintenance/','title':"Maintenance",'section':"Apache Paimon",'content':""});index.add({'id':82,'href':'/docs/0.9/flink/expire-partition/','title':"Expire Partition",'section':"Engine Flink",'content':"Expiring Partitions #  You can set partition.expiration-time when creating a partitioned table. Paimon streaming sink will periodically check the status of partitions and delete expired partitions according to time.\nHow to determine whether a partition has expired: you can set partition.expiration-strategy when creating a partitioned table, this strategy determines how to extract the partition time and compare it with the current time to see if survival time has exceeded the partition.expiration-time. Expiration strategy supported values are:\n values-time : The strategy compares the time extracted from the partition value with the current time, this strategy as the default. update-time : The strategy compares the last update time of the partition with the current time. What is the scenario for this strategy:  Your partition value is non-date formatted. You only want to keep data that has been updated in the last n days/months/years. Data initialization imports a large amount of historical data.    Note: After the partition expires, it is logically deleted and the latest snapshot cannot query its data. But the files in the file system are not immediately physically deleted, it depends on when the corresponding snapshot expires. See Expire Snapshots.  An example for single partition field:\nvalues-time strategy.\nCREATE TABLE t (...) PARTITIONED BY (dt) WITH ( \u0026#39;partition.expiration-time\u0026#39; = \u0026#39;7 d\u0026#39;, \u0026#39;partition.expiration-check-interval\u0026#39; = \u0026#39;1 d\u0026#39;, \u0026#39;partition.timestamp-formatter\u0026#39; = \u0026#39;yyyyMMdd\u0026#39; -- this is required in `values-time` strategy. ); -- Let\u0026#39;s say now the date is 2024-07-09，so before the date of 2024-07-02 will expire. insert into t values(\u0026#39;pk\u0026#39;, \u0026#39;2024-07-01\u0026#39;); -- An example for multiple partition fields CREATE TABLE t (...) PARTITIONED BY (other_key, dt) WITH ( \u0026#39;partition.expiration-time\u0026#39; = \u0026#39;7 d\u0026#39;, \u0026#39;partition.expiration-check-interval\u0026#39; = \u0026#39;1 d\u0026#39;, \u0026#39;partition.timestamp-formatter\u0026#39; = \u0026#39;yyyyMMdd\u0026#39;, \u0026#39;partition.timestamp-pattern\u0026#39; = \u0026#39;$dt\u0026#39; ); update-time strategy.\nCREATE TABLE t (...) PARTITIONED BY (dt) WITH ( \u0026#39;partition.expiration-time\u0026#39; = \u0026#39;7 d\u0026#39;, \u0026#39;partition.expiration-check-interval\u0026#39; = \u0026#39;1 d\u0026#39;, \u0026#39;partition.expiration-strategy\u0026#39; = \u0026#39;update-time\u0026#39; ); -- The last update time of the partition is now, so it will not expire. insert into t values(\u0026#39;pk\u0026#39;, \u0026#39;2024-01-01\u0026#39;); -- Support non-date formatted partition. insert into t values(\u0026#39;pk\u0026#39;, \u0026#39;par-1\u0026#39;); More options:\n  Option Default Type Description     partition.expiration-strategy values-time String  Specifies the expiration strategy for partition expiration. Possible values: values-time: The strategy compares the time extracted from the partition value with the current time. update-time: The strategy compares the last update time of the partition with the current time.    partition.expiration-check-interval 1 h Duration The check interval of partition expiration.   partition.expiration-time (none) Duration The expiration interval of a partition. A partition will be expired if it‘s lifetime is over this value. Partition time is extracted from the partition value.   partition.timestamp-formatter (none) String The formatter to format timestamp from string. It can be used with 'partition.timestamp-pattern' to create a formatter using the specified value.Default formatter is 'yyyy-MM-dd HH:mm:ss' and 'yyyy-MM-dd'.Supports multiple partition fields like '$year-$month-$day $hour:00:00'.The timestamp-formatter is compatible with Java's DateTimeFormatter.   partition.timestamp-pattern (none) String You can specify a pattern to get a timestamp from partitions. The formatter pattern is defined by 'partition.timestamp-formatter'.By default, read from the first field.If the timestamp in the partition is a single field called 'dt', you can use '$dt'.If it is spread across multiple fields for year, month, day, and hour, you can use '$year-$month-$day $hour:00:00'.If the timestamp is in fields dt and hour, you can use '$dt $hour:00:00'.   end-input.check-partition-expire false Boolean Whether check partition expire after batch mode or bounded stream job finish.    "});index.add({'id':83,'href':'/docs/0.9/program-api/','title':"Program API",'section':"Apache Paimon",'content':""});index.add({'id':84,'href':'/docs/0.9/migration/','title':"Migration",'section':"Apache Paimon",'content':""});index.add({'id':85,'href':'/docs/0.9/flink/procedures/','title':"Procedures",'section':"Engine Flink",'content':"Procedures #  Flink 1.18 and later versions support Call Statements, which make it easier to manipulate data and metadata of Paimon table by writing SQLs instead of submitting Flink jobs.\nIn 1.18, the procedure only supports passing arguments by position. You must pass all arguments in order, and if you don\u0026rsquo;t want to pass some arguments, you must use '' as placeholder. For example, if you want to compact table default.t with parallelism 4, but you don\u0026rsquo;t want to specify partitions and sort strategy, the call statement should be\nCALL sys.compact('default.t', '', '', '', 'sink.parallelism=4').\nIn higher versions, the procedure supports passing arguments by name. You can pass arguments in any order and any optional argument can be omitted. For the above example, the call statement is\nCALL sys.compact(`table` =\u0026gt; 'default.t', options =\u0026gt; 'sink.parallelism=4').\nSpecify partitions: we use string to represent partition filter. \u0026ldquo;,\u0026rdquo; means \u0026ldquo;AND\u0026rdquo; and \u0026ldquo;;\u0026rdquo; means \u0026ldquo;OR\u0026rdquo;. For example, if you want to specify two partitions date=01 and date=02, you need to write \u0026lsquo;date=01;date=02\u0026rsquo;; If you want to specify one partition with date=01 and day=01, you need to write \u0026lsquo;date=01,day=01\u0026rsquo;.\nTable options syntax: we use string to represent table options. The format is \u0026lsquo;key1=value1,key2=value2\u0026hellip;\u0026rsquo;.\nAll available procedures are listed below.\n  Procedure Name Usage Explanation Example    compact  CALL [catalog.]sys.compact('table')  CALL [catalog.]sys.compact('table', 'partitions')  CALL [catalog.]sys.compact('table', 'partitions', 'order_strategy', 'order_by')  CALL [catalog.]sys.compact('table', 'partitions', 'order_strategy', 'order_by', 'options')  CALL [catalog.]sys.compact('table', 'partitions', 'order_strategy', 'order_by', 'options', 'where')  CALL [catalog.]sys.compact('table', 'partitions', 'order_strategy', 'order_by', 'options', 'where', 'partition_idle_time')    To compact a table. Arguments: table(required): the target table identifier. partitions(optional): partition filter. order_strategy(optional): 'order' or 'zorder' or 'hilbert' or 'none'. order_by(optional): the columns need to be sort. Left empty if 'order_strategy' is 'none'. options(optional): additional dynamic options of the table. where(optional): partition predicate(Can't be used together with \"partitions\"). Note: as where is a keyword,a pair of backticks need to add around like `where`. partition_idle_time(optional): this is used to do a full compaction for partition which had not received any new data for 'partition_idle_time'. And only these partitions will be compacted. This argument can not be used with order compact.   -- use partition filter  CALL sys.compact(`table` = 'default.T', partitions = 'p=0', order_strategy = 'zorder', order_by = 'a,b', options = 'sink.parallelism=4')  -- use partition predicate  CALL sys.compact(`table` = 'default.T', `where` = 'dt10 and h'zorder', order_by = 'a,b', options = 'sink.parallelism=4')    compact_database  CALL [catalog.]sys.compact_database()  CALL [catalog.]sys.compact_database('includingDatabases')  CALL [catalog.]sys.compact_database('includingDatabases', 'mode')  CALL [catalog.]sys.compact_database('includingDatabases', 'mode', 'includingTables')  CALL [catalog.]sys.compact_database('includingDatabases', 'mode', 'includingTables', 'excludingTables')  CALL [catalog.]sys.compact_database('includingDatabases', 'mode', 'includingTables', 'excludingTables', 'tableOptions') CALL [catalog.]sys.compact_database('includingDatabases', 'mode', 'includingTables', 'excludingTables', 'tableOptions', 'partitionIdleTime')   To compact databases. Arguments: includingDatabases: to specify databases. You can use regular expression. mode: compact mode. \"divided\": start a sink for each table, detecting the new table requires restarting the job; \"combined\" (default): start a single combined sink for all tables, the new table will be automatically detected.  includingTables: to specify tables. You can use regular expression. excludingTables: to specify tables that are not compacted. You can use regular expression. tableOptions: additional dynamic options of the table. partition_idle_time: this is used to do a full compaction for partition which had not received any new data for 'partition_idle_time'. And only these partitions will be compacted.   CALL sys.compact_database('db1|db2', 'combined', 'table_.*', 'ignore', 'sink.parallelism=4')    create_tag  -- based on the specified snapshot  CALL [catalog.]sys.create_tag('identifier', 'tagName', snapshotId)  -- based on the latest snapshot  CALL [catalog.]sys.create_tag('identifier', 'tagName')   To create a tag based on given snapshot. Arguments: identifier: the target table identifier. Cannot be empty. tagName: name of the new tag. snapshotId (Long): id of the snapshot which the new tag is based on. time_retained: The maximum time retained for newly created tags.   CALL sys.create_tag('default.T', 'my_tag', 10, '1 d')    create_tag_from_timestamp  -- Create a tag from the first snapshot whose commit-time greater than the specified timestamp.  CALL [catalog.]sys.create_tag_from_timestamp('identifier', 'tagName', timestamp, time_retained)   To create a tag based on given timestamp. Arguments: identifier: the target table identifier. Cannot be empty. tag: name of the new tag. timestamp (Long): Find the first snapshot whose commit-time greater than this timestamp. time_retained : The maximum time retained for newly created tags.   -- for Flink 1.18 CALL sys.create_tag_from_timestamp('default.T', 'my_tag', 1724404318750, '1 d') -- for Flink 1.19 and later CALL sys.create_tag_from_timestamp(`table` = 'default.T', `tag` = 'my_tag', `timestamp` = 1724404318750, time_retained = '1 d')    create_tag_from_watermark  -- Create a tag from the first snapshot whose watermark greater than the specified timestamp. CALL [catalog.]sys.create_tag_from_watermark('identifier', 'tagName', watermark, time_retained)   To create a tag based on given watermark timestamp. Arguments: identifier: the target table identifier. Cannot be empty. tag: name of the new tag. watermark (Long): Find the first snapshot whose watermark greater than the specified watermark. time_retained : The maximum time retained for newly created tags.   -- for Flink 1.18 CALL sys.create_tag_from_watermark('default.T', 'my_tag', 1724404318750, '1 d') -- for Flink 1.19 and later CALL sys.create_tag_from_watermark(`table` = 'default.T', `tag` = 'my_tag', `watermark` = 1724404318750, time_retained = '1 d')    delete_tag  CALL [catalog.]sys.delete_tag('identifier', 'tagName')   To delete a tag. Arguments: identifier: the target table identifier. Cannot be empty. tagName: name of the tag to be deleted. If you specify multiple tags, delimiter is ','.   CALL sys.delete_tag('default.T', 'my_tag')    merge_into  CALL [catalog].sys.merge_into('identifier','targetAlias', 'sourceSqls','sourceTable','mergeCondition', 'matchedUpsertCondition','matchedUpsertSetting', 'notMatchedInsertCondition','notMatchedInsertValues', 'matchedDeleteCondition')   To perform \"MERGE INTO\" syntax. See merge_into action for details of arguments.   -- for matched order rows, -- increase the price, -- and if there is no match, -- insert the order from -- the source table CALL sys.merge_into( target_table = 'default.T', source_table = 'default.S', merge_condition = 'T.id=S.order_id', matched_upsert_setting = 'price=T.price+20', not_matched_insert_values = '*')    remove_orphan_files  CALL [catalog.]sys.remove_orphan_files('identifier') CALL [catalog.]sys.remove_orphan_files('identifier', 'olderThan') CALL [catalog.]sys.remove_orphan_files('identifier', 'olderThan', 'dryRun')   To remove the orphan data files and metadata files. Arguments: identifier: the target table identifier. Cannot be empty, you can use database_name.* to clean whole database. olderThan: to avoid deleting newly written files, this procedure only deletes orphan files older than 1 day by default. This argument can modify the interval.  dryRun: when true, view only orphan files, don't actually remove files. Default is false.  CALL remove_orphan_files('default.T', '2023-10-31 12:00:00') CALL remove_orphan_files('default.*', '2023-10-31 12:00:00') CALL remove_orphan_files('default.T', '2023-10-31 12:00:00', true)    reset_consumer  -- reset the new next snapshot id in the consumer CALL [catalog.]sys.reset_consumer('identifier', 'consumerId', nextSnapshotId) -- delete consumer CALL [catalog.]sys.reset_consumer('identifier', 'consumerId')   To reset or delete consumer. Arguments: identifier: the target table identifier. Cannot be empty. consumerId: consumer to be reset or deleted. nextSnapshotId (Long): the new next snapshot id of the consumer.  CALL sys.reset_consumer('default.T', 'myid', 10)   rollback_to  -- rollback to a snapshot CALL sys.rollback_to(`table` = 'identifier', snapshot_id = snapshotId) -- rollback to a tag CALL sys.rollback_to(`table` = 'identifier', tag = 'tagName')   To rollback to a specific version of target table. Argument: identifier: the target table identifier. Cannot be empty. snapshotId (Long): id of the snapshot that will roll back to. tagName: name of the tag that will roll back to.  CALL sys.rollback_to(`table` = 'default.T', snapshot_id = 10)   expire_snapshots  -- for Flink 1.18 CALL sys.expire_snapshots(table, retain_max) -- for Flink 1.19 and later CALL sys.expire_snapshots(table, retain_max, retain_min, older_than, max_deletes)   To expire snapshots. Argument: table: the target table identifier. Cannot be empty. retain_max: the maximum number of completed snapshots to retain. retain_min: the minimum number of completed snapshots to retain. order_than: timestamp before which snapshots will be removed. max_deletes: the maximum number of snapshots that can be deleted at once.   -- for Flink 1.18 CALL sys.expire_snapshots('default.T', 2) -- for Flink 1.19 and later CALL sys.expire_snapshots(`table` = 'default.T', retain_max = 2) CALL sys.expire_snapshots(`table` = 'default.T', older_than = '2024-01-01 12:00:00') CALL sys.expire_snapshots(`table` = 'default.T', older_than = '2024-01-01 12:00:00', retain_min = 10) CALL sys.expire_snapshots(`table` = 'default.T', older_than = '2024-01-01 12:00:00', max_deletes = 10)    expire_partitions  CALL sys.expire_partitions(table, expiration_time, timestamp_formatter, expire_strategy)   To expire partitions. Argument: table: the target table identifier. Cannot be empty. expiration_time: the expiration interval of a partition. A partition will be expired if it‘s lifetime is over this value. Partition time is extracted from the partition value. timestamp_formatter: the formatter to format timestamp from string. timestamp_pattern: the pattern to get a timestamp from partitions. expire_strategy: specifies the expiration strategy for partition expiration, possible values: 'values-time' or 'update-time' , 'values-time' as default.   -- for Flink 1.18 CALL sys.expire_partitions('default.T', '1 d', 'yyyy-MM-dd', '$dt', 'values-time') -- for Flink 1.19 and later CALL sys.expire_partitions(`table` = 'default.T', expiration_time = '1 d', timestamp_formatter = 'yyyy-MM-dd', expire_strategy = 'values-time') CALL sys.expire_partitions(`table` = 'default.T', expiration_time = '1 d', timestamp_formatter = 'yyyy-MM-dd HH:mm', timestamp_pattern = '$dt $hm', expire_strategy = 'values-time')    repair  -- repair all databases and tables in catalog CALL sys.repair() -- repair all tables in a specific database CALL sys.repair('databaseName') -- repair a table CALL sys.repair('databaseName.tableName') -- repair database and table in a string if you specify multiple tags, delimiter is ',' CALL sys.repair('databaseName01,database02.tableName01,database03')   Synchronize information from the file system to Metastore. Argument: empty: all databases and tables in catalog. databaseName : the target database name. tableName: the target table identifier.  CALL sys.repair('test_db.T')   rewrite_file_index  CALL sys.rewrite_file_index(\u0026ltidentifier\u0026gt [, \u0026ltpartitions\u0026gt])   Rewrite the file index for the table. Argument: identifier: \u0026ltdatabaseName\u0026gt.\u0026lttableName\u0026gt. partitions : specific partitions.   -- rewrite the file index for the whole table CALL sys.rewrite_file_index('test_db.T') -- repair all tables in a specific partition CALL sys.rewrite_file_index('test_db.T', 'pt=a')   create_branch  -- based on the specified tag  CALL [catalog.]sys.create_branch('identifier', 'branchName', 'tagName') -- create empty branch  CALL [catalog.]sys.create_branch('identifier', 'branchName')   To create a branch based on given tag, or just create empty branch. Arguments: identifier: the target table identifier. Cannot be empty. branchName: name of the new branch. tagName: name of the tag which the new branch is based on.   CALL sys.create_branch('default.T', 'branch1', 'tag1') CALL sys.create_branch('default.T', 'branch1')    delete_branch  CALL [catalog.]sys.delete_branch('identifier', 'branchName')   To delete a branch. Arguments: identifier: the target table identifier. Cannot be empty. branchName: name of the branch to be deleted. If you specify multiple branches, delimiter is ','.   CALL sys.delete_branch('default.T', 'branch1')    fast_forward  CALL [catalog.]sys.fast_forward('identifier', 'branchName')   To fast_forward a branch to main branch. Arguments: identifier: the target table identifier. Cannot be empty. branchName: name of the branch to be merged.   CALL sys.fast_forward('default.T', 'branch1')     "});index.add({'id':86,'href':'/docs/0.9/flink/action-jars/','title':"Action Jars",'section':"Engine Flink",'content':"Action Jars #  After the Flink Local Cluster has been started, you can execute the action jar by using the following command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-0.9.0.jar \\ \u0026lt;action\u0026gt; \u0026lt;args\u0026gt; The following command is used to compact a table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-0.9.0.jar \\ compact \\ --path \u0026lt;TABLE_PATH\u0026gt; Merging into table #  Paimon supports \u0026ldquo;MERGE INTO\u0026rdquo; via submitting the \u0026lsquo;merge_into\u0026rsquo; job through flink run.\nImportant table properties setting:\n Only primary key table supports this feature. The action won\u0026rsquo;t produce UPDATE_BEFORE, so it\u0026rsquo;s not recommended to set \u0026lsquo;changelog-producer\u0026rsquo; = \u0026lsquo;input\u0026rsquo;.   The design referenced such syntax:\nMERGE INTO target-table USING source_table | source-expr AS source-alias ON merge-condition WHEN MATCHED [AND matched-condition] THEN UPDATE SET xxx WHEN MATCHED [AND matched-condition] THEN DELETE WHEN NOT MATCHED [AND not_matched_condition] THEN INSERT VALUES (xxx) WHEN NOT MATCHED BY SOURCE [AND not-matched-by-source-condition] THEN UPDATE SET xxx WHEN NOT MATCHED BY SOURCE [AND not-matched-by-source-condition] THEN DELETE The merge_into action use \u0026ldquo;upsert\u0026rdquo; semantics instead of \u0026ldquo;update\u0026rdquo;, which means if the row exists, then do update, else do insert. For example, for non-primary-key table, you can update every column, but for primary key table, if you want to update primary keys, you have to insert a new row which has different primary keys from rows in the table. In this scenario, \u0026ldquo;upsert\u0026rdquo; is useful.\nRun the following command to submit a \u0026lsquo;merge_into\u0026rsquo; job for the table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  merge_into \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;target-table\u0026gt; \\  [--target_as \u0026lt;target-table-alias\u0026gt;] \\  --source_table \u0026lt;source_table-name\u0026gt; \\  [--source_sql \u0026lt;sql\u0026gt; ...]\\  --on \u0026lt;merge-condition\u0026gt; \\  --merge_actions \u0026lt;matched-upsert,matched-delete,not-matched-insert,not-matched-by-source-upsert,not-matched-by-source-delete\u0026gt; \\  --matched_upsert_condition \u0026lt;matched-condition\u0026gt; \\  --matched_upsert_set \u0026lt;upsert-changes\u0026gt; \\  --matched_delete_condition \u0026lt;matched-condition\u0026gt; \\  --not_matched_insert_condition \u0026lt;not-matched-condition\u0026gt; \\  --not_matched_insert_values \u0026lt;insert-values\u0026gt; \\  --not_matched_by_source_upsert_condition \u0026lt;not-matched-by-source-condition\u0026gt; \\  --not_matched_by_source_upsert_set \u0026lt;not-matched-upsert-changes\u0026gt; \\  --not_matched_by_source_delete_condition \u0026lt;not-matched-by-source-condition\u0026gt; \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] You can pass sqls by \u0026#39;--source_sql \u0026lt;sql\u0026gt; [, --source_sql \u0026lt;sql\u0026gt; ...]\u0026#39; to config environment and create source table at runtime. -- Examples: -- Find all orders mentioned in the source table, then mark as important if the price is above 100 -- or delete if the price is under 10. ./flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  merge_into \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table T \\  --source_table S \\  --on \u0026#34;T.id = S.order_id\u0026#34; \\  --merge_actions \\  matched-upsert,matched-delete \\  --matched_upsert_condition \u0026#34;T.price \u0026gt; 100\u0026#34; \\  --matched_upsert_set \u0026#34;mark = \u0026#39;important\u0026#39;\u0026#34; \\  --matched_delete_condition \u0026#34;T.price \u0026lt; 10\u0026#34; -- For matched order rows, increase the price, and if there is no match, insert the order from the -- source table: ./flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  merge_into \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table T \\  --source_table S \\  --on \u0026#34;T.id = S.order_id\u0026#34; \\  --merge_actions \\  matched-upsert,not-matched-insert \\  --matched_upsert_set \u0026#34;price = T.price + 20\u0026#34; \\  --not_matched_insert_values * -- For not matched by source order rows (which are in the target table and does not match any row in the -- source table based on the merge-condition), decrease the price or if the mark is \u0026#39;trivial\u0026#39;, delete them: ./flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  merge_into \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table T \\  --source_table S \\  --on \u0026#34;T.id = S.order_id\u0026#34; \\  --merge_actions \\  not-matched-by-source-upsert,not-matched-by-source-delete \\  --not_matched_by_source_upsert_condition \u0026#34;T.mark \u0026lt;\u0026gt; \u0026#39;trivial\u0026#39;\u0026#34; \\  --not_matched_by_source_upsert_set \u0026#34;price = T.price - 20\u0026#34; \\  --not_matched_by_source_delete_condition \u0026#34;T.mark = \u0026#39;trivial\u0026#39;\u0026#34; -- A --source_sql example: -- Create a temporary view S in new catalog and use it as source table ./flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  merge_into \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table T \\  --source_sql \u0026#34;CREATE CATALOG test_cat WITH (...)\u0026#34; \\  --source_sql \u0026#34;CREATE TEMPORARY VIEW test_cat.`default`.S AS SELECT order_id, price, \u0026#39;important\u0026#39; FROM important_order\u0026#34; \\  --source_table test_cat.default.S \\  --on \u0026#34;T.id = S.order_id\u0026#34; \\  --merge_actions not-matched-insert\\  --not_matched_insert_values * The term \u0026lsquo;matched\u0026rsquo; explanation:\n matched: changed rows are from target table and each can match a source table row based on merge-condition and optional matched-condition (source ∩ target). not matched: changed rows are from source table and all rows cannot match any target table row based on merge-condition and optional not_matched_condition (source - target). not matched by source: changed rows are from target table and all row cannot match any source table row based on merge-condition and optional not-matched-by-source-condition (target - source).  Parameters format:\n matched_upsert_changes:\ncol = \u0026lt;source_table\u0026gt;.col | expression [, \u0026hellip;] (Means setting \u0026lt;target_table\u0026gt;.col with given value. Do not add \u0026lsquo;\u0026lt;target_table\u0026gt;.\u0026rsquo; before \u0026lsquo;col\u0026rsquo;.)\nEspecially, you can use \u0026lsquo;*\u0026rsquo; to set columns with all source columns (require target table\u0026rsquo;s schema is equal to source\u0026rsquo;s). not_matched_upsert_changes is similar to matched_upsert_changes, but you cannot reference source table\u0026rsquo;s column or use \u0026lsquo;*\u0026rsquo;. insert_values:\ncol1, col2, \u0026hellip;, col_end\nMust specify values of all columns. For each column, you can reference \u0026lt;source_table\u0026gt;.col or use an expression.\nEspecially, you can use \u0026lsquo;*\u0026rsquo; to insert with all source columns (require target table\u0026rsquo;s schema is equal to source\u0026rsquo;s). not_matched_condition cannot use target table\u0026rsquo;s columns to construct condition expression. not_matched_by_source_condition cannot use source table\u0026rsquo;s columns to construct condition expression.   Target alias cannot be duplicated with existed table name. If the source table is not in the current catalog and current database, the source-table-name must be qualified (database.table or catalog.database.table if created a new catalog). For examples:\n(1) If source table \u0026lsquo;my_source\u0026rsquo; is in \u0026lsquo;my_db\u0026rsquo;, qualify it:\n--source_table \u0026ldquo;my_db.my_source\u0026rdquo;\n(2) Example for sqls:\nWhen sqls changed current catalog and database, it\u0026rsquo;s OK to not qualify the source table name:\n--source_sql \u0026ldquo;CREATE CATALOG my_cat WITH (\u0026hellip;)\u0026quot;\n--source_sql \u0026ldquo;USE CATALOG my_cat\u0026rdquo;\n--source_sql \u0026ldquo;CREATE DATABASE my_db\u0026rdquo;\n--source_sql \u0026ldquo;USE my_db\u0026rdquo;\n--source_sql \u0026ldquo;CREATE TABLE S \u0026hellip;\u0026quot;\n--source_table S\nbut you must qualify it in the following case:\n--source_sql \u0026ldquo;CREATE CATALOG my_cat WITH (\u0026hellip;)\u0026quot;\n--source_sql \u0026ldquo;CREATE TABLE my_cat.`default`.S \u0026hellip;\u0026quot;\n--source_table my_cat.default.S\nYou can use just \u0026lsquo;S\u0026rsquo; as source table name in following arguments. At least one merge action must be specified. If both matched-upsert and matched-delete actions are present, their conditions must both be present too (same to not-matched-by-source-upsert and not-matched-by-source-delete). Otherwise, all conditions are optional. All conditions, set changes and values should use Flink SQL syntax. To ensure the whole command runs normally in Shell, please quote them with \u0026quot;\u0026quot; to escape blank spaces and use \u0026lsquo;\\\u0026rsquo; to escape special characters in statement. For example:\n--source_sql \u0026ldquo;CREATE TABLE T (k INT) WITH (\u0026lsquo;special-key\u0026rsquo; = \u0026lsquo;123\\!')\u0026rdquo;   For more information of \u0026lsquo;merge_into\u0026rsquo;, see\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  merge_into --help Deleting from table #  In Flink 1.16 and previous versions, Paimon only supports deleting records via submitting the \u0026lsquo;delete\u0026rsquo; job through flink run.\nRun the following command to submit a \u0026lsquo;delete\u0026rsquo; job for the table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  delete \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  --where \u0026lt;filter_spec\u0026gt; \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] filter_spec is equal to the \u0026#39;WHERE\u0026#39; clause in SQL DELETE statement. Examples: age \u0026gt;= 18 AND age \u0026lt;= 60 animal \u0026lt;\u0026gt; \u0026#39;cat\u0026#39; id \u0026gt; (SELECT count(*) FROM employee) For more information of \u0026lsquo;delete\u0026rsquo;, see\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  delete --help Drop Partition #  Run the following command to submit a drop_partition job for the table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  drop_partition \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  [--partition \u0026lt;partition_spec\u0026gt; [--partition \u0026lt;partition_spec\u0026gt; ...]] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] partition_spec: key1=value1,key2=value2... For more information of drop_partition, see\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  drop_partition --help Rewrite File Index #  Run the following command to submit a rewrite_file_index job for the table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  rewrite_file_index \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --identifier \u0026lt;database.table\u0026gt; \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] For more information of rewrite_file_index, see\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.9.0.jar \\  rewrite_file_index --help "});index.add({'id':87,'href':'/docs/0.9/project/','title':"Project",'section':"Apache Paimon",'content':""});index.add({'id':88,'href':'/docs/0.9/learn-paimon/','title':"Learn Paimon",'section':"Apache Paimon",'content':""});index.add({'id':89,'href':'/docs/0.9/spark/procedures/','title':"Procedures",'section':"Engine Spark",'content':"Procedures #  This section introduce all available spark procedures about paimon.\n  Procedure Name Explanation Example    compact  To compact files. Argument: table: the target table identifier. Cannot be empty. partitions: partition filter. \",\" means \"AND\"\n\";\" means \"OR\".If you want to compact one partition with date=01 and day=01, you need to write 'date=01,day=01'. Left empty for all partitions. (Can't be used together with \"where\") where: partition predicate. Left empty for all partitions. (Can't be used together with \"partitions\") order_strategy: 'order' or 'zorder' or 'hilbert' or 'none'. Left empty for 'none'. order_columns: the columns need to be sort. Left empty if 'order_strategy' is 'none'. partition_idle_time: this is used to do a full compaction for partition which had not received any new data for 'partition_idle_time'. And only these partitions will be compacted. This argument can not be used with order compact.   SET spark.sql.shuffle.partitions=10; --set the compact parallelism  CALL sys.compact(table = 'T', partitions = 'p=0;p=1', order_strategy = 'zorder', order_by = 'a,b')  CALL sys.compact(table = 'T', where = 'p0 and p'zorder', order_by = 'a,b')  CALL sys.compact(table = 'T', partition_idle_time = '60s')    expire_snapshots  To expire snapshots. Argument: table: the target table identifier. Cannot be empty. retain_max: the maximum number of completed snapshots to retain. retain_min: the minimum number of completed snapshots to retain. older_than: timestamp before which snapshots will be removed. max_deletes: the maximum number of snapshots that can be deleted at once.  CALL sys.expire_snapshots(table = 'default.T', retain_max = 10)   expire_partitions  To expire partitions. Argument: table: the target table identifier. Cannot be empty. expiration_time: the expiration interval of a partition. A partition will be expired if it‘s lifetime is over this value. Partition time is extracted from the partition value. timestamp_formatter: the formatter to format timestamp from string. timestamp_pattern: the pattern to get a timestamp from partitions. expire_strategy: specifies the expiration strategy for partition expiration, possible values: 'values-time' or 'update-time' , 'values-time' as default.  CALL sys.expire_partitions(table = 'default.T', expiration_time = '1 d', timestamp_formatter = 'yyyy-MM-dd', timestamp_pattern = '$dt', expire_strategy = 'values-time')   create_tag  To create a tag based on given snapshot. Arguments: table: the target table identifier. Cannot be empty. tag: name of the new tag. Cannot be empty. snapshot(Long): id of the snapshot which the new tag is based on. time_retained: The maximum time retained for newly created tags.   -- based on snapshot 10 with 1d  CALL sys.create_tag(table = 'default.T', tag = 'my_tag', snapshot = 10, time_retained = '1 d')  -- based on the latest snapshot  CALL sys.create_tag(table = 'default.T', tag = 'my_tag')    create_tag_from_timestamp  To create a tag based on given timestamp. Arguments: identifier: the target table identifier. Cannot be empty. tag: name of the new tag. timestamp (Long): Find the first snapshot whose commit-time is greater than this timestamp. time_retained : The maximum time retained for newly created tags.   CALL sys.create_tag_from_timestamp(`table` = 'default.T', `tag` = 'my_tag', `timestamp` = 1724404318750, time_retained = '1 d')    delete_tag  To delete a tag. Arguments: table: the target table identifier. Cannot be empty. tag: name of the tag to be deleted. If you specify multiple tags, delimiter is ','.  CALL sys.delete_tag(table = 'default.T', tag = 'my_tag')   rollback  To rollback to a specific version of target table. Argument: table: the target table identifier. Cannot be empty. version: id of the snapshot or name of tag that will roll back to.   CALL sys.rollback(table = 'default.T', version = 'my_tag') CALL sys.rollback(table = 'default.T', version = 10)    migrate_table  Migrate hive table to a paimon table. Arguments: source_type: the origin table's type to be migrated, such as hive. Cannot be empty. table: name of the origin table to be migrated. Cannot be empty. options: the table options of the paimon table to migrate. target_table: name of the target paimon table to migrate. If not set would keep the same name with origin table delete_origin: If had set target_table, can set delete_origin to decide whether delete the origin table metadata from hms after migrate. Default is true options_map: Options map for adding key-value options which is a map.  CALL sys.migrate_table(source_type = 'hive', table = 'default.T', options = 'file.format=parquet', options_map = map('k1','v1'))   migrate_file  Migrate from hive table to a paimon table. Arguments: source_type: the origin table's type to be migrated, such as hive. Cannot be empty. source_table: name of the origin table to migrate. Cannot be empty. target_table: name of the target table to be migrated. Cannot be empty. delete_origin: If had set target_table, can set delete_origin to decide whether delete the origin table metadata from hms after migrate. Default is true  CALL sys.migrate_file(source_type = 'hive', table = 'default.T', delete_origin = true)   remove_orphan_files  To remove the orphan data files and metadata files. Arguments: table: the target table identifier. Cannot be empty, you can use database_name.* to clean whole database. older_than: to avoid deleting newly written files, this procedure only deletes orphan files older than 1 day by default. This argument can modify the interval. dry_run: when true, view only orphan files, don't actually remove files. Default is false.   CALL sys.remove_orphan_files(table = 'default.T', older_than = '2023-10-31 12:00:00') CALL sys.remove_orphan_files(table = 'default.*', older_than = '2023-10-31 12:00:00') CALL sys.remove_orphan_files(table = 'default.T', older_than = '2023-10-31 12:00:00', dry_run = true)    repair  Synchronize information from the file system to Metastore. Argument: database_or_table: empty or the target database name or the target table identifier, if you specify multiple tags, delimiter is ','   CALL sys.repair('test_db.T') CALL sys.repair('test_db.T,test_db01,test_db.T2')    create_branch  To merge a branch to main branch. Arguments: table: the target table identifier. Cannot be empty. branch: name of the branch to be merged. tag: name of the new tag. Cannot be empty.   CALL sys.create_branch(table = 'test_db.T', branch = 'test_branch') CALL sys.create_branch(table = 'test_db.T', branch = 'test_branch', tag = 'my_tag')    delete_branch  To merge a branch to main branch. Arguments: table: the target table identifier. Cannot be empty. branch: name of the branch to be merged. If you specify multiple branches, delimiter is ','.   CALL sys.delete_branch(table = 'test_db.T', branch = 'test_branch')    fast_forward  To fast_forward a branch to main branch. Arguments: table: the target table identifier. Cannot be empty. branch: name of the branch to be merged.   CALL sys.fast_forward(table = 'test_db.T', branch = 'test_branch')    reset_consumer  To reset or delete consumer. Arguments: identifier: the target table identifier. Cannot be empty. consumerId: consumer to be reset or deleted. nextSnapshotId (Long): the new next snapshot id of the consumer.   -- reset the new next snapshot id in the consumer CALL sys.reset_consumer(table = 'default.T', consumerId = 'myid', nextSnapshotId = 10) -- delete consumer CALL sys.reset_consumer(table = 'default.T', consumerId = 'myid')    mark_partition_done  To mark partition to be done. Arguments: table: the target table identifier. Cannot be empty. partitions: partitions need to be mark done, If you specify multiple partitions, delimiter is ';'.   -- mark single partition done CALL sys.mark_partition_done(table = 'default.T', parititions = 'day=2024-07-01') -- mark multiple partitions done CALL sys.mark_partition_done(table = 'default.T', parititions = 'day=2024-07-01;day=2024-07-02')     "});index.add({'id':90,'href':'/docs/0.9/flink/savepoint/','title':"Savepoint",'section':"Engine Flink",'content':"Savepoint #  Paimon has its own snapshot management, this may conflict with Flink\u0026rsquo;s checkpoint management, causing exceptions when restoring from savepoint (don\u0026rsquo;t worry, it will not cause the storage to be damaged).\nIt is recommended that you use the following methods to savepoint:\n Use Flink Stop with savepoint. Use Paimon Tag with Flink Savepoint, and rollback-to-tag before restoring from savepoint.  Stop with savepoint #  This feature of Flink ensures that the last checkpoint is fully processed, which means there will be no more uncommitted metadata left. This is very safe, so we recommend using this feature to stop and start job.\nTag with Savepoint #  In Flink, we may consume from kafka and then write to paimon. Since flink\u0026rsquo;s checkpoint only retains a limited number, we will trigger a savepoint at certain time (such as code upgrades, data updates, etc.) to ensure that the state can be retained for a longer time, so that the job can be restored incrementally.\nPaimon\u0026rsquo;s snapshot is similar to flink\u0026rsquo;s checkpoint, and both will automatically expire, but the tag feature of paimon allows snapshots to be retained for a long time. Therefore, we can combine the two features of paimon\u0026rsquo;s tag and flink\u0026rsquo;s savepoint to achieve incremental recovery of job from the specified savepoint.\nStarting from Flink 1.15 intermediate savepoints (savepoints other than created with stop-with-savepoint) are not used for recovery and do not commit any side effects.\nFor savepoint created with stop-with-savepoint, tags will be created automatically. For other savepoints, tags will be created after the next checkpoint succeeds.\n Step 1: Enable automatically create tags for savepoint.\nYou can set sink.savepoint.auto-tag to true to enable the feature of automatically creating tags for savepoint.\nStep 2: Trigger savepoint.\nYou can refer to flink savepoint to learn how to configure and trigger savepoint.\nStep 3: Choose the tag corresponding to the savepoint.\nThe tag corresponding to the savepoint will be named in the form of savepoint-${savepointID}. You can refer to Tags Table to query.\nStep 4: Rollback the paimon table.\nRollback the paimon table to the specified tag.\nStep 5: Restart from the savepoint.\nYou can refer to here to learn how to restart from a specified savepoint.\n"});index.add({'id':91,'href':'/docs/0.9/maintenance/configurations/','title':"Configurations",'section':"Maintenance",'content':"Configuration #  CoreOptions #  Core options for paimon.\n  Key Default Type Description     async-file-write true Boolean Whether to enable asynchronous IO writing when writing files.   auto-create false Boolean Whether to create underlying storage when reading and writing the table.   branch \"main\" String Specify branch name.   bucket -1 Integer Bucket number for file store.\nIt should either be equal to -1 (dynamic bucket mode), or it must be greater than 0 (fixed bucket mode).   bucket-key (none) String Specify the paimon distribution policy. Data is assigned to each bucket according to the hash value of bucket-key.\nIf you specify multiple fields, delimiter is ','.\nIf not specified, the primary key will be used; if there is no primary key, the full row will be used.   cache-page-size 64 kb MemorySize Memory page size for caching.   changelog-producer none Enum\n Whether to double write to a changelog file. This changelog file keeps the details of data changes, it can be read directly during stream reads. This can be applied to tables with primary keys. Possible values:\"none\": No changelog file.\"input\": Double write to a changelog file when flushing memory table, the changelog is from input.\"full-compaction\": Generate changelog files with each full compaction.\"lookup\": Generate changelog files through 'lookup' before committing the data writing.   changelog-producer.row-deduplicate false Boolean Whether to generate -U, +U changelog for the same record. This configuration is only valid for the changelog-producer is lookup or full-compaction.   changelog.num-retained.max (none) Integer The maximum number of completed changelog to retain. Should be greater than or equal to the minimum number.   changelog.num-retained.min (none) Integer The minimum number of completed changelog to retain. Should be greater than or equal to 1.   changelog.time-retained (none) Duration The maximum time of completed changelog to retain.   commit.callback.#.param (none) String Parameter string for the constructor of class #. Callback class should parse the parameter by itself.   commit.callbacks (none) String A list of commit callback classes to be called after a successful commit. Class names are connected with comma (example: com.test.CallbackA,com.sample.CallbackB).   commit.force-compact false Boolean Whether to force a compaction before commit.   commit.force-create-snapshot false Boolean Whether to force create snapshot on commit.   commit.user-prefix (none) String Specifies the commit user prefix.   compaction.max-size-amplification-percent 200 Integer The size amplification is defined as the amount (in percentage) of additional storage needed to store a single byte of data in the merge tree for changelog mode table.   compaction.max.file-num (none) Integer For file set [f_0,...,f_N], the maximum file number to trigger a compaction for append-only table, even if sum(size(f_i)) \u0026lt; targetFileSize. This value avoids pending too much small files.Default value of Append Table is '50'.Default value of Bucketed Append Table is '5'.   compaction.min.file-num 5 Integer For file set [f_0,...,f_N], the minimum file number which satisfies sum(size(f_i)) \u0026gt;= targetFileSize to trigger a compaction for append-only table. This value avoids almost-full-file to be compacted, which is not cost-effective.   compaction.optimization-interval (none) Duration Implying how often to perform an optimization compaction, this configuration is used to ensure the query timeliness of the read-optimized system table.   compaction.size-ratio 1 Integer Percentage flexibility while comparing sorted run size for changelog mode table. If the candidate sorted run(s) size is 1% smaller than the next sorted run's size, then include next sorted run into this candidate set.   consumer-id (none) String Consumer id for recording the offset of consumption in the storage.   consumer.expiration-time (none) Duration The expiration interval of consumer files. A consumer file will be expired if it's lifetime after last modification is over this value.   consumer.ignore-progress false Boolean Whether to ignore consumer progress for the newly started job.   consumer.mode exactly-once Enum\n Specify the consumer consistency mode for table.\nPossible values:\"exactly-once\": Readers consume data at snapshot granularity, and strictly ensure that the snapshot-id recorded in the consumer is the snapshot-id + 1 that all readers have exactly consumed.\"at-least-once\": Each reader consumes snapshots at a different rate, and the snapshot with the slowest consumption progress among all readers will be recorded in the consumer.   continuous.discovery-interval 10 s Duration The discovery interval of continuous reading.   cross-partition-upsert.bootstrap-parallelism 10 Integer The parallelism for bootstrap in a single task for cross partition upsert.   cross-partition-upsert.index-ttl (none) Duration The TTL in rocksdb index for cross partition upsert (primary keys not contain all partition fields), this can avoid maintaining too many indexes and lead to worse and worse performance, but please note that this may also cause data duplication.   delete-file.thread-num (none) Integer The maximum number of concurrent deleting files. By default is the number of processors available to the Java virtual machine.   delete.force-produce-changelog false Boolean Force produce changelog in delete sql, or you can use 'streaming-read-overwrite' to read changelog from overwrite commit.   deletion-vector.index-file.target-size 2 mb MemorySize The target size of deletion vector index file.   deletion-vectors.enabled false Boolean Whether to enable deletion vectors mode. In this mode, index files containing deletion vectors are generated when data is written, which marks the data for deletion. During read operations, by applying these index files, merging can be avoided.   dynamic-bucket.assigner-parallelism (none) Integer Parallelism of assigner operator for dynamic bucket mode, it is related to the number of initialized bucket, too small will lead to insufficient processing speed of assigner.   dynamic-bucket.initial-buckets (none) Integer Initial buckets for a partition in assigner operator for dynamic bucket mode.   dynamic-bucket.target-row-num 2000000 Long If the bucket is -1, for primary key table, is dynamic bucket mode, this option controls the target row number for one bucket.   dynamic-partition-overwrite true Boolean Whether only overwrite dynamic partition when overwriting a partitioned table with dynamic partition columns. Works only when the table has partition keys.   end-input.check-partition-expire false Boolean Optional endInput check partition expire used in case of batch mode or bounded stream.   fields.default-aggregate-function (none) String Default aggregate function of all fields for partial-update and aggregate merge function.   file-index.in-manifest-threshold 500 bytes MemorySize The threshold to store file index bytes in manifest.   file-index.read.enabled true Boolean Whether enabled read file index.   file-reader-async-threshold 10 mb MemorySize The threshold for read file async.   file.block-size (none) MemorySize File block size of format, default value of orc stripe is 64 MB, and parquet row group is 128 MB.   file.compression \"zstd\" String Default file compression. For faster read and write, it is recommended to use zstd.   file.compression.per.level  Map Define different compression policies for different level, you can add the conf like this: 'file.compression.per.level' = '0:lz4,1:zstd'.   file.compression.zstd-level 1 Integer Default file compression zstd level. For higher compression rates, it can be configured to 9, but the read and write speed will significantly decrease.   file.format \"parquet\" String Specify the message format of data files, currently orc, parquet and avro are supported.   file.format.per.level  Map Define different file format for different level, you can add the conf like this: 'file.format.per.level' = '0:avro,3:parquet', if the file format for level is not provided, the default format which set by `file.format` will be used.   force-lookup false Boolean Whether to force the use of lookup for compaction.   full-compaction.delta-commits (none) Integer Full compaction will be constantly triggered after delta commits.   ignore-delete false Boolean Whether to ignore delete records.   incremental-between (none) String Read incremental changes between start snapshot (exclusive) and end snapshot, for example, '5,10' means changes between snapshot 5 and snapshot 10.   incremental-between-scan-mode auto Enum\n Scan kind when Read incremental changes between start snapshot (exclusive) and end snapshot. Possible values:\"auto\": Scan changelog files for the table which produces changelog files. Otherwise, scan newly changed files.\"delta\": Scan newly changed files between snapshots.\"changelog\": Scan changelog files between snapshots.   incremental-between-timestamp (none) String Read incremental changes between start timestamp (exclusive) and end timestamp, for example, 't1,t2' means changes between timestamp t1 and timestamp t2.   local-merge-buffer-size (none) MemorySize Local merge will buffer and merge input records before they're shuffled by bucket and written into sink. The buffer will be flushed when it is full. Mainly to resolve data skew on primary keys. We recommend starting with 64 mb when trying out this feature.   local-sort.max-num-file-handles 128 Integer The maximal fan-in for external merge sort. It limits the number of file handles. If it is too small, may cause intermediate merging. But if it is too large, it will cause too many files opened at the same time, consume memory and lead to random reading.   lookup-wait true Boolean When need to lookup, commit will wait for compaction by lookup.   lookup.cache-file-retention 1 h Duration The cached files retention time for lookup. After the file expires, if there is a need for access, it will be re-read from the DFS to build an index on the local disk.   lookup.cache-max-disk-size infinite MemorySize Max disk size for lookup cache, you can use this option to limit the use of local disks.   lookup.cache-max-memory-size 256 mb MemorySize Max memory size for lookup cache.   lookup.cache-spill-compression \"zstd\" String Spill compression for lookup cache, currently zstd, none, lz4 and lzo are supported.   lookup.cache.bloom.filter.enabled true Boolean Whether to enable the bloom filter for lookup cache.   lookup.cache.bloom.filter.fpp 0.05 Double Define the default false positive probability for lookup cache bloom filters.   lookup.hash-load-factor 0.75 Float The index load factor for lookup.   lookup.local-file-type hash Enum\n The local file type for lookup.\nPossible values:\"sort\": Construct a sorted file for lookup.\"hash\": Construct a hash file for lookup.   manifest.compression \"zstd\" String Default file compression for manifest.   manifest.format \"avro\" String Specify the message format of manifest files.   manifest.full-compaction-threshold-size 16 mb MemorySize The size threshold for triggering full compaction of manifest.   manifest.merge-min-count 30 Integer To avoid frequent manifest merges, this parameter specifies the minimum number of ManifestFileMeta to merge.   manifest.target-file-size 8 mb MemorySize Suggested file size of a manifest file.   merge-engine deduplicate Enum\n Specify the merge engine for table with primary key.\nPossible values:\"deduplicate\": De-duplicate and keep the last row.\"partial-update\": Partial update non-null fields.\"aggregation\": Aggregate fields with same primary key.\"first-row\": De-duplicate and keep the first row.   metadata.iceberg-compatible false Boolean When set to true, produce Iceberg metadata after a snapshot is committed, so that Iceberg readers can read Paimon's raw files.   metadata.stats-mode \"truncate(16)\" String The mode of metadata stats collection. none, counts, truncate(16), full is available.\n\"none\": means disable the metadata stats collection.\"counts\" means only collect the null count.\"full\": means collect the null count, min/max value.\"truncate(16)\": means collect the null count, min/max value with truncated length of 16.Field level stats mode can be specified by fields.{field_name}.stats-mode   metastore.partitioned-table false Boolean Whether to create this table as a partitioned table in metastore. For example, if you want to list all partitions of a Paimon table in Hive, you need to create this table as a partitioned table in Hive metastore. This config option does not affect the default filesystem metastore.   metastore.tag-to-partition (none) String Whether to create this table as a partitioned table for mapping non-partitioned table tags in metastore. This allows the Hive engine to view this table in a partitioned table view and use partitioning field to read specific partitions (specific tags).   metastore.tag-to-partition.preview none Enum\n Whether to preview tag of generated snapshots in metastore. This allows the Hive engine to query specific tag before creation.\nPossible values:\"none\": No automatically created tags.\"process-time\": Based on the time of the machine, create TAG once the processing time passes period time plus delay.\"watermark\": Based on the watermark of the input, create TAG once the watermark passes period time plus delay.\"batch\": In the batch processing scenario, the tag corresponding to the current snapshot is generated after the task is completed.   num-levels (none) Integer Total level number, for example, there are 3 levels, including 0,1,2 levels.   num-sorted-run.compaction-trigger 5 Integer The sorted run number to trigger compaction. Includes level0 files (one file one sorted run) and high-level runs (one level one sorted run).   num-sorted-run.stop-trigger (none) Integer The number of sorted runs that trigger the stopping of writes, the default value is 'num-sorted-run.compaction-trigger' + 3.   page-size 64 kb MemorySize Memory page size.   parquet.enable.dictionary (none) Integer Turn off the dictionary encoding for all fields in parquet.   partial-update.remove-record-on-delete false Boolean Whether to remove the whole row in partial-update engine when -D records are received.   partition (none) String Define partition by table options, cannot define partition on DDL and table options at the same time.   partition.default-name \"__DEFAULT_PARTITION__\" String The default partition name in case the dynamic partition column value is null/empty string.   partition.expiration-check-interval 1 h Duration The check interval of partition expiration.   partition.expiration-strategy values-time Enum\n The strategy determines how to extract the partition time and compare it with the current time.\nPossible values:\"values-time\": This strategy compares the time extracted from the partition value with the current time.\"update-time\": This strategy compares the last update time of the partition with the current time.   partition.expiration-time (none) Duration The expiration interval of a partition. A partition will be expired if it‘s lifetime is over this value. Partition time is extracted from the partition value.   partition.mark-done-action \"success-file\" String Action to mark a partition done is to notify the downstream application that the partition has finished writing, the partition is ready to be read.\n1. 'success-file': add '_success' file to directory.\n2. 'done-partition': add 'xxx.done' partition to metastore.\n3. 'mark-event': mark partition event to metastore.\nBoth can be configured at the same time: 'done-partition,success-file,mark-event'.   partition.timestamp-formatter (none) String The formatter to format timestamp from string. It can be used with 'partition.timestamp-pattern' to create a formatter using the specified value.Default formatter is 'yyyy-MM-dd HH:mm:ss' and 'yyyy-MM-dd'.Supports multiple partition fields like '$year-$month-$day $hour:00:00'.The timestamp-formatter is compatible with Java's DateTimeFormatter.   partition.timestamp-pattern (none) String You can specify a pattern to get a timestamp from partitions. The formatter pattern is defined by 'partition.timestamp-formatter'.By default, read from the first field.If the timestamp in the partition is a single field called 'dt', you can use '$dt'.If it is spread across multiple fields for year, month, day, and hour, you can use '$year-$month-$day $hour:00:00'.If the timestamp is in fields dt and hour, you can use '$dt $hour:00:00'.   primary-key (none) String Define primary key by table options, cannot define primary key on DDL and table options at the same time.   read.batch-size 1024 Integer Read batch size for orc and parquet.   record-level.expire-time (none) Duration Record level expire time for primary key table, expiration happens in compaction, there is no strong guarantee to expire records in time. You must specific 'record-level.time-field' too.   record-level.time-field (none) String Time field for record level expire.   record-level.time-field-type seconds-int Enum\n Time field type for record level expire, it can be seconds-int or millis-long.\nPossible values:\"seconds-int\": Timestamps in seconds should be INT type.\"millis-long\": Timestamps in milliseconds should be BIGINT type.   rowkind.field (none) String The field that generates the row kind for primary key table, the row kind determines which data is '+I', '-U', '+U' or '-D'.   scan.bounded.watermark (none) Long End condition \"watermark\" for bounded streaming mode. Stream reading will end when a larger watermark snapshot is encountered.   scan.fallback-branch (none) String When a batch job queries from a table, if a partition does not exist in the current branch, the reader will try to get this partition from this fallback branch.   scan.file-creation-time-millis (none) Long After configuring this time, only the data files created after this time will be read. It is independent of snapshots, but it is imprecise filtering (depending on whether or not compaction occurs).   scan.manifest.parallelism (none) Integer The parallelism of scanning manifest files, default value is the size of cpu processor. Note: Scale-up this parameter will increase memory usage while scanning manifest files. We can consider downsize it when we encounter an out of memory exception while scanning   scan.max-splits-per-task 10 Integer Max split size should be cached for one task while scanning. If splits size cached in enumerator are greater than tasks size multiply by this value, scanner will pause scanning.   scan.mode default Enum\n Specify the scanning behavior of the source.\nPossible values:\"default\": Determines actual startup mode according to other table properties. If \"scan.timestamp-millis\" is set the actual startup mode will be \"from-timestamp\", and if \"scan.snapshot-id\" or \"scan.tag-name\" is set the actual startup mode will be \"from-snapshot\". Otherwise the actual startup mode will be \"latest-full\".\"latest-full\": For streaming sources, produces the latest snapshot on the table upon first startup, and continue to read the latest changes. For batch sources, just produce the latest snapshot but does not read new changes.\"full\": Deprecated. Same as \"latest-full\".\"latest\": For streaming sources, continuously reads latest changes without producing a snapshot at the beginning. For batch sources, behaves the same as the \"latest-full\" startup mode.\"compacted-full\": For streaming sources, produces a snapshot after the latest compaction on the table upon first startup, and continue to read the latest changes. For batch sources, just produce a snapshot after the latest compaction but does not read new changes. Snapshots of full compaction are picked when scheduled full-compaction is enabled.\"from-timestamp\": For streaming sources, continuously reads changes starting from timestamp specified by \"scan.timestamp-millis\", without producing a snapshot at the beginning. For batch sources, produces a snapshot at timestamp specified by \"scan.timestamp-millis\" but does not read new changes.\"from-file-creation-time\": For streaming and batch sources, produces a snapshot and filters the data files by creation time. For streaming sources, upon first startup, and continue to read the latest changes.\"from-snapshot\": For streaming sources, continuously reads changes starting from snapshot specified by \"scan.snapshot-id\", without producing a snapshot at the beginning. For batch sources, produces a snapshot specified by \"scan.snapshot-id\" or \"scan.tag-name\" but does not read new changes.\"from-snapshot-full\": For streaming sources, produces from snapshot specified by \"scan.snapshot-id\" on the table upon first startup, and continuously reads changes. For batch sources, produces a snapshot specified by \"scan.snapshot-id\" but does not read new changes.\"incremental\": Read incremental changes between start and end snapshot or timestamp.   scan.plan-sort-partition false Boolean Whether to sort plan files by partition fields, this allows you to read according to the partition order, even if your partition writes are out of order.\nIt is recommended that you use this for streaming read of the 'append-only' table. By default, streaming read will read the full snapshot first. In order to avoid the disorder reading for partitions, you can open this option.   scan.snapshot-id (none) Long Optional snapshot id used in case of \"from-snapshot\" or \"from-snapshot-full\" scan mode   scan.tag-name (none) String Optional tag name used in case of \"from-snapshot\" scan mode.   scan.timestamp (none) String Optional timestamp used in case of \"from-timestamp\" scan mode, it will be automatically converted to timestamp in unix milliseconds, use local time zone   scan.timestamp-millis (none) Long Optional timestamp used in case of \"from-timestamp\" scan mode. If there is no snapshot earlier than this time, the earliest snapshot will be chosen.   scan.watermark (none) Long Optional watermark used in case of \"from-snapshot\" scan mode. If there is no snapshot later than this watermark, will throw an exceptions.   sequence.field (none) String The field that generates the sequence number for primary key table, the sequence number determines which data is the most recent.   sink.watermark-time-zone \"UTC\" String The time zone to parse the long watermark value to TIMESTAMP value. The default value is 'UTC', which means the watermark is defined on TIMESTAMP column or not defined. If the watermark is defined on TIMESTAMP_LTZ column, the time zone of watermark is user configured time zone, the value should be the user configured local time zone. The option value is either a full name such as 'America/Los_Angeles', or a custom timezone id such as 'GMT-08:00'.   snapshot.clean-empty-directories false Boolean Whether to try to clean empty directories when expiring snapshots, if enabled, please note:hdfs: may print exceptions in NameNode.oss/s3: may cause performance issue.   snapshot.expire.execution-mode sync Enum\n Specifies the execution mode of expire.\nPossible values:\"sync\": Execute expire synchronously. If there are too many files, it may take a long time and block stream processing.\"async\": Execute expire asynchronously. If the generation of snapshots is greater than the deletion, there will be a backlog of files.   snapshot.expire.limit 10 Integer The maximum number of snapshots allowed to expire at a time.   snapshot.num-retained.max infinite Integer The maximum number of completed snapshots to retain. Should be greater than or equal to the minimum number.   snapshot.num-retained.min 10 Integer The minimum number of completed snapshots to retain. Should be greater than or equal to 1.   snapshot.time-retained 1 h Duration The maximum time of completed snapshots to retain.   snapshot.watermark-idle-timeout (none) Duration In watermarking, if a source remains idle beyond the specified timeout duration, it triggers snapshot advancement and facilitates tag creation.   sort-compaction.local-sample.magnification 1000 Integer The magnification of local sample for sort-compaction.The size of local sample is sink parallelism * magnification.   sort-compaction.range-strategy QUANTITY Enum\n The range strategy of sort compaction, the default value is quantity. If the data size allocated for the sorting task is uneven,which may lead to performance bottlenecks, the config can be set to size.\nPossible values:\"SIZE\"\"QUANTITY\"   sort-engine loser-tree Enum\n Specify the sort engine for table with primary key.\nPossible values:\"min-heap\": Use min-heap for multiway sorting.\"loser-tree\": Use loser-tree for multiway sorting. Compared with heapsort, loser-tree has fewer comparisons and is more efficient.   sort-spill-buffer-size 64 mb MemorySize Amount of data to spill records to disk in spilled sort.   sort-spill-threshold (none) Integer If the maximum number of sort readers exceeds this value, a spill will be attempted. This prevents too many readers from consuming too much memory and causing OOM.   source.split.open-file-cost 4 mb MemorySize Open file cost of a source file. It is used to avoid reading too many files with a source split, which can be very slow.   source.split.target-size 128 mb MemorySize Target size of a source split when scanning a bucket.   spill-compression \"zstd\" String Compression for spill, currently zstd, lzo and zstd are supported.   spill-compression.zstd-level 1 Integer Default spill compression zstd level. For higher compression rates, it can be configured to 9, but the read and write speed will significantly decrease.   streaming-read-mode (none) Enum\n The mode of streaming read that specifies to read the data of table file or log.\nPossible values:\"log\": Read from the data of table log store.\"file\": Read from the data of table file store.   streaming-read-overwrite false Boolean Whether to read the changes from overwrite in streaming mode. Cannot be set to true when changelog producer is full-compaction or lookup because it will read duplicated changes.   streaming.read.snapshot.delay (none) Duration The delay duration of stream read when scan incremental snapshots.   tag.automatic-completion false Boolean Whether to automatically complete missing tags.   tag.automatic-creation none Enum\n Whether to create tag automatically. And how to generate tags.\nPossible values:\"none\": No automatically created tags.\"process-time\": Based on the time of the machine, create TAG once the processing time passes period time plus delay.\"watermark\": Based on the watermark of the input, create TAG once the watermark passes period time plus delay.\"batch\": In the batch processing scenario, the tag corresponding to the current snapshot is generated after the task is completed.   tag.callback.#.param (none) String Parameter string for the constructor of class #. Callback class should parse the parameter by itself.   tag.callbacks (none) String A list of commit callback classes to be called after a successful tag. Class names are connected with comma (example: com.test.CallbackA,com.sample.CallbackB).   tag.creation-delay 0 ms Duration How long is the delay after the period ends before creating a tag. This can allow some late data to enter the Tag.   tag.creation-period daily Enum\n What frequency is used to generate tags.\nPossible values:\"daily\": Generate a tag every day.\"hourly\": Generate a tag every hour.\"two-hours\": Generate a tag every two hours.   tag.default-time-retained (none) Duration The default maximum time retained for newly created tags. It affects both auto-created tags and manually created (by procedure) tags.   tag.num-retained-max (none) Integer The maximum number of tags to retain. It only affects auto-created tags.   tag.period-formatter with_dashes Enum\n The date format for tag periods.\nPossible values:\"with_dashes\": Dates and hours with dashes, e.g., 'yyyy-MM-dd HH'\"without_dashes\": Dates and hours without dashes, e.g., 'yyyyMMdd HH'   target-file-size (none) MemorySize Target size of a file.primary key table: the default value is 128 MB.append table: the default value is 256 MB.   write-buffer-for-append false Boolean This option only works for append-only table. Whether the write use write buffer to avoid out-of-memory error.   write-buffer-size 256 mb MemorySize Amount of data to build up in memory before converting to a sorted on-disk file.   write-buffer-spill.max-disk-size infinite MemorySize The max disk to use for write buffer spill. This only work when the write buffer spill is enabled   write-buffer-spillable (none) Boolean Whether the write buffer can be spillable. Enabled by default when using object storage.   write-manifest-cache 0 bytes MemorySize Cache size for reading manifest files for write initialization.   write-max-writers-to-spill 10 Integer When in batch append inserting, if the writer number is greater than this option, we open the buffer cache and spill function to avoid out-of-memory.    write-only false Boolean If set to true, compactions and snapshot expiration will be skipped. This option is used along with dedicated compact jobs.   zorder.var-length-contribution 8 Integer The bytes of types (CHAR, VARCHAR, BINARY, VARBINARY) devote to the zorder sort.    CatalogOptions #  Options for paimon catalog.\n  Key Default Type Description     allow-upper-case (none) Boolean Indicates whether this catalog allow upper case, its default value depends on the implementation of the specific catalog.   cache-enabled true Boolean Controls whether the catalog will cache databases, tables and manifests.   cache.expiration-interval 1 min Duration Controls the duration for which databases and tables in the catalog are cached.   cache.manifest.max-memory (none) MemorySize Controls the maximum memory to cache manifest content.   cache.manifest.small-file-memory 128 mb MemorySize Controls the cache memory to cache small manifest files.   cache.manifest.small-file-threshold 1 mb MemorySize Controls the threshold of small manifest file.   client-pool-size 2 Integer Configure the size of the connection pool.   fs.allow-hadoop-fallback true Boolean Allow to fallback to hadoop File IO when no file io found for the scheme.   lineage-meta (none) String The lineage meta to store table and data lineage information.\nPossible values:\n\"jdbc\": Use standard jdbc to store table and data lineage information.\"custom\": You can implement LineageMetaFactory and LineageMeta to store lineage information in customized storage.   lock-acquire-timeout 8 min Duration The maximum time to wait for acquiring the lock.   lock-check-max-sleep 8 s Duration The maximum sleep time when retrying to check the lock.   lock.enabled (none) Boolean Enable Catalog Lock.   lock.type (none) String The Lock Type for Catalog, such as 'hive', 'zookeeper'.   metastore \"filesystem\" String Metastore of paimon catalog, supports filesystem, hive and jdbc.   sync-all-properties false Boolean Sync all table properties to hive metastore   table.type managed Enum\n Type of table.\nPossible values:\"managed\": Paimon owned table where the entire lifecycle of the table data is managed.\"external\": The table where Paimon has loose coupling with the data stored in external locations.   uri (none) String Uri of metastore server.   warehouse (none) String The warehouse root path of catalog.    FilesystemCatalogOptions #  Options for Filesystem catalog.\n  Key Default Type Description     case-sensitive true Boolean Is case sensitive. If case insensitive, you need to set this option to false, and the table name and fields be converted to lowercase.    HiveCatalogOptions #  Options for Hive catalog.\n  Key Default Type Description     client-pool-cache.eviction-interval-ms 300000 Long Setting the client's pool cache eviction interval(ms).    client-pool-cache.keys (none) String Specify client cache key, multiple elements separated by commas.\n\"ugi\": the Hadoop UserGroupInformation instance that represents the current user using the cache.\"user_name\" similar to UGI but only includes the user's name determined by UserGroupInformation#getUserName.\"conf\": name of an arbitrary configuration. The value of the configuration will be extracted from catalog properties and added to the cache key. A conf element should start with a \"conf:\" prefix which is followed by the configuration name. E.g. specifying \"conf:a.b.c\" will add \"a.b.c\" to the key, and so that configurations with different default catalog wouldn't share the same client pool. Multiple conf elements can be specified.   format-table.enabled false Boolean Whether to support format tables, format table corresponds to a regular Hive table, allowing read and write operations. However, during these processes, it does not connect to the metastore; hence, newly added partitions will not be reflected in the metastore and need to be manually added as separate partition operations.   hadoop-conf-dir (none) String File directory of the core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml. Currently, only local file system paths are supported. If not configured, try to load from 'HADOOP_CONF_DIR' or 'HADOOP_HOME' system environment. Configure Priority: 1.from 'hadoop-conf-dir' 2.from HADOOP_CONF_DIR 3.from HADOOP_HOME/conf 4.HADOOP_HOME/etc/hadoop.    hive-conf-dir (none) String File directory of the hive-site.xml , used to create HiveMetastoreClient and security authentication, such as Kerberos, LDAP, Ranger and so on. If not configured, try to load from 'HIVE_CONF_DIR' env.    location-in-properties false Boolean Setting the location in properties of hive table/database. If you don't want to access the location by the filesystem of hive when using a object storage such as s3,oss you can set this option to true.     JdbcCatalogOptions #  Options for Jdbc catalog.\n  Key Default Type Description     catalog-key \"jdbc\" String Custom jdbc catalog store key.   lock-key-max-length 255 Integer Set the maximum length of the lock key. The 'lock-key' is composed of concatenating three fields : 'catalog-key', 'database', and 'table'.    FlinkCatalogOptions #  Flink catalog options for paimon.\n  Key Default Type Description     default-database \"default\" String    disable-create-table-in-default-db false Boolean If true, creating table in default database is not allowed. Default is false.    FlinkConnectorOptions #  Flink connector options for paimon.\n  Key Default Type Description     end-input.watermark (none) Long Optional endInput watermark used in case of batch mode or bounded stream.   lookup.async false Boolean Whether to enable async lookup join.   lookup.async-thread-number 16 Integer The thread number for lookup async.   lookup.bootstrap-parallelism 4 Integer The parallelism for bootstrap in a single task for lookup join.   lookup.cache AUTO Enum\n The cache mode of lookup join.\nPossible values:\"AUTO\"\"FULL\"   lookup.dynamic-partition (none) String Specific dynamic partition for lookup, only support 'max_pt()' currently.   lookup.dynamic-partition.refresh-interval 1 h Duration Specific dynamic partition refresh interval for lookup, scan all partitions and obtain corresponding partition.   lookup.refresh.async false Boolean Whether to refresh lookup table in an async thread.   lookup.refresh.async.pending-snapshot-count 5 Integer If the pending snapshot count exceeds the threshold, lookup operator will refresh the table in sync.   partition.end-input-to-done false Boolean Whether mark the done status to indicate that the data is ready when end input.   partition.idle-time-to-done (none) Duration Set a time duration when a partition has no new data after this time duration, mark the done status to indicate that the data is ready.   partition.time-interval (none) Duration You can specify time interval for partition, for example, daily partition is '1 d', hourly partition is '1 h'.   scan.infer-parallelism true Boolean If it is false, parallelism of source are set by global parallelism. Otherwise, source parallelism is inferred from splits number (batch mode) or bucket number(streaming mode).   scan.infer-parallelism.max 1024 Integer If scan.infer-parallelism is true, limit the parallelism of source through this option.   scan.parallelism (none) Integer Define a custom parallelism for the scan source. By default, if this option is not defined, the planner will derive the parallelism for each statement individually by also considering the global configuration. If user enable the scan.infer-parallelism, the planner will derive the parallelism by inferred parallelism.   scan.push-down true Boolean If true, flink will push down projection, filters, limit to the source. The cost is that it is difficult to reuse the source in a job. With flink 1.18 or higher version, it is possible to reuse the source even with projection push down.   scan.remove-normalize false Boolean Whether to force the removal of the normalize node when streaming read. Note: This is dangerous and is likely to cause data errors if downstream is used to calculate aggregation and the input is not complete changelog.   scan.split-enumerator.batch-size 10 Integer How many splits should assign to subtask per batch in StaticFileStoreSplitEnumerator to avoid exceed `akka.framesize` limit.   scan.split-enumerator.mode fair Enum\n The mode used by StaticFileStoreSplitEnumerator to assign splits.\nPossible values:\"fair\": Distribute splits evenly when batch reading to prevent a few tasks from reading all.\"preemptive\": Distribute splits preemptively according to the consumption speed of the task.   scan.watermark.alignment.group (none) String A group of sources to align watermarks.   scan.watermark.alignment.max-drift (none) Duration Maximal drift to align watermarks, before we pause consuming from the source/task/partition.   scan.watermark.alignment.update-interval 1 s Duration How often tasks should notify coordinator about the current watermark and how often the coordinator should announce the maximal aligned watermark.   scan.watermark.emit.strategy on-event Enum\n Emit strategy for watermark generation.\nPossible values:\"on-periodic\": Emit watermark periodically, interval is controlled by Flink 'pipeline.auto-watermark-interval'.\"on-event\": Emit watermark per record.   scan.watermark.idle-timeout (none) Duration If no records flow in a partition of a stream for that amount of time, then that partition is considered \"idle\" and will not hold back the progress of watermarks in downstream operators.   sink.clustering.by-columns (none) String Specifies the column name(s) used for comparison during range partitioning, in the format 'columnName1,columnName2'. If not set or set to an empty string, it indicates that the range partitioning feature is not enabled. This option will be effective only for bucket unaware table without primary keys and batch execution mode.   sink.clustering.sample-factor 100 Integer Specifies the sample factor. Let S represent the total number of samples, F represent the sample factor, and P represent the sink parallelism, then S=F×P. The minimum allowed sample factor is 20.   sink.clustering.sort-in-cluster true Boolean Indicates whether to further sort data belonged to each sink task after range partitioning.   sink.clustering.strategy \"auto\" String Specifies the comparison algorithm used for range partitioning, including 'zorder', 'hilbert', and 'order', corresponding to the z-order curve algorithm, hilbert curve algorithm, and basic type comparison algorithm, respectively. When not configured, it will automatically determine the algorithm based on the number of columns in 'sink.clustering.by-columns'. 'order' is used for 1 column, 'zorder' for less than 5 columns, and 'hilbert' for 5 or more columns.   sink.committer-cpu 1.0 Double Sink committer cpu to control cpu cores of global committer.   sink.committer-memory (none) MemorySize Sink committer memory to control heap memory of global committer.   sink.committer-operator-chaining true Boolean Allow sink committer and writer operator to be chained together   sink.cross-partition.managed-memory 256 mb MemorySize Weight of managed memory for RocksDB in cross-partition update, Flink will compute the memory size according to the weight, the actual memory used depends on the running environment.   sink.managed.writer-buffer-memory 256 mb MemorySize Weight of writer buffer in managed memory, Flink will compute the memory size for writer according to the weight, the actual memory used depends on the running environment.   sink.parallelism (none) Integer Defines a custom parallelism for the sink. By default, if this option is not defined, the planner will derive the parallelism for each statement individually by also considering the global configuration.   sink.savepoint.auto-tag false Boolean If true, a tag will be automatically created for the snapshot created by flink savepoint.   sink.use-managed-memory-allocator false Boolean If true, flink sink will use managed memory for merge tree; otherwise, it will create an independent memory allocator.   source.checkpoint-align.enabled false Boolean Whether to align the flink checkpoint with the snapshot of the paimon table, If true, a checkpoint will only be made if a snapshot is consumed.   source.checkpoint-align.timeout 30 s Duration If the new snapshot has not been generated when the checkpoint starts to trigger, the enumerator will block the checkpoint and wait for the new snapshot. Set the maximum waiting time to avoid infinite waiting, if timeout, the checkpoint will fail. Note that it should be set smaller than the checkpoint timeout.   streaming-read.shuffle-bucket-with-partition true Boolean Whether shuffle by partition and bucket when streaming read.   unaware-bucket.compaction.parallelism (none) Integer Defines a custom parallelism for the unaware-bucket table compaction job. By default, if this option is not defined, the planner will derive the parallelism for each statement individually by also considering the global configuration.    SparkCatalogOptions #  Spark catalog options for paimon.\n  Key Default Type Description     catalog.create-underlying-session-catalog false Boolean If true, create and use an underlying session catalog instead of default session catalog when use SparkGenericCatalog.   defaultDatabase \"default\" String The default database name.    SparkConnectorOptions #  Spark connector options for paimon.\n  Key Default Type Description     read.changelog false Boolean Whether to read row in the form of changelog (add rowkind column in row to represent its change type).   read.stream.maxBytesPerTrigger (none) Long The maximum number of bytes returned in a single batch.   read.stream.maxFilesPerTrigger (none) Integer The maximum number of files returned in a single batch.   read.stream.maxRowsPerTrigger (none) Long The maximum number of rows returned in a single batch.   read.stream.maxTriggerDelayMs (none) Long The maximum delay between two adjacent batches, which used to create MinRowsReadLimit with read.stream.minRowsPerTrigger together.   read.stream.minRowsPerTrigger (none) Long The minimum number of rows returned in a single batch, which used to create MinRowsReadLimit with read.stream.maxTriggerDelayMs together.   write.merge-schema false Boolean If true, merge the data schema and the table schema automatically before write data.   write.merge-schema.explicit-cast false Boolean If true, allow to merge data types if the two types meet the rules for explicit casting.    ORC Options #    Key Default Type Description     orc.column.encoding.direct (none) Integer Comma-separated list of fields for which dictionary encoding is to be skipped in orc.   orc.dictionary.key.threshold 0.8 Double If the number of distinct keys in a dictionary is greater than this fraction of the total number of non-null rows, turn off dictionary encoding in orc. Use 0 to always disable dictionary encoding. Use 1 to always use dictionary encoding.   orc.write.batch-size 1024 Integer write batch size for orc.    RocksDB Options #  The following options allow users to finely adjust RocksDB for better performance. You can either specify them in table properties or in dynamic table hints.\n  Key Default Type Description     lookup.cache-rows 10000 Long The maximum number of rows to store in the cache.   lookup.continuous.discovery-interval (none) Duration The discovery interval of lookup continuous reading. This is used as an SQL hint. If it's not configured, the lookup function will fallback to 'continuous.discovery-interval'.   rocksdb.block.blocksize 4 kb MemorySize The approximate size (in bytes) of user data packed per block. The default blocksize is '4KB'.   rocksdb.block.cache-size 128 mb MemorySize The amount of the cache for data blocks in RocksDB.   rocksdb.block.metadata-blocksize 4 kb MemorySize Approximate size of partitioned metadata packed per block. Currently applied to indexes block when partitioned index/filters option is enabled. The default blocksize is '4KB'.   rocksdb.bloom-filter.bits-per-key 10.0 Double Bits per key that bloom filter will use, this only take effect when bloom filter is used. The default value is 10.0.   rocksdb.bloom-filter.block-based-mode false Boolean If true, RocksDB will use block-based filter instead of full filter, this only take effect when bloom filter is used. The default value is 'false'.   rocksdb.compaction.level.max-size-level-base 256 mb MemorySize The upper-bound of the total size of level base files in bytes. The default value is '256MB'.   rocksdb.compaction.level.target-file-size-base 64 mb MemorySize The target file size for compaction, which determines a level-1 file size. The default value is '64MB'.   rocksdb.compaction.level.use-dynamic-size false Boolean If true, RocksDB will pick target size of each level dynamically. From an empty DB, RocksDB would make last level the base level, which means merging L0 data into the last level, until it exceeds max_bytes_for_level_base. And then repeat this process for second last level and so on. The default value is 'false'. For more information, please refer to RocksDB's doc.   rocksdb.compaction.style LEVEL Enum\n The specified compaction style for DB. Candidate compaction style is LEVEL, FIFO, UNIVERSAL or NONE, and Flink chooses 'LEVEL' as default style.\nPossible values:\"LEVEL\"\"UNIVERSAL\"\"FIFO\"\"NONE\"   rocksdb.compression.type LZ4_COMPRESSION Enum\n The compression type.\nPossible values:\"NO_COMPRESSION\"\"SNAPPY_COMPRESSION\"\"ZLIB_COMPRESSION\"\"BZLIB2_COMPRESSION\"\"LZ4_COMPRESSION\"\"LZ4HC_COMPRESSION\"\"XPRESS_COMPRESSION\"\"ZSTD_COMPRESSION\"\"DISABLE_COMPRESSION_OPTION\"   rocksdb.files.open -1 Integer The maximum number of open files (per stateful operator) that can be used by the DB, '-1' means no limit. The default value is '-1'.   rocksdb.thread.num 2 Integer The maximum number of concurrent background flush and compaction jobs (per stateful operator). The default value is '2'.   rocksdb.use-bloom-filter false Boolean If true, every newly created SST file will contain a Bloom filter. It is disabled by default.   rocksdb.writebuffer.count 2 Integer The maximum number of write buffers that are built up in memory. The default value is '2'.   rocksdb.writebuffer.number-to-merge 1 Integer The minimum number of write buffers that will be merged together before writing to storage. The default value is '1'.   rocksdb.writebuffer.size 64 mb MemorySize The amount of data built up in memory (backed by an unsorted log on disk) before converting to a sorted on-disk files. The default writebuffer size is '64MB'.    "});index.add({'id':92,'href':'/docs/0.9/versions/','title':"Versions",'section':"Apache Paimon",'content':"Versions #  An appendix of hosted documentation for all versions of Apache Paimon.\n master    stable    0.9    0.8    "});})();