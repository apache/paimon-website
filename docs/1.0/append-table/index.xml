<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Table w/o PK on Apache Paimon</title>
    <link>//paimon.apache.org/docs/1.0/append-table/</link>
    <description>Recent content in Table w/o PK on Apache Paimon</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="//paimon.apache.org/docs/1.0/append-table/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Overview</title>
      <link>//paimon.apache.org/docs/1.0/append-table/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/1.0/append-table/overview/</guid>
      <description>Overview #  If a table does not have a primary key defined, it is an append table. Compared to the primary key table, it does not have the ability to directly receive changelogs. It cannot be directly updated with data through upsert. It can only receive incoming data from append data.
Flink CREATE TABLE my_table ( product_id BIGINT, price DOUBLE, sales BIGINT ) WITH ( -- &amp;#39;target-file-size&amp;#39; = &amp;#39;256 MB&amp;#39;,  -- &amp;#39;file.</description>
    </item>
    
    <item>
      <title>Streaming</title>
      <link>//paimon.apache.org/docs/1.0/append-table/streaming/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/1.0/append-table/streaming/</guid>
      <description>Streaming #  You can streaming write to the Append table in a very flexible way through Flink, or through read the Append table Flink, using it like a queue. The only difference is that its latency is in minutes. Its advantages are very low cost and the ability to push down filters and projection.
Automatic small file merging #  In streaming writing job, without bucket definition, there is no compaction in writer, instead, will use Compact Coordinator to scan the small files and pass compaction task to Compact Worker.</description>
    </item>
    
    <item>
      <title>Query Performance</title>
      <link>//paimon.apache.org/docs/1.0/append-table/query-performance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/1.0/append-table/query-performance/</guid>
      <description>Query Performance #  Data Skipping By Order #  Paimon by default records the maximum and minimum values of each field in the manifest file.
In the query, according to the WHERE condition of the query, according to the statistics in the manifest do files filtering, if the filtering effect is good, the query would have been minutes of the query will be accelerated to milliseconds to complete the execution.</description>
    </item>
    
    <item>
      <title>Update</title>
      <link>//paimon.apache.org/docs/1.0/append-table/update/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/1.0/append-table/update/</guid>
      <description>Update #  Now, only Spark SQL supports DELETE &amp;amp; UPDATE, you can take a look to Spark Write.
Example:
DELETE FROM my_table WHERE currency = &amp;#39;UNKNOWN&amp;#39;; Update append table has two modes:
 COW (Copy on Write): search for the hit files and then rewrite each file to remove the data that needs to be deleted from the files. This operation is costly. MOW (Merge on Write): By specifying &#39;deletion-vectors.</description>
    </item>
    
    <item>
      <title>Bucketed</title>
      <link>//paimon.apache.org/docs/1.0/append-table/bucketed/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/1.0/append-table/bucketed/</guid>
      <description>Bucketed Append #  You can define the bucket and bucket-key to get a bucketed append table.
Example to create bucketed append table:
Flink CREATE TABLE my_table ( product_id BIGINT, price DOUBLE, sales BIGINT ) WITH ( &amp;#39;bucket&amp;#39; = &amp;#39;8&amp;#39;, &amp;#39;bucket-key&amp;#39; = &amp;#39;product_id&amp;#39; );  Streaming #  An ordinary Append table has no strict ordering guarantees for its streaming writes and reads, but there are some cases where you need to define a key similar to Kafka&amp;rsquo;s.</description>
    </item>
    
  </channel>
</rss>
