'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href','section'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/docs/1.1/concepts/','title':"Concepts",'section':"Apache Paimon",'content':""});index.add({'id':1,'href':'/docs/1.1/maintenance/filesystems/','title':"Filesystems",'section':"Maintenance",'content':"Filesystems #  Apache Paimon utilizes the same pluggable file systems as Apache Flink. Users can follow the standard plugin mechanism to configure the plugin structure if using Flink as compute engine. However, for other engines like Spark or Hive, the provided opt jars (by Flink) may get conflicts and cannot be used directly. It is not convenient for users to fix class conflicts, thus Paimon provides the self-contained and engine-unified FileSystem pluggable jars for user to query tables from Spark/Hive side.\nSupported FileSystems #     FileSystem URI Scheme Pluggable Description     Local File System file:// N Built-in Support   HDFS hdfs:// N Built-in Support, ensure that the cluster is in the hadoop environment   Aliyun OSS oss:// Y    S3 s3:// Y    Tencent Cloud Object Storage cosn:// Y    Huawei OBS obs:// Y     Dependency #  We recommend you to download the jar directly: Download Link.\nYou can also manually build bundled jar from the source code.\nTo build from source code, clone the git repository.\nBuild shaded jar with the following command.\nmvn clean install -DskipTests You can find the shaded jars under ./paimon-filesystems/paimon-${fs}/target/paimon-${fs}-1.1.1.jar.\nHDFS #  You don\u0026rsquo;t need any additional dependencies to access HDFS because you have already taken care of the Hadoop dependencies.\nHDFS Configuration #  For HDFS, the most important thing is to be able to read your HDFS configuration.\nFlink You may not have to do anything, if you are in a hadoop environment. Otherwise pick one of the following ways to configure your HDFS:\n Set environment variable HADOOP_HOME or HADOOP_CONF_DIR. Configure 'hadoop-conf-dir' in the paimon catalog. Configure Hadoop options through prefix 'hadoop.' in the paimon catalog.  The first approach is recommended.\nIf you do not want to include the value of the environment variable, you can configure hadoop-conf-loader to option.\nHive/Spark HDFS Configuration is available directly through the computation cluster, see cluster configuration of Hive and Spark for details. Hadoop-compatible file systems (HCFS) #  All Hadoop file systems are automatically available when the Hadoop libraries are on the classpath.\nThis way, Paimon seamlessly supports all of Hadoop file systems implementing the org.apache.hadoop.fs.FileSystem interface, and all Hadoop-compatible file systems (HCFS).\n HDFS Alluxio (see configuration specifics below) XtreemFS …  The Hadoop configuration has to have an entry for the required file system implementation in the core-site.xml file.\nFor Alluxio support add the following entry into the core-site.xml file:\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt;fs.alluxio.impl\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;alluxio.hadoop.FileSystem\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; Kerberos #  Flink It is recommended to use Flink Kerberos Keytab.Spark It is recommended to use Spark Kerberos Keytab.Hive An intuitive approach is to configure Hive\u0026rsquo;s kerberos authentication.Trino/JavaAPI Configure the following three options in your catalog configuration:\n security.kerberos.login.keytab: Absolute path to a Kerberos keytab file that contains the user credentials. Please make sure it is copied to each machine. security.kerberos.login.principal: Kerberos principal name associated with the keytab. security.kerberos.login.use-ticket-cache: True or false, indicates whether to read from your Kerberos ticket cache.  For JavaAPI:\nSecurityContext.install(catalogOptions);  HDFS HA #  Ensure that hdfs-site.xml and core-site.xml contain the necessary HA configuration.\nHDFS ViewFS #  Ensure that hdfs-site.xml and core-site.xml contain the necessary ViewFs configuration.\nOSS #  Download paimon-oss-1.1.1.jar. Flink If you have already configured oss access through Flink (Via Flink FileSystem), here you can skip the following configuration.  Put paimon-oss-1.1.1.jar into lib directory of your Flink home, and create catalog:\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;oss://\u0026lt;bucket\u0026gt;/\u0026lt;path\u0026gt;\u0026#39;, \u0026#39;fs.oss.endpoint\u0026#39; = \u0026#39;oss-cn-hangzhou.aliyuncs.com\u0026#39;, \u0026#39;fs.oss.accessKeyId\u0026#39; = \u0026#39;xxx\u0026#39;, \u0026#39;fs.oss.accessKeySecret\u0026#39; = \u0026#39;yyy\u0026#39; ); Spark If you have already configured oss access through Spark (Via Hadoop FileSystem), here you can skip the following configuration.  Place paimon-oss-1.1.1.jar together with paimon-spark-1.1.1.jar under Spark\u0026rsquo;s jars directory, and start like\nspark-sql \\  --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\  --conf spark.sql.catalog.paimon.warehouse=oss://\u0026lt;bucket\u0026gt;/\u0026lt;path\u0026gt; \\  --conf spark.sql.catalog.paimon.fs.oss.endpoint=oss-cn-hangzhou.aliyuncs.com \\  --conf spark.sql.catalog.paimon.fs.oss.accessKeyId=xxx \\  --conf spark.sql.catalog.paimon.fs.oss.accessKeySecret=yyy Hive If you have already configured oss access through Hive (Via Hadoop FileSystem), here you can skip the following configuration.  NOTE: You need to ensure that Hive metastore can access oss.\nPlace paimon-oss-1.1.1.jar together with paimon-hive-connector-1.1.1.jar under Hive\u0026rsquo;s auxlib directory, and start like\nSET paimon.fs.oss.endpoint=oss-cn-hangzhou.aliyuncs.com; SET paimon.fs.oss.accessKeyId=xxx; SET paimon.fs.oss.accessKeySecret=yyy; And read table from hive metastore, table can be created by Flink or Spark, see Catalog with Hive Metastore\nSELECT * FROM test_table; SELECT COUNT(1) FROM test_table; Trino From version 0.8, paimon-trino uses trino filesystem as basic file read and write system. We strongly recommend you to use jindo-sdk in trino.\nYou can find How to config jindo sdk on trino here. Please note that:\n Use paimon to replace hive-hadoop2 when you decompress the plugin jar and find location to put in. You can specify the core-site.xml in paimon.properties on configuration hive.config.resources. Presto and Jindo use the same configuration method.   If you environment has jindo sdk dependencies, you can use Jindo Fs to connect OSS. Jindo has better read and write efficiency.\nDownload paimon-jindo-1.1.1.jar. S3 #  Download paimon-s3-1.1.1.jar. Flink If you have already configured s3 access through Flink (Via Flink FileSystem), here you can skip the following configuration.  Put paimon-s3-1.1.1.jar into lib directory of your Flink home, and create catalog:\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;s3://\u0026lt;bucket\u0026gt;/\u0026lt;path\u0026gt;\u0026#39;, \u0026#39;s3.endpoint\u0026#39; = \u0026#39;your-endpoint-hostname\u0026#39;, \u0026#39;s3.access-key\u0026#39; = \u0026#39;xxx\u0026#39;, \u0026#39;s3.secret-key\u0026#39; = \u0026#39;yyy\u0026#39; ); Spark If you have already configured s3 access through Spark (Via Hadoop FileSystem), here you can skip the following configuration.  Place paimon-s3-1.1.1.jar together with paimon-spark-1.1.1.jar under Spark\u0026rsquo;s jars directory, and start like\nspark-sql \\  --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\  --conf spark.sql.catalog.paimon.warehouse=s3://\u0026lt;bucket\u0026gt;/\u0026lt;path\u0026gt; \\  --conf spark.sql.catalog.paimon.s3.endpoint=your-endpoint-hostname \\  --conf spark.sql.catalog.paimon.s3.access-key=xxx \\  --conf spark.sql.catalog.paimon.s3.secret-key=yyy Hive If you have already configured s3 access through Hive ((Via Hadoop FileSystem)), here you can skip the following configuration.  NOTE: You need to ensure that Hive metastore can access s3.\nPlace paimon-s3-1.1.1.jar together with paimon-hive-connector-1.1.1.jar under Hive\u0026rsquo;s auxlib directory, and start like\nSET paimon.s3.endpoint=your-endpoint-hostname; SET paimon.s3.access-key=xxx; SET paimon.s3.secret-key=yyy; And read table from hive metastore, table can be created by Flink or Spark, see Catalog with Hive Metastore\nSELECT * FROM test_table; SELECT COUNT(1) FROM test_table; Trino Paimon use shared trino filesystem as basic read and write system.\nPlease refer to Trino S3 to config s3 filesystem in trino.\n S3 Compliant Object Stores #  The S3 Filesystem also support using S3 compliant object stores such as MinIO, Tencent\u0026rsquo;s COS and IBM’s Cloud Object Storage. Just configure your endpoint to the provider of the object store service.\ns3.endpoint:your-endpoint-hostnameConfigure Path Style Access #  Some S3 compliant object stores might not have virtual host style addressing enabled by default, for example when using Standalone MinIO for testing purpose. In such cases, you will have to provide the property to enable path style access.\ns3.path.style.access:trueS3A Performance #  Tune Performance for S3AFileSystem.\nIf you encounter the following exception:\nCaused by: org.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool. Try to configure this in catalog options: fs.s3a.connection.maximum=1000.\nGoogle Cloud Storage #  Download paimon-gs-1.1.1.jar. Flink If you have already configured oss access through Flink (Via Flink FileSystem), here you can skip the following configuration.  Put paimon-gs-1.1.1.jar into lib directory of your Flink home, and create catalog:\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;oss://\u0026lt;bucket\u0026gt;/\u0026lt;path\u0026gt;\u0026#39;, \u0026#39;fs.gs.auth.type\u0026#39; = \u0026#39;SERVICE_ACCOUNT_JSON_KEYFILE\u0026#39;, \u0026#39;fs.gs.auth.service.account.json.keyfile\u0026#39; = \u0026#39;/path/to/service-account-.json\u0026#39; );  Microsoft Azure Storage #  Download paimon-azure-1.1.1.jar. Flink If you have already configured oss access through Flink (Via Flink FileSystem), here you can skip the following configuration.  Put paimon-gs-1.1.1.jar into lib directory of your Flink home, and create catalog:\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;oss://\u0026lt;bucket\u0026gt;/\u0026lt;path\u0026gt;\u0026#39;, \u0026#39;fs.gs.auth.type\u0026#39; = \u0026#39;SERVICE_ACCOUNT_JSON_KEYFILE\u0026#39;, \u0026#39;fs.gs.auth.service.account.json.keyfile\u0026#39; = \u0026#39;/path/to/service-account-.json\u0026#39; );  Microsoft Azure Storage #  Download paimon-azure-1.1.1.jar. Flink If you have already configured azure access through Flink (Via Flink FileSystem), here you can skip the following configuration.  Put paimon-azure-1.1.1.jar into lib directory of your Flink home, and create catalog:\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;wasb://,\u0026lt;container\u0026gt;@\u0026lt;account\u0026gt;.blob.core.windows.net/\u0026lt;path\u0026gt;\u0026#39;, \u0026#39;fs.azure.account.key.Account.blob.core.windows.net\u0026#39; = \u0026#39;yyy\u0026#39; ); Spark If you have already configured azure access through Spark (Via Hadoop FileSystem), here you can skip the following configuration.  Place paimon-azure-1.1.1.jar together with paimon-spark-1.1.1.jar under Spark\u0026rsquo;s jars directory, and start like\nspark-sql \\  --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\  --conf spark.sql.catalog.paimon.warehouse=wasb://,\u0026lt;container\u0026gt;@\u0026lt;account\u0026gt;.blob.core.windows.net/\u0026lt;path\u0026gt; \\  --conf fs.azure.account.key.Account.blob.core.windows.net=yyy \\  OBS #  Download paimon-obs-1.1.1.jar. Flink If you have already configured obs access through Flink (Via Flink FileSystem), here you can skip the following configuration.  Put paimon-obs-1.1.1.jar into lib directory of your Flink home, and create catalog:\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;obs://\u0026lt;bucket\u0026gt;/\u0026lt;path\u0026gt;\u0026#39;, \u0026#39;fs.obs.endpoint\u0026#39; = \u0026#39;obs-endpoint-hostname\u0026#39;, \u0026#39;fs.obs.access.key\u0026#39; = \u0026#39;xxx\u0026#39;, \u0026#39;fs.obs.secret.key\u0026#39; = \u0026#39;yyy\u0026#39; ); Spark If you have already configured obs access through Spark (Via Hadoop FileSystem), here you can skip the following configuration.  Place paimon-obs-1.1.1.jar together with paimon-spark-1.1.1.jar under Spark\u0026rsquo;s jars directory, and start like\nspark-sql \\  --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\  --conf spark.sql.catalog.paimon.warehouse=obs://\u0026lt;bucket\u0026gt;/\u0026lt;path\u0026gt; \\  --conf spark.sql.catalog.paimon.fs.obs.endpoint=obs-endpoint-hostname \\  --conf spark.sql.catalog.paimon.fs.obs.access.key=xxx \\  --conf spark.sql.catalog.paimon.fs.obs.secret.key=yyy Hive If you have already configured obs access through Hive ((Via Hadoop FileSystem)), here you can skip the following configuration.  NOTE: You need to ensure that Hive metastore can access obs.\nPlace paimon-obs-1.1.1.jar together with paimon-hive-connector-1.1.1.jar under Hive\u0026rsquo;s auxlib directory, and start like\nSET paimon.fs.obs.endpoint=obs-endpoint-hostname; SET paimon.fs.obs.access.key=xxx; SET paimon.fs.obs.secret.key=yyy; And read table from hive metastore, table can be created by Flink or Spark, see Catalog with Hive Metastore\nSELECT * FROM test_table; SELECT COUNT(1) FROM test_table;  "});index.add({'id':2,'href':'/docs/1.1/program-api/flink-api/','title':"Flink API",'section':"Program API",'content':"Flink API #  If possible, recommend using Flink SQL or Spark SQL, or simply use SQL APIs in programs.  Dependency #  Maven dependency:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.paimon\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;paimon-flink-1.20\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.1.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-table-api-java-bridge\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.20.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; Or download the jar file: Paimon Flink. Please choose your Flink version.\nPaimon relies on Hadoop environment, you should add hadoop classpath or bundled jar.\nNot only DataStream API, you can also read or write to Paimon tables by the conversion between DataStream and Table in Flink. See DataStream API Integration.\nWrite to Table #  import org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; import org.apache.paimon.flink.FlinkCatalogFactory; import org.apache.paimon.flink.sink.FlinkSinkBuilder; import org.apache.paimon.options.Options; import org.apache.paimon.table.Table; import org.apache.flink.api.common.typeinfo.Types; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.table.api.DataTypes; import org.apache.flink.table.types.DataType; import org.apache.flink.types.Row; import org.apache.flink.types.RowKind; public class WriteToTable { public static void writeTo() throws Exception { // create environments of both APIs  StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // for CONTINUOUS_UNBOUNDED source, set checkpoint interval  // env.enableCheckpointing(60_000);  // create a changelog DataStream  DataStream\u0026lt;Row\u0026gt; input = env.fromElements( Row.ofKind(RowKind.INSERT, \u0026#34;Alice\u0026#34;, 12), Row.ofKind(RowKind.INSERT, \u0026#34;Bob\u0026#34;, 5), Row.ofKind(RowKind.UPDATE_BEFORE, \u0026#34;Alice\u0026#34;, 12), Row.ofKind(RowKind.UPDATE_AFTER, \u0026#34;Alice\u0026#34;, 100)) .returns( Types.ROW_NAMED( new String[] {\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;}, Types.STRING, Types.INT)); // get table from catalog  Options catalogOptions = new Options(); catalogOptions.set(\u0026#34;warehouse\u0026#34;, \u0026#34;/path/to/warehouse\u0026#34;); Catalog catalog = FlinkCatalogFactory.createPaimonCatalog(catalogOptions); Table table = catalog.getTable(Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;T\u0026#34;)); DataType inputType = DataTypes.ROW( DataTypes.FIELD(\u0026#34;name\u0026#34;, DataTypes.STRING()), DataTypes.FIELD(\u0026#34;age\u0026#34;, DataTypes.INT())); FlinkSinkBuilder builder = new FlinkSinkBuilder(table).forRow(input, inputType); // set sink parallelism  // builder.parallelism(_your_parallelism)  // set overwrite mode  // builder.overwrite(...)  builder.build(); env.execute(); } } Read from Table #  import org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; import org.apache.paimon.flink.FlinkCatalogFactory; import org.apache.paimon.flink.source.FlinkSourceBuilder; import org.apache.paimon.options.Options; import org.apache.paimon.table.Table; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.types.Row; public class ReadFromTable { public static void readFrom() throws Exception { // create environments of both APIs  StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // get table from catalog  Options catalogOptions = new Options(); catalogOptions.set(\u0026#34;warehouse\u0026#34;, \u0026#34;/path/to/warehouse\u0026#34;); Catalog catalog = FlinkCatalogFactory.createPaimonCatalog(catalogOptions); Table table = catalog.getTable(Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;T\u0026#34;)); // table = table.copy(Collections.singletonMap(\u0026#34;scan.file-creation-time-millis\u0026#34;, \u0026#34;...\u0026#34;));  FlinkSourceBuilder builder = new FlinkSourceBuilder(table).env(env); // builder.sourceBounded(true);  // builder.projection(...);  // builder.predicate(...);  // builder.limit(...);  // builder.sourceParallelism(...);  DataStream\u0026lt;Row\u0026gt; dataStream = builder.buildForRow(); // use this datastream  dataStream.executeAndCollect().forEachRemaining(System.out::println); // prints:  // +I[Bob, 12]  // +I[Alice, 12]  // -U[Alice, 12]  // +U[Alice, 14]  } } Cdc ingestion Table #  Paimon supports ingest data into Paimon tables with schema evolution.\n You can use Java API to write cdc records into Paimon Tables. You can write records to Paimon\u0026rsquo;s partial-update table with adding columns dynamically.  Here is an example to use RichCdcSinkBuilder API:\nimport org.apache.paimon.catalog.CatalogLoader; import org.apache.paimon.flink.FlinkCatalogFactory; import org.apache.paimon.catalog.Identifier; import org.apache.paimon.flink.sink.cdc.RichCdcRecord; import org.apache.paimon.flink.sink.cdc.RichCdcSinkBuilder; import org.apache.paimon.options.Options; import org.apache.paimon.table.Table; import org.apache.paimon.types.DataTypes; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import static org.apache.paimon.types.RowKind.INSERT; public class WriteCdcToTable { public static void writeTo() throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // for CONTINUOUS_UNBOUNDED source, set checkpoint interval  // env.enableCheckpointing(60_000);  DataStream\u0026lt;RichCdcRecord\u0026gt; dataStream = env.fromElements( RichCdcRecord.builder(INSERT) .field(\u0026#34;order_id\u0026#34;, DataTypes.BIGINT(), \u0026#34;123\u0026#34;) .field(\u0026#34;price\u0026#34;, DataTypes.DOUBLE(), \u0026#34;62.2\u0026#34;) .build(), // dt field will be added with schema evolution  RichCdcRecord.builder(INSERT) .field(\u0026#34;order_id\u0026#34;, DataTypes.BIGINT(), \u0026#34;245\u0026#34;) .field(\u0026#34;price\u0026#34;, DataTypes.DOUBLE(), \u0026#34;82.1\u0026#34;) .field(\u0026#34;dt\u0026#34;, DataTypes.TIMESTAMP(), \u0026#34;2023-06-12 20:21:12\u0026#34;) .build()); Identifier identifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;T\u0026#34;); Options catalogOptions = new Options(); catalogOptions.set(\u0026#34;warehouse\u0026#34;, \u0026#34;/path/to/warehouse\u0026#34;); CatalogLoader catalogLoader = () -\u0026gt; FlinkCatalogFactory.createPaimonCatalog(catalogOptions); Table table = catalogLoader.load().getTable(identifier); new RichCdcSinkBuilder(table) .forRichCdcRecord(dataStream) .identifier(identifier) .catalogLoader(catalogLoader) .build(); env.execute(); } } "});index.add({'id':3,'href':'/docs/1.1/migration/migration-from-hive/','title':"Migration From Hive",'section':"Migration",'content':"Hive Table Migration #  Apache Hive supports ORC, Parquet file formats that could be migrated to Paimon. When migrating data to a paimon table, the origin table will be permanently disappeared. So please back up your data if you still need the original table. The migrated table will be append table.\nNow, we can use paimon hive catalog with Migrate Table Procedure to totally migrate a table from hive to paimon. At the same time, you can use paimon hive catalog with Migrate Database Procedure to fully synchronize all tables in the database to paimon.\n Migrate Table Procedure: Paimon table does not exist, use the procedure upgrade hive table to paimon table. Hive table will disappear after action done. Migrate Database Procedure: Paimon table does not exist, use the procedure upgrade all hive tables in database to paimon table. All hive tables will disappear after action done.  These three actions now support file format of hive \u0026ldquo;orc\u0026rdquo; and \u0026ldquo;parquet\u0026rdquo; and \u0026ldquo;avro\u0026rdquo;.\nWe highly recommend to back up hive table data before migrating, because migrating action is not atomic. If been interrupted while migrating, you may lose your data. \nMigrate Hive Table #  Flink SQL CREATE CATALOG PAIMON WITH ( \u0026#39;type\u0026#39;=\u0026#39;paimon\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;hive\u0026#39;, \u0026#39;uri\u0026#39; = \u0026#39;thrift://localhost:9083\u0026#39;, \u0026#39;warehouse\u0026#39;=\u0026#39;/path/to/warehouse/\u0026#39;); USE CATALOG PAIMON; CALL sys.migrate_table( connector =\u0026gt; \u0026#39;hive\u0026#39;, source_table =\u0026gt; \u0026#39;default.hivetable\u0026#39;, -- You can specify the target table, and if the target table already exists  -- the file will be migrated directly to it  -- target_table =\u0026gt; \u0026#39;default.paimontarget\u0026#39;,  -- You can specify delete_origin is false, this won\u0026#39;t delete hivetable  -- delete_origin =\u0026gt; false,  options =\u0026gt; \u0026#39;file.format=orc\u0026#39;); Flink Action \u0026lt;FLINK_HOME\u0026gt;/flink run ./paimon-flink-action-1.1.1.jar \\ migrate_table \\ --warehouse /path/to/warehouse \\ --catalog_conf uri=thrift://localhost:9083 \\ --catalog_conf metastore=hive \\ --source_type hive \\ --table default.hive_or_paimon  After invoke, \u0026ldquo;hivetable\u0026rdquo; will totally convert to paimon format. Writing and reading the table by old \u0026ldquo;hive way\u0026rdquo; will fail.\nMigrate Hive Database #  Flink SQL CREATE CATALOG PAIMON WITH ( \u0026#39;type\u0026#39;=\u0026#39;paimon\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;hive\u0026#39;, \u0026#39;uri\u0026#39; = \u0026#39;thrift://localhost:9083\u0026#39;, \u0026#39;warehouse\u0026#39;=\u0026#39;/path/to/warehouse/\u0026#39;); USE CATALOG PAIMON; CALL sys.migrate_database( connector =\u0026gt; \u0026#39;hive\u0026#39;, source_database =\u0026gt; \u0026#39;default\u0026#39;, options =\u0026gt; \u0026#39;file.format=orc\u0026#39;); Flink Action \u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ migrate_databse \\ --warehouse \u0026lt;warehouse-path\u0026gt; \\ --source_type hive \\ --database \u0026lt;database\u0026gt; \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\ [--options \u0026lt;paimon-table-conf [,paimon-table-conf ...]\u0026gt; ] Example:\n\u0026lt;FLINK_HOME\u0026gt;/flink run ./paimon-flink-action-1.1.1.jar migrate_table \\ --warehouse /path/to/warehouse \\ --catalog_conf uri=thrift://localhost:9083 \\ --catalog_conf metastore=hive \\ --source_type hive \\ --database default  After invoke, all tables in \u0026ldquo;default\u0026rdquo; database will totally convert to paimon format. Writing and reading the table by old \u0026ldquo;hive way\u0026rdquo; will fail.\n"});index.add({'id':4,'href':'/docs/1.1/migration/migration-from-iceberg/','title':"Migration From Iceberg",'section':"Migration",'content':"Iceberg Migration #  Apache Iceberg data with parquet file format could be migrated to Apache Paimon. When migrating an iceberg table to a paimon table, the origin iceberg table will permanently disappear. So please back up your data if you still need the original table. The migrated paimon table will be an append table.\nWe highly recommend to back up iceberg table data before migrating, because migrating action is not atomic. If been interrupted while migrating, you may lose your data. \nMigrate Iceberg Table #  Currently, we can use paimon catalog with MigrateIcebergTableProcedure or MigrateIcebergTableAction to migrate the data used by latest iceberg snapshot in an iceberg table to a paimon table.\nIceberg tables managed by hadoop-catalog or hive-catalog are supported to be migrated to paimon. As for the type of paimon catalog, it needs to have access to the file system where the iceberg metadata and data files are located. This means we could migrate an iceberg table managed by hadoop-catalog to a paimon table in hive catalog if their warehouses are in the same file system.\nWhen migrating, the iceberg data files which were marked by DELETED will be ignored. Only the data files referenced by manifest entries with \u0026lsquo;EXISTING\u0026rsquo; and \u0026lsquo;ADDED\u0026rsquo; content will be migrated to paimon. Notably, now we don\u0026rsquo;t support migrating iceberg tables with delete files(deletion vectors, position delete files, equality delete files etc.)\nNow only parquet format is supported in iceberg migration.\nMigrateIcebergTableProcedure #  You can run the following command to migrate an iceberg table to a paimon table.\n-- Use named argument CALL sys.migrate_iceberg_table(source_table =\u0026gt; \u0026#39;database_name.table_name\u0026#39;, iceberg_options =\u0026gt; \u0026#39;iceberg_options\u0026#39;, options =\u0026gt; \u0026#39;paimon_options\u0026#39;, parallelism =\u0026gt; parallelism); -- Use indexed argument CALL sys.migrate_iceberg_table(\u0026#39;source_table\u0026#39;,\u0026#39;iceberg_options\u0026#39;, \u0026#39;options\u0026#39;, \u0026#39;parallelism\u0026#39;);  source_table, string type, is used to specify the source iceberg table to migrate, it\u0026rsquo;s required. iceberg_options, string type, is used to specify the configuration of migration, multiple configuration items are separated by commas. it\u0026rsquo;s required. options, string type, is used to specify the additional options for the target paimon table, it\u0026rsquo;s optional. parallelism, integer type, is used to specify the parallelism of the migration job, it\u0026rsquo;s optional.  hadoop-catalog #  To migrate iceberg table managed by hadoop-catalog, you need set metadata.iceberg.storage=hadoop-catalog and iceberg_warehouse. Example:\nCREATE CATALOG paimon_catalog WITH (\u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;/path/to/paimon/warehouse\u0026#39;); USE CATALOG paimon_catalog; CALL sys.migrate_iceberg_table( source_table =\u0026gt; \u0026#39;iceberg_db.iceberg_tbl\u0026#39;, iceberg_options =\u0026gt; \u0026#39;metadata.iceberg.storage=hadoop-catalog,iceberg_warehouse=/path/to/iceberg/warehouse\u0026#39; ); If you want the metadata of the migrated paimon table to be managed by hive, you can also create a hive catalog of paimon for migration. Example:\nCREATE CATALOG paimon_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;hive\u0026#39;, \u0026#39;uri\u0026#39; = \u0026#39;thrift://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;/path/to/paimon/warehouse\u0026#39; ); USE CATALOG paimon_catalog; CALL sys.migrate_iceberg_table( source_table =\u0026gt; \u0026#39;iceberg_db.iceberg_tbl\u0026#39;, iceberg_options =\u0026gt; \u0026#39;metadata.iceberg.storage=hadoop-catalog,iceberg_warehouse=/path/to/iceberg/warehouse\u0026#39; ); hive-catalog #  To migrate iceberg table managed by hive-catalog, you need set metadata.iceberg.storage=hive-catalog and provide information about Hive Metastore used by the iceberg table in iceberg_options.\n  Option Default Type Description     metadata.iceberg.uri none String Hive metastore uri for Iceberg Hive catalog.   metadata.iceberg.hive-conf-dir none String hive-conf-dir for Iceberg Hive catalog.   metadata.iceberg.hadoop-conf-dir none String hadoop-conf-dir for Iceberg Hive catalog.   metadata.iceberg.hive-client-class org.apache.hadoop.hive.metastore.HiveMetaStoreClient String Hive client class name for Iceberg Hive Catalog.    Example:\nCREATE CATALOG paimon_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;hive\u0026#39;, \u0026#39;uri\u0026#39; = \u0026#39;thrift://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;/path/to/paimon/warehouse\u0026#39; ); USE CATALOG paimon_catalog; CALL sys.migrate_iceberg_table( source_table =\u0026gt; \u0026#39;iceberg_db.iceberg_tbl\u0026#39;, iceberg_options =\u0026gt; \u0026#39;metadata.iceberg.storage=hive-catalog,metadata.iceberg.uri=thrift://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;\u0026#39; ); MigrateIcebergTableAction #  You can also use flink action for migration:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ migrate_iceberg_table \\ --table \u0026lt;icebergDatabase.icebergTable\u0026gt; \\ --iceberg_options \u0026lt;iceberg-conf [,iceberg-conf ...]\u0026gt; \\ [--parallelism \u0026lt;parallelism\u0026gt;] \\ [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\ [--options \u0026lt;paimon-table-conf [,paimon-table-conf ...]\u0026gt; ] Example:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-1.1.1.jar \\ migrate_iceberg_table \\ --table iceberg_db.iceberg_tbl \\ --iceberg_options metadata.iceberg.storage=hive-catalog, metadata.iceberg.uri=thrift://localhost:9083 \\ --parallelism 6 \\ --catalog_conf warehouse=/path/to/paimon/warehouse \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://localhost:9083 "});index.add({'id':5,'href':'/docs/1.1/append-table/overview/','title':"Overview",'section':"Table w/o PK",'content':"Overview #  If a table does not have a primary key defined, it is an append table. Compared to the primary key table, it does not have the ability to directly receive changelogs. It cannot be directly updated with data through upsert. It can only receive incoming data from append data.\nFlink CREATE TABLE my_table ( product_id BIGINT, price DOUBLE, sales BIGINT ) WITH ( -- \u0026#39;target-file-size\u0026#39; = \u0026#39;256 MB\u0026#39;,  -- \u0026#39;file.format\u0026#39; = \u0026#39;parquet\u0026#39;,  -- \u0026#39;file.compression\u0026#39; = \u0026#39;zstd\u0026#39;,  -- \u0026#39;file.compression.zstd-level\u0026#39; = \u0026#39;3\u0026#39; );  Batch write and batch read in typical application scenarios, similar to a regular Hive partition table, but compared to the Hive table, it can bring:\n Object storage (S3, OSS) friendly Time Travel and Rollback DELETE / UPDATE with low cost Automatic small file merging in streaming sink Streaming read \u0026amp; write like a queue High performance query with order and index  "});index.add({'id':6,'href':'/docs/1.1/cdc-ingestion/overview/','title':"Overview",'section':"CDC Ingestion",'content':"Overview #  Paimon supports a variety of ways to ingest data into Paimon tables with schema evolution. This means that the added columns are synchronized to the Paimon table in real time and the synchronization job will not be restarted for this purpose.\nWe currently support the following sync ways:\n MySQL Synchronizing Table: synchronize one or multiple tables from MySQL into one Paimon table. MySQL Synchronizing Database: synchronize the whole MySQL database into one Paimon database. Program API Sync: synchronize your custom DataStream input into one Paimon table. Kafka Synchronizing Table: synchronize one Kafka topic\u0026rsquo;s table into one Paimon table. Kafka Synchronizing Database: synchronize one Kafka topic containing multiple tables or multiple topics containing one table each into one Paimon database. MongoDB Synchronizing Collection: synchronize one Collection from MongoDB into one Paimon table. MongoDB Synchronizing Database: synchronize the whole MongoDB database into one Paimon database. Pulsar Synchronizing Table: synchronize one Pulsar topic\u0026rsquo;s table into one Paimon table. Pulsar Synchronizing Database: synchronize one Pulsar topic containing multiple tables or multiple topics containing one table each into one Paimon database.  What is Schema Evolution #  Suppose we have a MySQL table named tableA, it has three fields: field_1, field_2, field_3. When we want to load this MySQL table to Paimon, we can do this in Flink SQL, or use MySqlSyncTableAction.\nFlink SQL:\nIn Flink SQL, if we change the table schema of the MySQL table after the ingestion, the table schema change will not be synchronized to Paimon.\nMySqlSyncTableAction:\nIn MySqlSyncTableAction, if we change the table schema of the MySQL table after the ingestion, the table schema change will be synchronized to Paimon, and the data of field_4 which is newly added will be synchronized to Paimon too.\nSchema Change Evolution #  Cdc Ingestion supports a limited number of schema changes. Currently, the framework can not rename table, drop columns, so the behaviors of RENAME TABLE and DROP COLUMN will be ignored, RENAME COLUMN will add a new column. Currently supported schema changes includes:\n  Adding columns.\n  Altering column types. More specifically,\n altering from a string type (char, varchar, text) to another string type with longer length, altering from a non-string type to string type (char, varchar, text), altering from a binary type (binary, varbinary, blob) to another binary type with longer length, altering from an integer type (tinyint, smallint, int, bigint) to another integer type with wider range, altering from a floating-point type (float, double) to another floating-point type with wider range,  are supported.\n  Computed Functions #  --computed_column are the definitions of computed columns. The argument field is from source table field name.\nTemporal Functions #  Temporal functions can convert date and epoch time to another form. A common use case is to generate partition values.\n  Function Description     year(temporal-column [, precision]) Extract year from the input. Output is an INT value represent the year.   month(temporal-column [, precision]) Extract month of year from the input. Output is an INT value represent the month of year.   day(temporal-column [, precision]) Extract day of month from the input. Output is an INT value represent the day of month.   hour(temporal-column [, precision]) Extract hour from the input. Output is an INT value represent the hour.   minute(temporal-column [, precision]) Extract minute from the input. Output is an INT value represent the minute.   second(temporal-column [, precision]) Extract second from the input. Output is an INT value represent the second.   date_format(temporal-column, format-string [, precision]) Convert the input to desired formatted string. Output type is STRING.   now() Get the timestamp when ingesting the record. Output type is TIMESTAMP_LTZ(3).    The data type of the temporal-column can be one of the following cases:\n DATE, DATETIME or TIMESTAMP. Any integer numeric type (such as INT and BIGINT). In this case, the data will be considered as epoch time of 1970-01-01 00:00:00. You should set precision of the value (default is 0). STRING. In this case, if you didn\u0026rsquo;t set the time unit, the data will be considered as formatted string of DATE, DATETIME or TIMESTAMP value. Otherwise, the data will be considered as string value of epoch time. So you must set time unit in the latter case.  The precision represents the unit of the epoch time. Currently, There are four valid precisions: 0 (for epoch seconds), 3 (for epoch milliseconds), 6(for epoch microseconds) and 9 (for epoch nanoseconds). Take the time point 1970-01-01 00:00:00.123456789 as an example, the epoch seconds are 0, the epoch milliseconds are 123, the epoch microseconds are 123456, and the epoch nanoseconds are 123456789. The precision should match the input values. You can set precision in this way: date_format(epoch_col, yyyy-MM-dd, 0).\ndate_format is a flexible function which is able to convert the temporal value to various formats with different format strings. A most common format string is yyyy-MM-dd HH:mm:ss.SSS. Another example is yyyy-ww which can extract the year and the week-of-the-year from the input. Note that the output is affected by the locale. For example, in some regions the first day of a week is Monday while in others is Sunday, so if you use date_format(date_col, yyyy-ww) and the input of date_col is 2024-01-07 (Sunday), the output maybe 2024-01 (if the first day of a week is Monday) or 2024-02 (if the first day of a week is Sunday).\nOther Functions #    Function Description     substring(column,beginInclusive) Get column.substring(beginInclusive). Output is a STRING.   substring(column,beginInclusive,endExclusive) Get column.substring(beginInclusive,endExclusive). Output is a STRING.   truncate(column,width) truncate column by width. Output type is same with column.If the column is a STRING, truncate(column,width) will truncate the string to width characters, namely `value.substring(0, width)`. If the column is an INT or LONG, truncate(column,width) will truncate the number with the algorithm `v - (((v % W) + W) % W)`. The `redundant` compute part is to keep the result always positive. If the column is a DECIMAL, truncate(column,width) will truncate the decimal with the algorithm: let `scaled_W = decimal(W, scale(v))`, then return `v - (v % scaled_W)`.   cast(value,dataType) Get a constant value. The output is an atomic type, such as STRING, INT, BOOLEAN, etc.    Special Data Type Mapping #   MySQL TINYINT(1) type will be mapped to Boolean by default. If you want to store number (-128~127) in it like MySQL, you can specify type mapping option tinyint1-not-bool (Use --type_mapping), then the column will be mapped to TINYINT in Paimon table. You can use type mapping option to-nullable (Use --type_mapping) to ignore all NOT NULL constraints (except primary keys). You can use type mapping option to-string (Use --type_mapping) to map all MySQL data type to STRING. You can use type mapping option char-to-string (Use --type_mapping) to map MySQL CHAR(length)/VARCHAR(length) types to STRING. You can use type mapping option longtext-to-bytes (Use --type_mapping) to map MySQL LONGTEXT types to BYTES. MySQL BIGINT UNSIGNED, BIGINT UNSIGNED ZEROFILL, SERIAL will be mapped to DECIMAL(20, 0) by default. You can use type mapping option bigint-unsigned-to-bigint (Use --type_mapping) to map these types to Paimon BIGINT, but there is potential data overflow because BIGINT UNSIGNED can store up to 20 digits integer value but Paimon BIGINT can only store up to 19 digits integer value. So you should ensure the overflow won\u0026rsquo;t occur when using this option. MySQL BIT(1) type will be mapped to Boolean. When using Hive catalog, MySQL TIME type will be mapped to STRING. MySQL BINARY will be mapped to Paimon VARBINARY. This is because the binary value is passed as bytes in binlog, so it should be mapped to byte type (BYTES or VARBINARY). We choose VARBINARY because it can retain the length information.  Custom Job Settings #  Checkpointing #  Use -Dexecution.checkpointing.interval=\u0026lt;interval\u0026gt; to enable checkpointing and set interval. For 0.7 and later versions, if you haven\u0026rsquo;t enabled checkpointing, Paimon will enable checkpointing by default and set checkpoint interval to 180 seconds.\nJob Name #  Use -Dpipeline.name=\u0026lt;job-name\u0026gt; to set custom synchronization job name.\ntable configuration #  You can use --table_conf to set table properties and some flink job properties (like sink.parallelism). If the table is created by the cdc job, the table\u0026rsquo;s properties will be equal to the given properties. Otherwise, the job will use the given properties to alter table\u0026rsquo;s properties. But note that immutable options (like merge-engine) and bucket number won\u0026rsquo;t be altered.\n"});index.add({'id':7,'href':'/docs/1.1/concepts/overview/','title':"Overview",'section':"Concepts",'content':"Overview #  Apache Paimon\u0026rsquo;s Architecture:\nAs shown in the architecture above:\nRead/Write: Paimon supports a versatile way to read/write data and perform OLAP queries.\n For reads, it supports consuming data  from historical snapshots (in batch mode), from the latest offset (in streaming mode), or reading incremental snapshots in a hybrid way.   For writes, it supports  streaming synchronization from the changelog of databases (CDC) batch insert/overwrite from offline data.    Ecosystem: In addition to Apache Flink, Paimon also supports read by other computation engines like Apache Hive, Apache Spark and Trino.\nInternal:\n Under the hood, Paimon stores the columnar files on the filesystem/object-store The metadata of the file is saved in the manifest file, providing large-scale storage and data skipping. For primary key table, uses the LSM tree structure to support a large volume of data updates and high-performance queries.  Unified Storage #  For streaming engines like Apache Flink, there are typically three types of connectors:\n Message queue, such as Apache Kafka, it is used in both source and intermediate stages in this pipeline, to guarantee the latency stay within seconds. OLAP system, such as ClickHouse, it receives processed data in streaming fashion and serving user’s ad-hoc queries. Batch storage, such as Apache Hive, it supports various operations of the traditional batch processing, including INSERT OVERWRITE.  Paimon provides table abstraction. It is used in a way that does not differ from the traditional database:\n In batch execution mode, it acts like a Hive table and supports various operations of Batch SQL. Query it to see the latest snapshot. In streaming execution mode, it acts like a message queue. Query it acts like querying a stream changelog from a message queue where historical data never expires.  "});index.add({'id':8,'href':'/docs/1.1/concepts/rest/overview/','title':"Overview",'section':"RESTCatalog",'content':"RESTCatalog #  Overview #  Paimon REST Catalog provides a lightweight implementation to access the catalog service. Paimon could access the catalog service through a catalog server which implements REST API. You can see all APIs in REST API.\nKey Features #   User Defined Technology-Specific Logic Implementation  All technology-specific logic within the catalog server. This ensures that the user can define logic that could be owned by the user.   Decoupled Architecture  The REST Catalog interacts with the catalog server through a well-defined REST API. This decoupling allows for independent evolution and scaling of the catalog server and clients.   Language Agnostic  Developers can implement the catalog server in any programming language, provided that it adheres to the specified REST API. This flexibility enables teams to utilize their existing tech stacks and expertise.   Support for Any Catalog Backend  REST Catalog is designed to work with any catalog backend. As long as they implement the relevant APIs, they can seamlessly integrate with REST Catalog.    Conclusion #  REST Catalog offers adaptable solution for accessing the catalog service. According to REST API is decoupled from the catalog service.\nTechnology-specific Logic is encapsulated on the catalog server. At the same time, the catalog server supports any backend and languages.\nToken Provider #  RESTCatalog supports multiple access authentication methods, including the following:\n Bear Token. DLF Token.  "});index.add({'id':9,'href':'/docs/1.1/concepts/spec/overview/','title':"Overview",'section':"Specification",'content':"Spec Overview #  This is the specification for the Paimon table format, this document standardizes the underlying file structure and design of Paimon.\nTerms #   Schema: fields, primary keys definition, partition keys definition and options. Snapshot: the entrance to all data committed at some specific time point. Manifest list: includes several manifest files. Manifest: includes several data files or changelog files. Data File: contains incremental records. Changelog File: contains records produced by changelog-producer. Global Index: index for a bucket or partition. Data File Index: index for a data file.  Run Flink SQL with Paimon:\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;/your/path\u0026#39; ); USE CATALOG my_catalog; CREATE TABLE my_table ( k INT PRIMARY KEY NOT ENFORCED, f0 INT, f1 STRING ); INSERT INTO my_table VALUES (1, 11, \u0026#39;111\u0026#39;); Take a look to the disk:\nwarehouse └── default.db └── my_table ├── bucket-0 │ └── data-59f60cb9-44af-48cc-b5ad-59e85c663c8f-0.orc ├── index │ └── index-5625e6d9-dd44-403b-a738-2b6ea92e20f1-0 ├── manifest │ ├── index-manifest-5d670043-da25-4265-9a26-e31affc98039-0 │ ├── manifest-6758823b-2010-4d06-aef0-3b1b597723d6-0 │ ├── manifest-list-9f856d52-5b33-4c10-8933-a0eddfaa25bf-0 │ └── manifest-list-9f856d52-5b33-4c10-8933-a0eddfaa25bf-1 ├── schema │ └── schema-0 └── snapshot ├── EARLIEST ├── LATEST └── snapshot-1 "});index.add({'id':10,'href':'/docs/1.1/ecosystem/overview/','title':"Overview",'section':"Ecosystem",'content':"Overview #  Compatibility Matrix #     Engine Version Batch Read Batch Write Create Table Alter Table Streaming Write Streaming Read Batch Overwrite DELETE \u0026amp; UPDATE MERGE INTO Time Travel     Flink 1.15 - 1.20 ✅ ✅ ✅ ✅(1.17+) ✅ ✅ ✅ ✅(1.17+) ❌ ✅   Spark 3.2 - 3.5 ✅ ✅ ✅ ✅ ✅(3.3+) ✅(3.3+) ✅ ✅ ✅ ✅(3.3+)   Hive 2.1 - 3.1 ✅ ✅ ✅ ❌ ❌ ❌ ❌ ❌ ❌ ✅   Trino 420 - 440 ✅ ✅(427+) ✅(427+) ✅(427+) ❌ ❌ ❌ ❌ ❌ ✅   Presto 0.236 - 0.280 ✅ ❌ ✅ ✅ ❌ ❌ ❌ ❌ ❌ ❌   StarRocks 3.1+ ✅ ❌ ❌ ❌ ❌ ❌ ❌ ❌ ❌ ✅   Doris 2.0.6+ ✅ ❌ ❌ ❌ ❌ ❌ ❌ ❌ ❌ ✅    Streaming Engines #  Flink Streaming #  Flink is the most comprehensive streaming computing engine that is widely used for data CDC ingestion and the construction of streaming pipelines.\nRecommended version is Flink 1.17.2.\nSpark Streaming #  You can also use Spark Streaming to build a streaming pipeline. Spark\u0026rsquo;s schema evolution capability will be better implemented, but you must accept the mechanism of mini-batch.\nBatch Engines #  Spark Batch #  Spark Batch is the most widely used batch computing engine.\nRecommended version is Spark 3.4.3.\nFlink Batch #  Flink Batch is also available, which can make your pipeline more integrated with streaming and batch unified.\nOLAP Engines #  StarRocks #  StarRocks is the most recommended OLAP engine with the most advanced integration.\nRecommended version is StarRocks 3.2.6.\nOther OLAP #  You can also use Doris and Trino and Presto, or, you can just use Spark, Flink and Hive to query Paimon tables.\nDownload #  Download Link\n"});index.add({'id':11,'href':'/docs/1.1/primary-key-table/merge-engine/overview/','title':"Overview",'section':"Merge Engine",'content':"Overview #  When Paimon sink receives two or more records with the same primary keys, it will merge them into one record to keep primary keys unique. By specifying the merge-engine table property, users can choose how records are merged together.\nAlways set table.exec.sink.upsert-materialize to NONE in Flink SQL TableConfig, sink upsert-materialize may result in strange behavior. When the input is out of order, we recommend that you use Sequence Field to correct disorder.  Deduplicate #  The deduplicate merge engine is the default merge engine. Paimon will only keep the latest record and throw away other records with the same primary keys.\nSpecifically, if the latest record is a DELETE record, all records with the same primary keys will be deleted. You can config ignore-delete to ignore it.\n"});index.add({'id':12,'href':'/docs/1.1/primary-key-table/overview/','title':"Overview",'section':"Table with PK",'content':"Overview #  If you define a table with primary key, you can insert, update or delete records in the table.\nPrimary keys consist of a set of columns that contain unique values for each record. Paimon enforces data ordering by sorting the primary key within each bucket, allowing users to achieve high performance by applying filtering conditions on the primary key. See CREATE TABLE.\nBucket #  Unpartitioned tables, or partitions in partitioned tables, are sub-divided into buckets, to provide extra structure to the data that may be used for more efficient querying.\nEach bucket directory contains an LSM tree and its changelog files.\nThe range for a bucket is determined by the hash value of one or more columns in the records. Users can specify bucketing columns by providing the bucket-key option. If no bucket-key option is specified, the primary key (if defined) or the complete record will be used as the bucket key.\nA bucket is the smallest storage unit for reads and writes, so the number of buckets limits the maximum processing parallelism. This number should not be too big, though, as it will result in lots of small files and low read performance. In general, the recommended data size in each bucket is about 200MB - 1GB.\nAlso, see rescale bucket if you want to adjust the number of buckets after a table is created.\nLSM Trees #  Paimon adopts the LSM tree (log-structured merge-tree) as the data structure for file storage. This documentation briefly introduces the concepts about LSM trees.\nSorted Runs #  LSM tree organizes files into several sorted runs. A sorted run consists of one or multiple data files and each data file belongs to exactly one sorted run.\nRecords within a data file are sorted by their primary keys. Within a sorted run, ranges of primary keys of data files never overlap.\nAs you can see, different sorted runs may have overlapped primary key ranges, and may even contain the same primary key. When querying the LSM tree, all sorted runs must be combined and all records with the same primary key must be merged according to the user-specified merge engine and the timestamp of each record.\nNew records written into the LSM tree will be first buffered in memory. When the memory buffer is full, all records in memory will be sorted and flushed to disk. A new sorted run is now created.\n"});index.add({'id':13,'href':'/docs/1.1/flink/quick-start/','title':"Quick Start",'section':"Engine Flink",'content':"Quick Start #  This documentation is a guide for using Paimon in Flink.\nJars #  Paimon currently supports Flink 1.20, 1.19, 1.18, 1.17, 1.16, 1.15. We recommend the latest Flink version for a better experience.\nDownload the jar file with corresponding version.\n Currently, paimon provides two types jar: one of which(the bundled jar) is used for read/write data, and the other(action jar) for operations such as manually compaction,    Version Type Jar     Flink 2.0 Bundled Jar paimon-flink-2.0-1.1.1.jar   Flink 1.20 Bundled Jar paimon-flink-1.20-1.1.1.jar   Flink 1.19 Bundled Jar paimon-flink-1.19-1.1.1.jar   Flink 1.18 Bundled Jar paimon-flink-1.18-1.1.1.jar   Flink 1.17 Bundled Jar paimon-flink-1.17-1.1.1.jar   Flink 1.16 Bundled Jar paimon-flink-1.16-1.1.1.jar   Flink 1.15 Bundled Jar paimon-flink-1.15-1.1.1.jar   Flink Action Action Jar paimon-flink-action-1.1.1.jar     You can also manually build bundled jar from the source code.\nTo build from source code, clone the git repository.\nBuild bundled jar with the following command.\n mvn clean install -DskipTests  You can find the bundled jar in ./paimon-flink/paimon-flink-\u0026lt;flink-version\u0026gt;/target/paimon-flink-\u0026lt;flink-version\u0026gt;-1.1.1.jar, and the action jar in ./paimon-flink/paimon-flink-action/target/paimon-flink-action-1.1.1.jar.\nStart #  Step 1: Download Flink\nIf you haven\u0026rsquo;t downloaded Flink, you can download Flink, then extract the archive with the following command.\ntar -xzf flink-*.tgz Step 2: Copy Paimon Bundled Jar\nCopy paimon bundled jar to the lib directory of your Flink home.\ncp paimon-flink-*.jar \u0026lt;FLINK_HOME\u0026gt;/lib/ Step 3: Copy Hadoop Bundled Jar\nIf the machine is in a hadoop environment, please ensure the value of the environment variable HADOOP_CLASSPATH include path to the common Hadoop libraries, you do not need to use the following pre-bundled Hadoop jar.  Download Pre-bundled Hadoop jar and copy the jar file to the lib directory of your Flink home.\ncp flink-shaded-hadoop-2-uber-*.jar \u0026lt;FLINK_HOME\u0026gt;/lib/ Step 4: Start a Flink Local Cluster\nIn order to run multiple Flink jobs at the same time, you need to modify the cluster configuration in \u0026lt;FLINK_HOME\u0026gt;/conf/flink-conf.yaml(Flink version \u0026lt; 1.19) or \u0026lt;FLINK_HOME\u0026gt;/conf/config.yaml(Flink version \u0026gt;= 1.19).\ntaskmanager.numberOfTaskSlots:2To start a local cluster, run the bash script that comes with Flink:\n\u0026lt;FLINK_HOME\u0026gt;/bin/start-cluster.sh You should be able to navigate to the web UI at localhost:8081 to view the Flink dashboard and see that the cluster is up and running.\nYou can now start Flink SQL client to execute SQL scripts.\n\u0026lt;FLINK_HOME\u0026gt;/bin/sql-client.sh Step 5: Create a Catalog and a Table\nCatalog -- if you\u0026#39;re trying out Paimon in a distributed environment, -- the warehouse path should be set to a shared file system, such as HDFS or OSS CREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39;=\u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39;=\u0026#39;file:/tmp/paimon\u0026#39; ); USE CATALOG my_catalog; -- create a word count table CREATE TABLE word_count ( word STRING PRIMARY KEY NOT ENFORCED, cnt BIGINT ); Generic-Catalog Using FlinkGenericCatalog, you need to use Hive metastore. Then, you can use all the tables from Paimon, Hive, and Flink Generic Tables (Kafka and other tables)!\nIn this mode, you should use \u0026lsquo;connector\u0026rsquo; option for creating tables.\nPaimon will use hive.metastore.warehouse.dir in your hive-site.xml, please use path with scheme. For example, hdfs://.... Otherwise, Paimon will use the local path.  CREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39;=\u0026#39;paimon-generic\u0026#39;, \u0026#39;hive-conf-dir\u0026#39;=\u0026#39;...\u0026#39;, \u0026#39;hadoop-conf-dir\u0026#39;=\u0026#39;...\u0026#39; ); USE CATALOG my_catalog; -- create a word count table CREATE TABLE word_count ( word STRING PRIMARY KEY NOT ENFORCED, cnt BIGINT ) WITH ( \u0026#39;connector\u0026#39;=\u0026#39;paimon\u0026#39; );  Step 6: Write Data\n-- create a word data generator table CREATE TEMPORARY TABLE word_table ( word STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;, \u0026#39;fields.word.length\u0026#39; = \u0026#39;1\u0026#39; ); -- paimon requires checkpoint interval in streaming mode SET \u0026#39;execution.checkpointing.interval\u0026#39; = \u0026#39;10 s\u0026#39;; -- write streaming data to dynamic table INSERT INTO word_count SELECT word, COUNT(*) FROM word_table GROUP BY word; Step 7: OLAP Query\n-- use tableau result mode SET \u0026#39;sql-client.execution.result-mode\u0026#39; = \u0026#39;tableau\u0026#39;; -- switch to batch mode RESET \u0026#39;execution.checkpointing.interval\u0026#39;; SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;batch\u0026#39;; -- olap query the table SELECT * FROM word_count; You can execute the query multiple times and observe the changes in the results.\nStep 8: Streaming Query\n-- switch to streaming mode SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;streaming\u0026#39;; -- track the changes of table and calculate the count interval statistics SELECT `interval`, COUNT(*) AS interval_cnt FROM (SELECT cnt / 10000 AS `interval` FROM word_count) GROUP BY `interval`; Step 9: Exit\nCancel streaming job in localhost:8081, then execute the following SQL script to exit Flink SQL client.\n-- uncomment the following line if you want to drop the dynamic table and clear the files -- DROP TABLE word_count;  -- exit sql-client EXIT; Stop the Flink local cluster.\n./bin/stop-cluster.sh Use Flink Managed Memory #  Paimon tasks can create memory pools based on executor memory which will be managed by Flink executor, such as managed memory in Flink task manager. It will improve the stability and performance of sinks by managing writer buffers for multiple tasks through executor.\nThe following properties can be set if using Flink managed memory:\n   Option Default Description     sink.use-managed-memory-allocator false If true, flink sink will use managed memory for merge tree; otherwise, it will create an independent memory allocator, which means each task allocates and manages its own memory pool (heap memory), if there are too many tasks in one Executor, it may cause performance issues and even OOM.   sink.managed.writer-buffer-memory 256M Weight of writer buffer in managed memory, Flink will compute the memory size, for writer according to the weight, the actual memory used depends on the running environment. Now the memory size defined in this property are equals to the exact memory allocated to write buffer in runtime.    Use In SQL Users can set memory weight in SQL for Flink Managed Memory, then Flink sink operator will get the memory pool size and create allocator for Paimon writer.\nINSERT INTO paimon_table /*+ OPTIONS(\u0026#39;sink.use-managed-memory-allocator\u0026#39;=\u0026#39;true\u0026#39;, \u0026#39;sink.managed.writer-buffer-memory\u0026#39;=\u0026#39;256M\u0026#39;) */ SELECT * FROM ....; Setting dynamic options #  When interacting with the Paimon table, table options can be tuned without changing the options in the catalog. Paimon will extract job-level dynamic options and take effect in the current session. The dynamic table option\u0026rsquo;s key format is paimon.${catalogName}.${dbName}.${tableName}.${config_key}. The catalogName/dbName/tableName can be *, which means matching all the specific parts. The dynamic global option\u0026rsquo;s key format is ${config_key}. Global options will take effect for all the tables. Table options will override global options if there are conflicts.\nFor example:\n-- set scan.timestamp-millis=1697018249001 for all tables SET \u0026#39;scan.timestamp-millis\u0026#39; = \u0026#39;1697018249001\u0026#39;; SELECT * FROM T; -- set scan.timestamp-millis=1697018249000 for the table mycatalog.default.T SET \u0026#39;paimon.mycatalog.default.T.scan.timestamp-millis\u0026#39; = \u0026#39;1697018249000\u0026#39;; SELECT * FROM T; -- set scan.timestamp-millis=1697018249000 for the table default.T in any catalog SET \u0026#39;paimon.*.default.T.scan.timestamp-millis\u0026#39; = \u0026#39;1697018249000\u0026#39;; SELECT * FROM T; -- set scan.timestamp-millis=1697018249000 for the table mycatalog.default.T1 -- set scan.timestamp-millis=1697018249001 for others tables SET \u0026#39;paimon.mycatalog.default.T1.scan.timestamp-millis\u0026#39; = \u0026#39;1697018249000\u0026#39;; SET \u0026#39;scan.timestamp-millis\u0026#39; = \u0026#39;1697018249001\u0026#39;; SELECT * FROM T1 JOIN T2 ON xxxx; "});index.add({'id':14,'href':'/docs/1.1/spark/quick-start/','title':"Quick Start",'section':"Engine Spark",'content':"Quick Start #  Preparation #  Paimon currently supports Spark 3.5, 3.4, 3.3, and 3.2. We recommend the latest Spark version for a better experience.\nDownload the jar file with corresponding version.\n   Version Jar     Spark 3.5 paimon-spark-3.5-1.1.1.jar   Spark 3.4 paimon-spark-3.4-1.1.1.jar   Spark 3.3 paimon-spark-3.3-1.1.1.jar   Spark 3.2 paimon-spark-3.2-1.1.1.jar    You can also manually build bundled jar from the source code.\nTo build from source code, clone the git repository.\nBuild bundled jar with the following command.\nmvn clean install -DskipTests For Spark 3.3, you can find the bundled jar in ./paimon-spark/paimon-spark-3.3/target/paimon-spark-3.3-1.1.1.jar.\nSetup #  If you are using HDFS, make sure that the environment variable HADOOP_HOME or HADOOP_CONF_DIR is set.  Step 1: Specify Paimon Jar File\nAppend path to paimon jar file to the --jars argument when starting spark-sql.\nspark-sql ... --jars /path/to/paimon-spark-3.3-1.1.1.jar OR use the --packages option.\nspark-sql ... --packages org.apache.paimon:paimon-spark-3.3:1.1.1 Alternatively, you can copy paimon-spark-3.3-1.1.1.jar under spark/jars in your Spark installation directory.\nStep 2: Specify Paimon Catalog\nCatalog When starting spark-sql, use the following command to register Paimon’s Spark catalog with the name paimon. Table files of the warehouse is stored under /tmp/paimon.\nspark-sql ... \\  --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\  --conf spark.sql.catalog.paimon.warehouse=file:/tmp/paimon \\  --conf spark.sql.extensions=org.apache.paimon.spark.extensions.PaimonSparkSessionExtensions Catalogs are configured using properties under spark.sql.catalog.(catalog_name). In above case, \u0026lsquo;paimon\u0026rsquo; is the catalog name, you can change it to your own favorite catalog name.\nAfter spark-sql command line has started, run the following SQL to create and switch to database default.\nUSE paimon; USE default; After switching to the catalog ('USE paimon'), Spark\u0026rsquo;s existing tables will not be directly accessible, you can use the spark_catalog.${database_name}.${table_name} to access Spark tables.\nGeneric Catalog When starting spark-sql, use the following command to register Paimon’s Spark Generic catalog to replace Spark default catalog spark_catalog. (default warehouse is Spark spark.sql.warehouse.dir)\nCurrently, it is only recommended to use SparkGenericCatalog in the case of Hive metastore, Paimon will infer Hive conf from Spark session, you just need to configure Spark\u0026rsquo;s Hive conf.\nspark-sql ... \\  --conf spark.sql.catalog.spark_catalog=org.apache.paimon.spark.SparkGenericCatalog \\  --conf spark.sql.extensions=org.apache.paimon.spark.extensions.PaimonSparkSessionExtensions Using SparkGenericCatalog, you can use Paimon tables in this Catalog or non-Paimon tables such as Spark\u0026rsquo;s csv, parquet, Hive tables, etc.\n Create Table #  Catalog create table my_table ( k int, v string ) tblproperties ( \u0026#39;primary-key\u0026#39; = \u0026#39;k\u0026#39; ); Generic Catalog create table my_table ( k int, v string ) USING paimon tblproperties ( \u0026#39;primary-key\u0026#39; = \u0026#39;k\u0026#39; );  Insert Table #  SQL INSERT INTO my_table VALUES (1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;); DataFrame -- you can use Seq((1, \u0026#34;Hi\u0026#34;), (2, \u0026#34;Hello\u0026#34;)).toDF(\u0026#34;k\u0026#34;, \u0026#34;v\u0026#34;) .write.format(\u0026#34;paimon\u0026#34;).mode(\u0026#34;append\u0026#34;).saveAsTable(\u0026#34;my_table\u0026#34;) -- or Seq((1, \u0026#34;Hi\u0026#34;), (2, \u0026#34;Hello\u0026#34;)).toDF(\u0026#34;k\u0026#34;, \u0026#34;v\u0026#34;) .write.format(\u0026#34;paimon\u0026#34;).mode(\u0026#34;append\u0026#34;).save(\u0026#34;file:/tmp/paimon/default.db/my_table\u0026#34;)  Query Table #  SQL SELECT * FROM my_table; /* 1\tHi 2\tHello */ DataFrame -- you can use spark.read.format(\u0026#34;paimon\u0026#34;).table(\u0026#34;my_table\u0026#34;).show() -- or spark.read.format(\u0026#34;paimon\u0026#34;).load(\u0026#34;file:/tmp/paimon/default.db/my_table\u0026#34;).show() /* +---+------+ | k | v| +---+------+ | 1| Hi| | 2| Hello| +---+------+ */  Spark Type Conversion #  This section lists all supported type conversion between Spark and Paimon. All Spark\u0026rsquo;s data types are available in package org.apache.spark.sql.types.\n  Spark Data Type Paimon Data Type Atomic Type     StructType RowType false   MapType MapType false   ArrayType ArrayType false   BooleanType BooleanType true   ByteType TinyIntType true   ShortType SmallIntType true   IntegerType IntType true   LongType BigIntType true   FloatType FloatType true   DoubleType DoubleType true   StringType VarCharType(Integer.MAX_VALUE) true   VarCharType(length) VarCharType(length) true   CharType(length) CharType(length) true   DateType DateType true   TimestampType LocalZonedTimestamp true   TimestampNTZType(Spark3.4+) TimestampType true   DecimalType(precision, scale) DecimalType(precision, scale) true   BinaryType VarBinaryType, BinaryType true    Due to the previous design, in Spark3.3 and below, Paimon will map both Paimon\u0026rsquo;s TimestampType and LocalZonedTimestamp to Spark\u0026rsquo;s TimestampType, and only correctly handle with TimestampType.\nTherefore, when using Spark3.3 and below, reads Paimon table with LocalZonedTimestamp type written by other engines, such as Flink, the query result of LocalZonedTimestamp type will have time zone offset, which needs to be adjusted manually.\nWhen using Spark3.4 and above, all timestamp types can be parsed correctly.\n "});index.add({'id':15,'href':'/docs/1.1/project/roadmap/','title':"Roadmap",'section':"Project",'content':"Roadmap #  Flink Lookup Join #  Support Flink Custom Data Distribution Lookup Join to reach large-scale data lookup join.\nProduce Iceberg snapshots #  Introduce a mode to produce Iceberg snapshots.\nVariant Type #  Support Variant Type with Spark 4.0 and Flink 2.0. Unlocking support for semi-structured data.\nFile Index #  Add more index:\n Inverse  Vector Compaction #  Support Vector Compaction for super Wide Table.\nFunction support #  Paimon Catalog supports functions.\nFiles Schema Evolution Ingestion #  Introduce a files Ingestion with Schema Evolution.\n"});index.add({'id':16,'href':'/docs/1.1/learn-paimon/understand-files/','title':"Understand Files",'section':"Learn Paimon",'content':"Understand Files #  This article is specifically designed to clarify the impact that various file operations have on files.\nThis page provides concrete examples and practical tips for effectively managing them. Furthermore, through an in-depth exploration of operations such as commit and compact, we aim to offer insights into the creation and updates of files.\nPrerequisite #  Before delving further into this page, please ensure that you have read through the following sections:\n Basic Concepts, Primary Key Table and Append Table How to use Paimon in Flink.  Understand File Operations #  Create Catalog #  Start Flink SQL client via ./sql-client.sh and execute the following statements one by one to create a Paimon catalog.\nCREATE CATALOG paimon WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;file:///tmp/paimon\u0026#39; ); USE CATALOG paimon; This will only create a directory at given path file:///tmp/paimon.\nCreate Table #  Execute the following create table statement will create a Paimon table with 3 fields:\nCREATE TABLE T ( id BIGINT, a INT, b STRING, dt STRING COMMENT \u0026#39;timestamp string in format yyyyMMdd\u0026#39;, PRIMARY KEY(id, dt) NOT ENFORCED ) PARTITIONED BY (dt); This will create Paimon table T under the path /tmp/paimon/default.db/T, with its schema stored in /tmp/paimon/default.db/T/schema/schema-0\nInsert Records Into Table #  Run the following insert statement in Flink SQL:\nINSERT INTO T VALUES (1, 10001, \u0026#39;varchar00001\u0026#39;, \u0026#39;20230501\u0026#39;); Once the Flink job is completed, the records are written to the Paimon table through a successful commit. Users can verify the visibility of these records by executing the query SELECT * FROM T which will return a single row. The commit process creates a snapshot located at the path /tmp/paimon/default.db/T/snapshot/snapshot-1. The resulting file layout at snapshot-1 is as described below:\nThe content of snapshot-1 contains metadata of the snapshot, such as manifest list and schema id:\n{ \u0026#34;version\u0026#34; : 3, \u0026#34;id\u0026#34; : 1, \u0026#34;schemaId\u0026#34; : 0, \u0026#34;baseManifestList\u0026#34; : \u0026#34;manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-0\u0026#34;, \u0026#34;deltaManifestList\u0026#34; : \u0026#34;manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-1\u0026#34;, \u0026#34;changelogManifestList\u0026#34; : null, \u0026#34;commitUser\u0026#34; : \u0026#34;7d758485-981d-4b1a-a0c6-d34c3eb254bf\u0026#34;, \u0026#34;commitIdentifier\u0026#34; : 9223372036854775807, \u0026#34;commitKind\u0026#34; : \u0026#34;APPEND\u0026#34;, \u0026#34;timeMillis\u0026#34; : 1684155393354, \u0026#34;logOffsets\u0026#34; : { }, \u0026#34;totalRecordCount\u0026#34; : 1, \u0026#34;deltaRecordCount\u0026#34; : 1, \u0026#34;changelogRecordCount\u0026#34; : 0, \u0026#34;watermark\u0026#34; : -9223372036854775808 } Remind that a manifest list contains all changes of the snapshot, baseManifestList is the base file upon which the changes in deltaManifestList is applied. The first commit will result in 1 manifest file, and 2 manifest lists are created (the file names might differ from those in your experiment):\n./T/manifest: manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-1\tmanifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-0 manifest-2b833ea4-d7dc-4de0-ae0d-ad76eced75cc-0 manifest-2b833ea4-d7dc-4de0-ae0d-ad76eced75cc-0 is the manifest file (manifest-1-0 in the above graph), which stores the information about the data files in the snapshot.\nmanifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-0 is the baseManifestList (manifest-list-1-base in the above graph), which is effectively empty.\nmanifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-1 is the deltaManifestList (manifest-list-1-delta in the above graph), which contains a list of manifest entries that perform operations on data files, which, in this case, is manifest-1-0.\nNow let\u0026rsquo;s insert a batch of records across different partitions and see what happens. In Flink SQL, execute the following statement:\nINSERT INTO T VALUES (2, 10002, \u0026#39;varchar00002\u0026#39;, \u0026#39;20230502\u0026#39;), (3, 10003, \u0026#39;varchar00003\u0026#39;, \u0026#39;20230503\u0026#39;), (4, 10004, \u0026#39;varchar00004\u0026#39;, \u0026#39;20230504\u0026#39;), (5, 10005, \u0026#39;varchar00005\u0026#39;, \u0026#39;20230505\u0026#39;), (6, 10006, \u0026#39;varchar00006\u0026#39;, \u0026#39;20230506\u0026#39;), (7, 10007, \u0026#39;varchar00007\u0026#39;, \u0026#39;20230507\u0026#39;), (8, 10008, \u0026#39;varchar00008\u0026#39;, \u0026#39;20230508\u0026#39;), (9, 10009, \u0026#39;varchar00009\u0026#39;, \u0026#39;20230509\u0026#39;), (10, 10010, \u0026#39;varchar00010\u0026#39;, \u0026#39;20230510\u0026#39;); The second commit takes place and executing SELECT * FROM T will return 10 rows. A new snapshot, namely snapshot-2, is created and gives us the following physical file layout:\n% ls -1tR . ./T: dt=20230501 dt=20230502\tdt=20230503\tdt=20230504\tdt=20230505\tdt=20230506\tdt=20230507\tdt=20230508\tdt=20230509\tdt=20230510\tsnapshot schema manifest ./T/snapshot: LATEST snapshot-2 EARLIEST snapshot-1 ./T/manifest: manifest-list-9ac2-5e79-4978-a3bc-86c25f1a303f-1 # delta manifest list for snapshot-2 manifest-list-9ac2-5e79-4978-a3bc-86c25f1a303f-0 # base manifest list for snapshot-2\t manifest-f1267033-e246-4470-a54c-5c27fdbdd074-0\t# manifest file for snapshot-2 manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-1 # delta manifest list for snapshot-1  manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-0 # base manifest list for snapshot-1 manifest-2b833ea4-d7dc-4de0-ae0d-ad76eced75cc-0 # manifest file for snapshot-1 ./T/dt=20230501/bucket-0: data-b75b7381-7c8b-430f-b7e5-a204cb65843c-0.orc ... # each partition has the data written to bucket-0 ... ./T/schema: schema-0 The new file layout as of snapshot-2 looks like Delete Records From Table #  Now let\u0026rsquo;s delete records that meet the condition dt\u0026gt;=20230503. In Flink SQL, execute the following statement:\nBatch DELETE FROM T WHERE dt \u0026gt;= \u0026#39;20230503\u0026#39;; The third commit takes place and it gives us snapshot-3. Now, listing the files under the table and your will find out no partition is dropped. Instead, a new data file is created for partition 20230503 to 20230510:\n./T/dt=20230510/bucket-0: data-b93f468c-b56f-4a93-adc4-b250b3aa3462-0.orc # newer data file created by the delete statement  data-0fcacc70-a0cb-4976-8c88-73e92769a762-0.orc # older data file created by the insert statement This make sense since we insert a record in the second commit (represented by +I[10, 10010, 'varchar00010', '20230510']) and then delete the record in the third commit. Executing SELECT * FROM T will return 2 rows, namely:\n+I[1, 10001, 'varchar00001', '20230501'] +I[2, 10002, 'varchar00002', '20230502'] The new file layout as of snapshot-3 looks like Note that manifest-3-0 contains 8 manifest entries of ADD operation type, corresponding to 8 newly written data files.\nCompact Table #  As you may have noticed, the number of small files will augment over successive snapshots, which may lead to decreased read performance. Therefore, a full-compaction is needed in order to reduce the number of small files.\nLet\u0026rsquo;s trigger the full-compaction now, and run a dedicated compaction job through flink run:\nBatch Flink SQL CALL sys.compact( `table` =\u0026gt; \u0026#39;database_name.table_name\u0026#39;, partitions =\u0026gt; \u0026#39;partition_name\u0026#39;, order_strategy =\u0026gt; \u0026#39;order_strategy\u0026#39;, order_by =\u0026gt; \u0026#39;order_by\u0026#39;, options =\u0026gt; \u0026#39;paimon_table_dynamic_conf\u0026#39; ); Flink Action \u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  -D execution.runtime-mode=batch \\  /path/to/paimon-flink-action-1.1.1.jar \\  compact \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  [--partition \u0026lt;partition-name\u0026gt;] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table_conf \u0026lt;paimon-table-dynamic-conf\u0026gt; [--table_conf \u0026lt;paimon-table-dynamic-conf\u0026gt;] ...]  an example would be (suppose you\u0026rsquo;re already in Flink home)\nFlink SQL CALL sys.compact(\u0026#39;T\u0026#39;); Flink Action ./bin/flink run \\  ./lib/paimon-flink-action-1.1.1.jar \\  compact \\  --path file:///tmp/paimon/default.db/T  All current table files will be compacted and a new snapshot, namely snapshot-4, is made and contains the following information:\n{ \u0026#34;version\u0026#34; : 3, \u0026#34;id\u0026#34; : 4, \u0026#34;schemaId\u0026#34; : 0, \u0026#34;baseManifestList\u0026#34; : \u0026#34;manifest-list-9be16-82e7-4941-8b0a-7ce1c1d0fa6d-0\u0026#34;, \u0026#34;deltaManifestList\u0026#34; : \u0026#34;manifest-list-9be16-82e7-4941-8b0a-7ce1c1d0fa6d-1\u0026#34;, \u0026#34;changelogManifestList\u0026#34; : null, \u0026#34;commitUser\u0026#34; : \u0026#34;a3d951d5-aa0e-4071-a5d4-4c72a4233d48\u0026#34;, \u0026#34;commitIdentifier\u0026#34; : 9223372036854775807, \u0026#34;commitKind\u0026#34; : \u0026#34;COMPACT\u0026#34;, \u0026#34;timeMillis\u0026#34; : 1684163217960, \u0026#34;logOffsets\u0026#34; : { }, \u0026#34;totalRecordCount\u0026#34; : 2, \u0026#34;deltaRecordCount\u0026#34; : -16, \u0026#34;changelogRecordCount\u0026#34; : 0, \u0026#34;watermark\u0026#34; : -9223372036854775808 } The new file layout as of snapshot-4 looks like Note that manifest-4-0 contains 20 manifest entries (18 DELETE operations and 2 ADD operations)\n For partition 20230503 to 20230510, two DELETE operations for two data files For partition 20230501 to 20230502, one DELETE operation and one ADD operation for the same data file. This is because there has been an upgrade of the file from level 0 to the highest level. Please rest assured that this is only a change in metadata, and the file is still the same.  Alter Table #  Execute the following statement to configure full-compaction:\nALTER TABLE T SET (\u0026#39;full-compaction.delta-commits\u0026#39; = \u0026#39;1\u0026#39;); It will create a new schema for Paimon table, namely schema-1, but no snapshot has actually used this schema yet until the next commit.\nExpire Snapshots #  Remind that the marked data files are not truly deleted until the snapshot expires and no consumer depends on the snapshot. For more information, see Expiring Snapshots.\nDuring the process of snapshot expiration, the range of snapshots is initially determined, and then data files within these snapshots are marked for deletion. A data file is marked for deletion only when there is a manifest entry of kind DELETE that references that specific data file. This marking ensures that the file will not be utilized by subsequent snapshots and can be safely removed.\nLet\u0026rsquo;s say all 4 snapshots in the above diagram are about to expire. The expire process is as follows:\n  It first deletes all marked data files, and records any changed buckets.\n  It then deletes any changelog files and associated manifests.\n  Finally, it deletes the snapshots themselves and writes the earliest hint file.\n  If any directories are left empty after the deletion process, they will be deleted as well.\nLet\u0026rsquo;s say another snapshot, snapshot-5 is created and snapshot expiration is triggered. snapshot-1 to snapshot-4 are\nto be deleted. For simplicity, we will only focus on files from previous snapshots, the final layout after snapshot expiration looks like:\nAs a result, partition 20230503 to 20230510 are physically deleted.\nFlink Stream Write #  Finally, we will examine Flink Stream Write by utilizing the example of CDC ingestion. This section will address the capturing and writing of change data into Paimon, as well as the mechanisms behind asynchronous compact and snapshot commit and expiration.\nTo begin, let\u0026rsquo;s take a closer look at the CDC ingestion workflow and the unique roles played by each component involved.\n MySQL CDC Source uniformly reads snapshot and incremental data, with SnapshotReader reading snapshot data and BinlogReader reading incremental data, respectively. Paimon Sink writes data into Paimon table in bucket level. The CompactManager within it will trigger compaction asynchronously. Committer Operator is a singleton responsible for committing and expiring snapshots.  Next, we will go over end-to-end data flow.\nMySQL Cdc Source read snapshot and incremental data and emit them to downstream after normalization.\nPaimon Sink first buffers new records in a heap-based LSM tree, and flushes them to disk when the memory buffer is full. Note that each data file written is a sorted run. At this point, no manifest file and snapshot is created. Right before Flink checkpoint takes places, Paimon Sink will flush all buffered records and send committable message to downstream, which is read and committed by Committer Operator during checkpoint.\nDuring checkpoint, Committer Operator will create a new snapshot and associate it with manifest lists so that the snapshot\ncontains information about all data files in the table.\nAt later point asynchronous compaction might take place, and the committable produced by CompactManager contains information about previous files and merged files so that Committer Operator can construct corresponding manifest entries. In this case Committer Operator might produce two snapshot during Flink checkpoint, one for data written (snapshot of kind Append) and the other for compact (snapshot of kind Compact). If no data file is written during checkpoint interval, only snapshot of kind Compact will be created. Committer Operator will check against snapshot expiration and perform physical deletion of marked data files.\nUnderstand Small Files #  Many users are concerned about small files, which can lead to:\n Stability issue: Too many small files in HDFS, NameNode will be overstressed. Cost issue: A small file in HDFS will temporarily use the size of a minimum of one Block, for example 128 MB. Query efficiency: The efficiency of querying too many small files will be affected.  Understand Checkpoints #  Assuming you are using Flink Writer, each checkpoint generates 1-2 snapshots, and the checkpoint forces the files to be generated on DFS, so the smaller the checkpoint interval the more small files will be generated.\n So first thing is increase checkpoint interval.  By default, not only checkpoint will cause the file to be generated, but writer\u0026rsquo;s memory (write-buffer-size) exhaustion will also flush data to DFS and generate the corresponding file. You can enable write-buffer-spillable to generate spilled files in writer to generate bigger files in DFS.\nSo second thing is increase write-buffer-size or enable write-buffer-spillable.  Understand Snapshots #  Paimon maintains multiple versions of files, compaction and deletion of files are logical and do not actually delete files. Files are only really deleted when Snapshot is expired, so the first way to reduce files is to reduce the time it takes for snapshot to be expired. Flink writer will automatically expire snapshots.\nSee Expire Snapshots.\nUnderstand Partitions and Buckets #  Paimon files are organized in a layered style. The following image illustrates the file layout. Starting from a snapshot file, Paimon readers can recursively access all records from the table.\nFor example, the following table:\nCREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ) PARTITIONED BY (dt, hh) WITH ( \u0026#39;bucket\u0026#39; = \u0026#39;10\u0026#39; ); The table data will be physically sliced into different partitions, and different buckets inside, so if the overall data volume is too small, there is at least one file in a single bucket, I suggest you configure a smaller number of buckets, otherwise there will be quite a few small files as well.\nUnderstand LSM for Primary Table #  LSM tree organizes files into several sorted runs. A sorted run consists of one or multiple data files and each data file belongs to exactly one sorted run.\nBy default, sorted runs number depends on num-sorted-run.compaction-trigger, see Compaction for Primary Key Table, this means that there are at least 5 files in a bucket. If you want to reduce this number, you can keep fewer files, but write performance may suffer.\nUnderstand Files for Bucketed Append Table #  By default, Append also does automatic compaction to reduce the number of small files.\nHowever, for Bucketed Append table, it will only compact the files within the Bucket for sequential purposes, which may keep more small files. See Bucketed Append.\nUnderstand Full-Compaction #  Maybe you think the 5 files for the primary key table are actually okay, but the Append table (bucket) may have 50 small files in a single bucket, which is very difficult to accept. Worse still, partitions that are no longer active also keep so many small files.\nConfigure ‘full-compaction.delta-commits’ perform full-compaction periodically in Flink writing. And it can ensure that partitions are full compacted before writing ends.\n"});index.add({'id':17,'href':'/docs/1.1/migration/upsert-to-partitioned/','title':"Upsert To Partitioned",'section':"Migration",'content':"Upsert To Partitioned #  Note: Only Hive Engine can be used to query these upsert-to-partitioned tables.  The Tag Management will maintain the manifests and data files of the snapshot. A typical usage is creating tags daily, then you can maintain the historical data of each day for batch reading.\nWhen using primary key tables, a non-partitioned approach is often used to maintain updates, in order to mirror and synchronize tables from upstream database tables. This allows users to query the latest data. The tradition of Hive data warehouses is not like this. Offline data warehouses require an immutable view every day to ensure the idempotence of calculations. So we created a Tag mechanism to output these views.\nHowever, the traditional use of Hive data warehouses is more accustomed to using partitions to specify the query\u0026rsquo;s Tag, and is more accustomed to using Hive computing engines.\nSo, we introduce 'metastore.tag-to-partition' and 'metastore.tag-to-partition.preview' to mapping a non-partitioned primary key table to the partition table in Hive metastore, and mapping the partition field to the name of the Tag to be fully compatible with Hive.\nExample for Tag to Partition #  Step 1: Create table and tag in Flink SQL\nFlink CREATE CATALOG my_hive WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;hive\u0026#39;, -- \u0026#39;uri\u0026#39; = \u0026#39;thrift://\u0026lt;hive-metastore-host-name\u0026gt;:\u0026lt;port\u0026gt;\u0026#39;, default use \u0026#39;hive.metastore.uris\u0026#39; in HiveConf  -- \u0026#39;hive-conf-dir\u0026#39; = \u0026#39;...\u0026#39;, this is recommended in the kerberos environment  -- \u0026#39;hadoop-conf-dir\u0026#39; = \u0026#39;...\u0026#39;, this is recommended in the kerberos environment  -- \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs:///path/to/table/store/warehouse\u0026#39;, default use \u0026#39;hive.metastore.warehouse.dir\u0026#39; in HiveConf ); USE CATALOG my_hive; CREATE TABLE mydb.t ( pk INT, col1 STRING, col2 STRING ) WITH ( \u0026#39;bucket\u0026#39; = \u0026#39;-1\u0026#39;, \u0026#39;metastore.tag-to-partition\u0026#39; = \u0026#39;dt\u0026#39; ); INSERT INTO t VALUES (1, \u0026#39;10\u0026#39;, \u0026#39;100\u0026#39;), (2, \u0026#39;20\u0026#39;, \u0026#39;200\u0026#39;); -- create tag \u0026#39;2023-10-16\u0026#39; for snapshot 1 CALL sys.create_tag(\u0026#39;mydb.t\u0026#39;, \u0026#39;2023-10-16\u0026#39;, 1);  Step 2: Query table in Hive with Partition Pruning\nHive SHOW PARTITIONS t; /* OK dt=2023-10-16 */ SELECT * FROM t WHERE dt=\u0026#39;2023-10-16\u0026#39;; /* OK 1 10 100 2023-10-16 2 20 200 2023-10-16 */  Example for Tag Preview #  The above example can only query tags that have already been created, but Paimon is a real-time data lake, and you also need to query the latest data. Therefore, Paimon provides a preview feature:\nStep 1: Create table and tag in Flink SQL\nFlink CREATE CATALOG my_hive WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;hive\u0026#39;, -- \u0026#39;uri\u0026#39; = \u0026#39;thrift://\u0026lt;hive-metastore-host-name\u0026gt;:\u0026lt;port\u0026gt;\u0026#39;, default use \u0026#39;hive.metastore.uris\u0026#39; in HiveConf  -- \u0026#39;hive-conf-dir\u0026#39; = \u0026#39;...\u0026#39;, this is recommended in the kerberos environment  -- \u0026#39;hadoop-conf-dir\u0026#39; = \u0026#39;...\u0026#39;, this is recommended in the kerberos environment  -- \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs:///path/to/table/store/warehouse\u0026#39;, default use \u0026#39;hive.metastore.warehouse.dir\u0026#39; in HiveConf ); USE CATALOG my_hive; CREATE TABLE mydb.t ( pk INT, col1 STRING, col2 STRING ) WITH ( \u0026#39;bucket\u0026#39; = \u0026#39;-1\u0026#39;, \u0026#39;metastore.tag-to-partition\u0026#39; = \u0026#39;dt\u0026#39;, -- preview tag creation mode process-time  -- paimon will create partitions early based on process-time  \u0026#39;metastore.tag-to-partition.preview\u0026#39; = \u0026#39;process-time\u0026#39; ); INSERT INTO t VALUES (1, \u0026#39;10\u0026#39;, \u0026#39;100\u0026#39;), (2, \u0026#39;20\u0026#39;, \u0026#39;200\u0026#39;); -- create tag \u0026#39;2023-10-16\u0026#39; for snapshot 1 CALL sys.create_tag(\u0026#39;mydb.t\u0026#39;, \u0026#39;2023-10-16\u0026#39;, 1); -- new data in \u0026#39;2023-10-17\u0026#39; INSERT INTO t VALUES (3, \u0026#39;30\u0026#39;, \u0026#39;300\u0026#39;), (4, \u0026#39;40\u0026#39;, \u0026#39;400\u0026#39;); -- haven\u0026#39;t finished writing the data for \u0026#39;2023-10-17\u0026#39; yet, so there\u0026#39;s no need to create a tag for now -- but the data is already visible for Hive  Step 2: Query table in Hive with Partition Pruning\nHive SHOW PARTITIONS t; /* OK dt=2023-10-16 dt=2023-10-17 */ SELECT * FROM t WHERE dt=\u0026#39;2023-10-17\u0026#39;; -- preview tag \u0026#39;2023-10-17\u0026#39; /* OK 1 10 100 2023-10-17 2 20 200 2023-10-17 3 30 300 2023-10-17 4 40 400 2023-10-17 */  "});index.add({'id':18,'href':'/docs/1.1/concepts/basic-concepts/','title':"Basic Concepts",'section':"Concepts",'content':"Basic Concepts #  File Layouts #  All files of a table are stored under one base directory. Paimon files are organized in a layered style. The following image illustrates the file layout. Starting from a snapshot file, Paimon readers can recursively access all records from the table.\nSnapshot #  All snapshot files are stored in the snapshot directory.\nA snapshot file is a JSON file containing information about this snapshot, including\n the schema file in use the manifest list containing all changes of this snapshot  A snapshot captures the state of a table at some point in time. Users can access the latest data of a table through the latest snapshot. By time traveling, users can also access the previous state of a table through an earlier snapshot.\nManifest Files #  All manifest lists and manifest files are stored in the manifest directory.\nA manifest list is a list of manifest file names.\nA manifest file is a file containing changes about LSM data files and changelog files. For example, which LSM data file is created and which file is deleted in the corresponding snapshot.\nData Files #  Data files are grouped by partitions. Currently, Paimon supports using parquet (default), orc and avro as data file\u0026rsquo;s format.\nPartition #  Paimon adopts the same partitioning concept as Apache Hive to separate data.\nPartitioning is an optional way of dividing a table into related parts based on the values of particular columns like date, city, and department. Each table can have one or more partition keys to identify a particular partition.\nBy partitioning, users can efficiently operate on a slice of records in the table.\nConsistency Guarantees #  Paimon writers use two-phase commit protocol to atomically commit a batch of records to the table. Each commit produces at most two snapshots at commit time. It depends on the incremental write and compaction strategy. If only incremental writes are performed without triggering a compaction operation, only an incremental snapshot will be created. If a compaction operation is triggered, an incremental snapshot and a compacted snapshot will be created.\nFor any two writers modifying a table at the same time, as long as they do not modify the same partition, their commits can occur in parallel. If they modify the same partition, only snapshot isolation is guaranteed. That is, the final table state may be a mix of the two commits, but no changes are lost. See dedicated compaction job for more info.\n"});index.add({'id':19,'href':'/docs/1.1/concepts/rest/bear/','title':"Bear Token",'section':"RESTCatalog",'content':"Bear Token #  A bearer token is an encrypted string, typically generated by the server based on a secret key. When the client sends a request to the server, it must include Authorization: Bearer \u0026lt;token\u0026gt; in the request header. After receiving the request, the server extracts the \u0026lt;token\u0026gt; and validates its legitimacy. If the validation passes, the authentication is successful.\nCREATE CATALOG `paimon-rest-catalog` WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;uri\u0026#39; = \u0026#39;\u0026lt;catalog server url\u0026gt;\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;rest\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;my_instance_name\u0026#39;, \u0026#39;token.provider\u0026#39; = \u0026#39;bear\u0026#39; \u0026#39;token\u0026#39; = \u0026#39;\u0026lt;token\u0026gt;\u0026#39; ); "});index.add({'id':20,'href':'/docs/1.1/primary-key-table/data-distribution/','title':"Data Distribution",'section':"Table with PK",'content':"Data Distribution #  A bucket is the smallest storage unit for reads and writes, each bucket directory contains an LSM tree.\nFixed Bucket #  Configure a bucket greater than 0, using Fixed Bucket mode, according to Math.abs(key_hashcode % numBuckets) to compute the bucket of record.\nRescaling buckets can only be done through offline processes, see Rescale Bucket. A too large number of buckets leads to too many small files, and a too small number of buckets leads to poor write performance.\nDynamic Bucket #  Default mode for primary key table, or configure 'bucket' = '-1'.\nThe keys that arrive first will fall into the old buckets, and the new keys will fall into the new buckets, the distribution of buckets and keys depends on the order in which the data arrives. Paimon maintains an index to determine which key corresponds to which bucket.\nPaimon will automatically expand the number of buckets.\n Option1: 'dynamic-bucket.target-row-num': controls the target row number for one bucket. Option2: 'dynamic-bucket.initial-buckets': controls the number of initialized bucket. Option3: 'dynamic-bucket.max-buckets': controls the number of max buckets.  Dynamic Bucket only support single write job. Please do not start multiple jobs to write to the same partition (this can lead to duplicate data). Even if you enable 'write-only' and start a dedicated compaction job, it won\u0026rsquo;t work.  Normal Dynamic Bucket Mode #  When your updates do not cross partitions (no partitions, or primary keys contain all partition fields), Dynamic Bucket mode uses HASH index to maintain mapping from key to bucket, it requires more memory than fixed bucket mode.\nPerformance:\n Generally speaking, there is no performance loss, but there will be some additional memory consumption, 100 million entries in a partition takes up 1 GB more memory, partitions that are no longer active do not take up memory. For tables with low update rates, this mode is recommended to significantly improve performance.  Normal Dynamic Bucket Mode supports sort-compact to speed up queries. See Sort Compact.\nCross Partitions Upsert Dynamic Bucket Mode #  When you need cross partition upsert (primary keys not contain all partition fields), Dynamic Bucket mode directly maintains the mapping of keys to partition and bucket, uses local disks, and initializes indexes by reading all existing keys in the table when starting stream write job. Different merge engines have different behaviors:\n Deduplicate: Delete data from the old partition and insert new data into the new partition. PartialUpdate \u0026amp; Aggregation: Insert new data into the old partition. FirstRow: Ignore new data if there is old value.  Performance: For tables with a large amount of data, there will be a significant loss in performance. Moreover, initialization takes a long time.\nIf your upsert does not rely on too old data, you can consider configuring index TTL to reduce Index and initialization time:\n 'cross-partition-upsert.index-ttl': The TTL in rocksdb index and initialization, this can avoid maintaining too many indexes and lead to worse and worse performance.  But please note that this may also cause data duplication.\nPostpone Bucket #  Postpone bucket mode is configured by 'bucket' = '-2'. This mode aims to solve the difficulty to determine a fixed number of buckets and support different buckets for different partitions.\nWhen writing records into the table, all records will first be stored in the bucket-postpone directory of each partition and are not available to readers.\nTo move the records into the correct bucket and make them readable, you need to run a compaction job. See compact procedure. The bucket number for the partitions compacted for the first time is configured by the option postpone.default-bucket-num, whose default value is 4.\nFinally, when you feel that the bucket number of some partition is too small, you can also run a rescale job. See rescale procedure.\nPick Partition Fields #  The following three types of fields may be defined as partition fields in the warehouse:\n Creation Time (Recommended): The creation time is generally immutable, so you can confidently treat it as a partition field and add it to the primary key. Event Time: Event time is a field in the original table. For CDC data, such as tables synchronized from MySQL CDC or Changelogs generated by Paimon, they are all complete CDC data, including UPDATE_BEFORE records, even if you declare the primary key containing partition field, you can achieve the unique effect (require 'changelog-producer'='input'). CDC op_ts: It cannot be defined as a partition field, unable to know previous record timestamp. So you need to use cross partition upsert, it will consume more resources.  "});index.add({'id':21,'href':'/docs/1.1/project/download/','title':"Download",'section':"Project",'content':"Download #  This documentation is a guide for downloading Paimon Jars.\nEngine Jars #     Version Jar     Flink 2.0 paimon-flink-2.0-1.1.1.jar   Flink 1.20 paimon-flink-1.20-1.1.1.jar   Flink 1.19 paimon-flink-1.19-1.1.1.jar   Flink 1.18 paimon-flink-1.18-1.1.1.jar   Flink 1.17 paimon-flink-1.17-1.1.1.jar   Flink 1.16 paimon-flink-1.16-1.1.1.jar   Flink 1.15 paimon-flink-1.15-1.1.1.jar   Flink Action paimon-flink-action-1.1.1.jar   Spark 3.5 paimon-spark-3.5-1.1.1.jar   Spark 3.4 paimon-spark-3.4-1.1.1.jar   Spark 3.3 paimon-spark-3.3-1.1.1.jar   Spark 3.2 paimon-spark-3.2-1.1.1.jar   Hive 3.1 paimon-hive-connector-3.1-1.1.1.jar   Hive 2.3 paimon-hive-connector-2.3-1.1.1.jar   Hive 2.2 paimon-hive-connector-2.2-1.1.1.jar   Hive 2.1 paimon-hive-connector-2.1-1.1.1.jar   Hive 2.1-cdh-6.3 paimon-hive-connector-2.1-cdh-6.3-1.1.1.jar   Trino Download from master    Filesystem Jars #     Version Jar     paimon-oss paimon-oss-1.1.1.jar   paimon-jindo paimon-jindo-1.1.1.jar   paimon-s3 paimon-s3-1.1.1.jar    API Jars #     Version Jar     paimon-bundle paimon-bundle-1.1.1.jar    "});index.add({'id':22,'href':'/docs/1.1/program-api/java-api/','title':"Java API",'section':"Program API",'content':"Java API #  If possible, recommend using computing engines such as Flink SQL or Spark SQL.  Dependency #  Maven dependency:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.paimon\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;paimon-bundle\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.1.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Or download the jar file: Paimon Bundle. Paimon relies on Hadoop environment, you should add hadoop classpath or bundled jar.\nCreate Catalog #  Before coming into contact with the Table, you need to create a Catalog.\nimport org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.CatalogContext; import org.apache.paimon.catalog.CatalogFactory; import org.apache.paimon.fs.Path; import org.apache.paimon.options.Options; public class CreateCatalog { public static Catalog createFilesystemCatalog() { CatalogContext context = CatalogContext.create(new Path(\u0026#34;...\u0026#34;)); return CatalogFactory.createCatalog(context); } public static Catalog createHiveCatalog() { // Paimon Hive catalog relies on Hive jars  // You should add hive classpath or hive bundled jar.  Options options = new Options(); options.set(\u0026#34;warehouse\u0026#34;, \u0026#34;...\u0026#34;); options.set(\u0026#34;metastore\u0026#34;, \u0026#34;hive\u0026#34;); options.set(\u0026#34;uri\u0026#34;, \u0026#34;...\u0026#34;); options.set(\u0026#34;hive-conf-dir\u0026#34;, \u0026#34;...\u0026#34;); options.set(\u0026#34;hadoop-conf-dir\u0026#34;, \u0026#34;...\u0026#34;); CatalogContext context = CatalogContext.create(options); return CatalogFactory.createCatalog(context); } } Create Table #  You can use the catalog to create tables. The created tables are persistence in the file system. Next time you can directly obtain these tables.\nimport org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; import org.apache.paimon.schema.Schema; import org.apache.paimon.types.DataTypes; public class CreateTable { public static void main(String[] args) { Schema.Builder schemaBuilder = Schema.newBuilder(); schemaBuilder.primaryKey(\u0026#34;f0\u0026#34;, \u0026#34;f1\u0026#34;); schemaBuilder.partitionKeys(\u0026#34;f1\u0026#34;); schemaBuilder.column(\u0026#34;f0\u0026#34;, DataTypes.STRING()); schemaBuilder.column(\u0026#34;f1\u0026#34;, DataTypes.INT()); Schema schema = schemaBuilder.build(); Identifier identifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;my_table\u0026#34;); try { Catalog catalog = CreateCatalog.createFilesystemCatalog(); catalog.createTable(identifier, schema, false); } catch (Catalog.TableAlreadyExistException e) { // do something  } catch (Catalog.DatabaseNotExistException e) { // do something  } } } Get Table #  The Table interface provides access to the table metadata and tools to read and write table.\nimport org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; import org.apache.paimon.table.Table; public class GetTable { public static Table getTable() { Identifier identifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;my_table\u0026#34;); try { Catalog catalog = CreateCatalog.createFilesystemCatalog(); return catalog.getTable(identifier); } catch (Catalog.TableNotExistException e) { // do something  throw new RuntimeException(\u0026#34;table not exist\u0026#34;); } } } Batch Read #  For relatively small amounts of data, or for data that has undergone projection and filtering, you can directly use a standalone program to read the table data.\nBut if the data volume of the table is relatively large, you can distribute splits to different tasks for reading.\nThe reading is divided into two stages:\n Scan Plan: Generate plan splits in a global node (\u0026lsquo;Coordinator\u0026rsquo;, or named \u0026lsquo;Driver\u0026rsquo;). Read Split: Read split in distributed tasks.  import org.apache.paimon.data.InternalRow; import org.apache.paimon.predicate.Predicate; import org.apache.paimon.predicate.PredicateBuilder; import org.apache.paimon.reader.RecordReader; import org.apache.paimon.table.Table; import org.apache.paimon.table.source.ReadBuilder; import org.apache.paimon.table.source.Split; import org.apache.paimon.table.source.TableRead; import org.apache.paimon.types.DataTypes; import org.apache.paimon.types.RowType; import com.google.common.collect.Lists; import java.util.List; public class ReadTable { public static void main(String[] args) throws Exception { // 1. Create a ReadBuilder and push filter (`withFilter`)  // and projection (`withProjection`) if necessary  Table table = GetTable.getTable(); PredicateBuilder builder = new PredicateBuilder(RowType.of(DataTypes.STRING(), DataTypes.INT())); Predicate notNull = builder.isNotNull(0); Predicate greaterOrEqual = builder.greaterOrEqual(1, 12); int[] projection = new int[] {0, 1}; ReadBuilder readBuilder = table.newReadBuilder() .withProjection(projection) .withFilter(Lists.newArrayList(notNull, greaterOrEqual)); // 2. Plan splits in \u0026#39;Coordinator\u0026#39; (or named \u0026#39;Driver\u0026#39;)  List\u0026lt;Split\u0026gt; splits = readBuilder.newScan().plan().splits(); // 3. Distribute these splits to different tasks  // 4. Read a split in task  TableRead read = readBuilder.newRead(); RecordReader\u0026lt;InternalRow\u0026gt; reader = read.createReader(splits); reader.forEachRemaining(System.out::println); } } Batch Write #  The writing is divided into two stages:\n Write records: Write records in distributed tasks, generate commit messages. Commit/Abort: Collect all CommitMessages, commit them in a global node (\u0026lsquo;Coordinator\u0026rsquo;, or named \u0026lsquo;Driver\u0026rsquo;, or named \u0026lsquo;Committer\u0026rsquo;). When the commit fails for certain reason, abort unsuccessful commit via CommitMessages.  import org.apache.paimon.data.BinaryString; import org.apache.paimon.data.GenericRow; import org.apache.paimon.table.Table; import org.apache.paimon.table.sink.BatchTableCommit; import org.apache.paimon.table.sink.BatchTableWrite; import org.apache.paimon.table.sink.BatchWriteBuilder; import org.apache.paimon.table.sink.CommitMessage; import java.util.List; public class BatchWrite { public static void main(String[] args) throws Exception { // 1. Create a WriteBuilder (Serializable)  Table table = GetTable.getTable(); BatchWriteBuilder writeBuilder = table.newBatchWriteBuilder().withOverwrite(); // 2. Write records in distributed tasks  BatchTableWrite write = writeBuilder.newWrite(); GenericRow record1 = GenericRow.of(BinaryString.fromString(\u0026#34;Alice\u0026#34;), 12); GenericRow record2 = GenericRow.of(BinaryString.fromString(\u0026#34;Bob\u0026#34;), 5); GenericRow record3 = GenericRow.of(BinaryString.fromString(\u0026#34;Emily\u0026#34;), 18); // If this is a distributed write, you can use writeBuilder.newWriteSelector.  // WriteSelector determines to which logical downstream writers a record should be written to.  // If it returns empty, no data distribution is required.  write.write(record1); write.write(record2); write.write(record3); List\u0026lt;CommitMessage\u0026gt; messages = write.prepareCommit(); // 3. Collect all CommitMessages to a global node and commit  BatchTableCommit commit = writeBuilder.newCommit(); commit.commit(messages); // Abort unsuccessful commit to delete data files  // commit.abort(messages);  } } Stream Read #  The difference of Stream Read is that StreamTableScan can continuously scan and generate splits.\nStreamTableScan provides the ability to checkpoint and restore, which can let you save the correct state during stream reading.\nimport org.apache.paimon.data.InternalRow; import org.apache.paimon.predicate.Predicate; import org.apache.paimon.predicate.PredicateBuilder; import org.apache.paimon.reader.RecordReader; import org.apache.paimon.table.Table; import org.apache.paimon.table.source.ReadBuilder; import org.apache.paimon.table.source.Split; import org.apache.paimon.table.source.StreamTableScan; import org.apache.paimon.table.source.TableRead; import org.apache.paimon.types.DataTypes; import org.apache.paimon.types.RowType; import com.google.common.collect.Lists; import java.util.List; public class StreamReadTable { public static void main(String[] args) throws Exception { // 1. Create a ReadBuilder and push filter (`withFilter`)  // and projection (`withProjection`) if necessary  Table table = GetTable.getTable(); PredicateBuilder builder = new PredicateBuilder(RowType.of(DataTypes.STRING(), DataTypes.INT())); Predicate notNull = builder.isNotNull(0); Predicate greaterOrEqual = builder.greaterOrEqual(1, 12); int[] projection = new int[] {0, 1}; ReadBuilder readBuilder = table.newReadBuilder() .withProjection(projection) .withFilter(Lists.newArrayList(notNull, greaterOrEqual)); // 2. Plan splits in \u0026#39;Coordinator\u0026#39; (or named \u0026#39;Driver\u0026#39;)  StreamTableScan scan = readBuilder.newStreamScan(); while (true) { List\u0026lt;Split\u0026gt; splits = scan.plan().splits(); // Distribute these splits to different tasks  Long state = scan.checkpoint(); // can be restored in scan.restore(state) after fail over  // 3. Read a split in task  TableRead read = readBuilder.newRead(); RecordReader\u0026lt;InternalRow\u0026gt; reader = read.createReader(splits); reader.forEachRemaining(System.out::println); Thread.sleep(1000); } } } Stream Write #  The difference of Stream Write is that StreamTableCommit can continuously commit.\nKey points to achieve exactly-once consistency:\n CommitUser represents a user. A user can commit multiple times. In distributed processing, you are expected to use the same commitUser. Different applications need to use different commitUsers. The commitIdentifier of StreamTableWrite and StreamTableCommit needs to be consistent, and the id needs to be incremented for the next committing. When a failure occurs, if you still have uncommitted CommitMessages, please use StreamTableCommit#filterAndCommit to exclude the committed messages by commitIdentifier.  import org.apache.paimon.data.BinaryString; import org.apache.paimon.data.GenericRow; import org.apache.paimon.table.Table; import org.apache.paimon.table.sink.CommitMessage; import org.apache.paimon.table.sink.StreamTableCommit; import org.apache.paimon.table.sink.StreamTableWrite; import org.apache.paimon.table.sink.StreamWriteBuilder; import java.util.List; public class StreamWriteTable { public static void main(String[] args) throws Exception { // 1. Create a WriteBuilder (Serializable)  Table table = GetTable.getTable(); StreamWriteBuilder writeBuilder = table.newStreamWriteBuilder(); // 2. Write records in distributed tasks  StreamTableWrite write = writeBuilder.newWrite(); // commitIdentifier like Flink checkpointId  long commitIdentifier = 0; while (true) { GenericRow record1 = GenericRow.of(BinaryString.fromString(\u0026#34;Alice\u0026#34;), 12); GenericRow record2 = GenericRow.of(BinaryString.fromString(\u0026#34;Bob\u0026#34;), 5); GenericRow record3 = GenericRow.of(BinaryString.fromString(\u0026#34;Emily\u0026#34;), 18); // If this is a distributed write, you can use writeBuilder.newWriteSelector.  // WriteSelector determines to which logical downstream writers a record should be written to.  // If it returns empty, no data distribution is required.  write.write(record1); write.write(record2); write.write(record3); List\u0026lt;CommitMessage\u0026gt; messages = write.prepareCommit(false, commitIdentifier); commitIdentifier++; // 3. Collect all CommitMessages to a global node and commit  StreamTableCommit commit = writeBuilder.newCommit(); commit.commit(commitIdentifier, messages); // 4. When failure occurs and you\u0026#39;re not sure if the commit process is successful,  // you can use `filterAndCommit` to retry the commit process.  // Succeeded commits will be automatically skipped.  /* Map\u0026lt;Long, List\u0026lt;CommitMessage\u0026gt;\u0026gt; commitIdentifiersAndMessages = new HashMap\u0026lt;\u0026gt;(); commitIdentifiersAndMessages.put(commitIdentifier, messages); commit.filterAndCommit(commitIdentifiersAndMessages); */ Thread.sleep(1000); } } } Data Types #     Java Paimon     boolean boolean   byte byte   short short   int int   long long   float float   double double   string org.apache.paimon.data.BinaryString   decimal org.apache.paimon.data.Decimal   timestamp org.apache.paimon.data.Timestamp   byte[] byte[]   array org.apache.paimon.data.InternalArray   map org.apache.paimon.data.InternalMap   InternalRow org.apache.paimon.data.InternalRow    Predicate Types #     SQL Predicate Paimon Predicate     and org.apache.paimon.predicate.PredicateBuilder.And   or org.apache.paimon.predicate.PredicateBuilder.Or   is null org.apache.paimon.predicate.PredicateBuilder.IsNull   is not null org.apache.paimon.predicate.PredicateBuilder.IsNotNull   in org.apache.paimon.predicate.PredicateBuilder.In   not in org.apache.paimon.predicate.PredicateBuilder.NotIn   = org.apache.paimon.predicate.PredicateBuilder.Equal   \u0026lt;\u0026gt; org.apache.paimon.predicate.PredicateBuilder.NotEqual   \u0026lt; org.apache.paimon.predicate.PredicateBuilder.LessThan   \u0026lt;= org.apache.paimon.predicate.PredicateBuilder.LessOrEqual   \u0026gt; org.apache.paimon.predicate.PredicateBuilder.GreaterThan   \u0026gt;= org.apache.paimon.predicate.PredicateBuilder.GreaterOrEqual    "});index.add({'id':23,'href':'/docs/1.1/cdc-ingestion/mysql-cdc/','title':"Mysql CDC",'section':"CDC Ingestion",'content':"MySQL CDC #  Paimon supports synchronizing changes from different databases using change data capture (CDC). This feature requires Flink and its CDC connectors.\nPrepare CDC Bundled Jar #  Download CDC Bundled Jar and put them under \u0026lt;FLINK_HOME\u0026gt;/lib/.\n   Version Bundled Jar     3.1.x  flink-sql-connector-mysql-cdc-3.1.x.jar  mysql-connector-java-8.0.27.jar    Only cdc 3.1+ is supported.\nYou can download the flink-connector-mysql-cdc jar package by clicking here.\n Synchronizing Tables #  By using MySqlSyncTableAction in a Flink DataStream job or directly through flink run, users can synchronize one or multiple tables from MySQL into one Paimon table.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  mysql_sync_table \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  [--partition_keys \u0026lt;partition_keys\u0026gt;] \\  [--primary_keys \u0026lt;primary-keys\u0026gt;] \\  [--type_mapping \u0026lt;option1,option2...\u0026gt;] \\  [--computed_column \u0026lt;\u0026#39;column-name=expr-name(args[, ...])\u0026#39;\u0026gt; [--computed_column ...]] \\  [--metadata_column \u0026lt;metadata-column\u0026gt;] \\  [--mysql_conf \u0026lt;mysql-cdc-source-conf\u0026gt; [--mysql_conf \u0026lt;mysql-cdc-source-conf\u0026gt; ...]] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; ...]]    Configuration Description     --warehouse The path to Paimon warehouse.   --database The database name in Paimon catalog.   --table The Paimon table name.   --partition_keys The partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example \"dt,hh,mm\".   --primary_keys The primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example \"buyer_id,seller_id\".   --type_mapping It is used to specify how to map MySQL data type to Paimon type.\nSupported options:  \"tinyint1-not-bool\": maps MySQL TINYINT(1) to TINYINT instead of BOOLEAN. \"to-nullable\": ignores all NOT NULL constraints (except for primary keys). This is used to solve the problem that Flink cannot accept the MySQL 'ALTER TABLE ADD COLUMN type NOT NULL DEFAULT x' operation.  \"to-string\": maps all MySQL types to STRING. \"char-to-string\": maps MySQL CHAR(length)/VARCHAR(length) types to STRING. \"longtext-to-bytes\": maps MySQL LONGTEXT types to BYTES. \"bigint-unsigned-to-bigint\": maps MySQL BIGINT UNSIGNED, BIGINT UNSIGNED ZEROFILL, SERIAL to BIGINT. You should ensure overflow won't occur when using this option.     --computed_column The definitions of computed columns. The argument field is from MySQL table field name. See here for a complete list of configurations.    --metadata_column --metadata_column is used to specify which metadata columns to include in the output schema of the connector. Metadata columns provide additional information related to the source data, for example: --metadata_column table_name,database_name,op_ts. See its document for a complete list of available metadata.   --mysql_conf The configuration for Flink CDC MySQL sources. Each configuration should be specified in the format \"key=value\". hostname, username, password, database-name and table-name are required configurations, others are optional. See its document for a complete list of configurations.   --catalog_conf The configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.   --table_conf The configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.    If the Paimon table you specify does not exist, this action will automatically create the table. Its schema will be derived from all specified MySQL tables. If the Paimon table already exists, its schema will be compared against the schema of all specified MySQL tables.\nExample 1: synchronize tables into one Paimon table #  \u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  mysql_sync_table \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --partition_keys pt \\  --primary_keys pt,uid \\  --computed_column \u0026#39;_year=year(age)\u0026#39; \\  --mysql_conf hostname=127.0.0.1 \\  --mysql_conf username=root \\  --mysql_conf password=123456 \\  --mysql_conf database-name=\u0026#39;source_db\u0026#39; \\  --mysql_conf table-name=\u0026#39;source_table1|source_table2\u0026#39; \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --table_conf changelog-producer=input \\  --table_conf sink.parallelism=4 As example shows, the mysql_conf\u0026rsquo;s table-name supports regular expressions to monitor multiple tables that satisfy the regular expressions. The schemas of all the tables will be merged into one Paimon table schema.\nExample 2: synchronize shards into one Paimon table #  You can also set \u0026lsquo;database-name\u0026rsquo; with a regular expression to capture multiple databases. A typical scenario is that a table \u0026lsquo;source_table\u0026rsquo; is split into database \u0026lsquo;source_db1\u0026rsquo;, \u0026lsquo;source_db2\u0026rsquo; \u0026hellip;, then you can synchronize data of all the \u0026lsquo;source_table\u0026rsquo;s into one Paimon table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  mysql_sync_table \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --partition_keys pt \\  --primary_keys pt,uid \\  --computed_column \u0026#39;_year=year(age)\u0026#39; \\  --mysql_conf hostname=127.0.0.1 \\  --mysql_conf username=root \\  --mysql_conf password=123456 \\  --mysql_conf database-name=\u0026#39;source_db.+\u0026#39; \\  --mysql_conf table-name=\u0026#39;source_table\u0026#39; \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --table_conf changelog-producer=input \\  --table_conf sink.parallelism=4 Synchronizing Databases #  By using MySqlSyncDatabaseAction in a Flink DataStream job or directly through flink run, users can synchronize the whole MySQL database into one Paimon database.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  mysql_sync_database \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  [--ignore_incompatible \u0026lt;true/false\u0026gt;] \\  [--merge_shards \u0026lt;true/false\u0026gt;] \\  [--table_prefix \u0026lt;paimon-table-prefix\u0026gt;] \\  [--table_suffix \u0026lt;paimon-table-suffix\u0026gt;] \\  [--including_tables \u0026lt;mysql-table-name|name-regular-expr\u0026gt;] \\  [--excluding_tables \u0026lt;mysql-table-name|name-regular-expr\u0026gt;] \\  [--mode \u0026lt;sync-mode\u0026gt;] \\  [--metadata_column \u0026lt;metadata-column\u0026gt;] \\  [--type_mapping \u0026lt;option1,option2...\u0026gt;] \\  [--partition_keys \u0026lt;partition_keys\u0026gt;] \\  [--primary_keys \u0026lt;primary-keys\u0026gt;] \\  [--mysql_conf \u0026lt;mysql-cdc-source-conf\u0026gt; [--mysql_conf \u0026lt;mysql-cdc-source-conf\u0026gt; ...]] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; ...]]    Configuration Description     --warehouse The path to Paimon warehouse.   --database The database name in Paimon catalog.   --ignore_incompatible It is default false, in this case, if MySQL table name exists in Paimon and their schema is incompatible,an exception will be thrown. You can specify it to true explicitly to ignore the incompatible tables and exception.   --merge_shards It is default true, in this case, if some tables in different databases have the same name, their schemas will be merged and their records will be synchronized into one Paimon table. Otherwise, each table's records will be synchronized to a corresponding Paimon table, and the Paimon table will be named to 'databaseName_tableName' to avoid potential name conflict.   --table_prefix The prefix of all Paimon tables to be synchronized. For example, if you want all synchronized tables to have \"ods_\" as prefix, you can specify \"--table_prefix ods_\".   --table_suffix The suffix of all Paimon tables to be synchronized. The usage is same as \"--table_prefix\".   --including_tables It is used to specify which source tables are to be synchronized. You must use '|' to separate multiple tables.Because '|' is a special character, a comma is required, for example: 'a|b|c'.Regular expression is supported, for example, specifying \"--including_tables test|paimon.*\" means to synchronize table 'test' and all tables start with 'paimon'.   --excluding_tables It is used to specify which source tables are not to be synchronized. The usage is same as \"--including_tables\". \"--excluding_tables\" has higher priority than \"--including_tables\" if you specified both.   --mode It is used to specify synchronization mode.\nPossible values:\"divided\" (the default mode if you haven't specified one): start a sink for each table, the synchronization of the new table requires restarting the job.\"combined\": start a single combined sink for all tables, the new table will be automatically synchronized.   --metadata_column --metadata_column is used to specify which metadata columns to include in the output schema of the connector. Metadata columns provide additional information related to the source data, for example: --metadata_column table_name,database_name,op_ts. See its document for a complete list of available metadata.   --type_mapping It is used to specify how to map MySQL data type to Paimon type.\nSupported options:  \"tinyint1-not-bool\": maps MySQL TINYINT(1) to TINYINT instead of BOOLEAN. \"to-nullable\": ignores all NOT NULL constraints (except for primary keys). This is used to solve the problem that Flink cannot accept the MySQL 'ALTER TABLE ADD COLUMN type NOT NULL DEFAULT x' operation.  \"to-string\": maps all MySQL types to STRING. \"char-to-string\": maps MySQL CHAR(length)/VARCHAR(length) types to STRING. \"longtext-to-bytes\": maps MySQL LONGTEXT types to BYTES. \"bigint-unsigned-to-bigint\": maps MySQL BIGINT UNSIGNED, BIGINT UNSIGNED ZEROFILL, SERIAL to BIGINT. You should ensure overflow won't occur when using this option.     --partition_keys The partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example \"dt,hh,mm\". If the keys are not in source table, the sink table won't set partition keys.   --primary_keys The primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example \"buyer_id,seller_id\". If the keys are not in source table, but the source table has primary keys, the sink table will use source table's primary keys. Otherwise, the sink table won't set primary keys.   --mysql_conf The configuration for Flink CDC MySQL sources. Each configuration should be specified in the format \"key=value\". hostname, username, password, database-name and table-name are required configurations, others are optional. See its document for a complete list of configurations.   --catalog_conf The configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.   --table_conf The configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.    Only tables with primary keys will be synchronized.\nFor each MySQL table to be synchronized, if the corresponding Paimon table does not exist, this action will automatically create the table. Its schema will be derived from all specified MySQL tables. If the Paimon table already exists, its schema will be compared against the schema of all specified MySQL tables.\nExample 1: synchronize entire database #  \u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  mysql_sync_database \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --mysql_conf hostname=127.0.0.1 \\  --mysql_conf username=root \\  --mysql_conf password=123456 \\  --mysql_conf database-name=source_db \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --table_conf changelog-producer=input \\  --table_conf sink.parallelism=4 Example 2: synchronize newly added tables under database #  Let\u0026rsquo;s say at first a Flink job is synchronizing tables [product, user, address] under database source_db. The command to submit the job looks like:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  mysql_sync_database \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --mysql_conf hostname=127.0.0.1 \\  --mysql_conf username=root \\  --mysql_conf password=123456 \\  --mysql_conf database-name=source_db \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --table_conf changelog-producer=input \\  --table_conf sink.parallelism=4 \\  --including_tables \u0026#39;product|user|address\u0026#39; At a later point we would like the job to also synchronize tables [order, custom], which contains history data. We can achieve this by recovering from the previous snapshot of the job and thus reusing existing state of the job. The recovered job will first snapshot newly added tables, and then continue reading changelog from previous position automatically.\nThe command to recover from previous snapshot and add new tables to synchronize looks like:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  --fromSavepoint savepointPath \\  /path/to/paimon-flink-action-1.1.1.jar \\  mysql_sync_database \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --mysql_conf hostname=127.0.0.1 \\  --mysql_conf username=root \\  --mysql_conf password=123456 \\  --mysql_conf database-name=source_db \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --including_tables \u0026#39;product|user|address|order|custom\u0026#39; You can set --mode combined to enable synchronizing newly added tables without restarting job.  Example 3: synchronize and merge multiple shards #  Let\u0026rsquo;s say you have multiple database shards db1, db2, \u0026hellip; and each database has tables tbl1, tbl2, \u0026hellip;. You can synchronize all the db.+.tbl.+ into tables test_db.tbl1, test_db.tbl2 \u0026hellip; by following command:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  mysql_sync_database \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --mysql_conf hostname=127.0.0.1 \\  --mysql_conf username=root \\  --mysql_conf password=123456 \\  --mysql_conf database-name=\u0026#39;db.+\u0026#39; \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --table_conf changelog-producer=input \\  --table_conf sink.parallelism=4 \\  --including_tables \u0026#39;tbl.+\u0026#39; By setting database-name to a regular expression, the synchronization job will capture all tables under matched databases and merge tables of the same name into one table.\nYou can set --merge_shards false to prevent merging shards. The synchronized tables will be named to \u0026lsquo;databaseName_tableName\u0026rsquo; to avoid potential name conflict.  FAQ #   Chinese characters in records ingested from MySQL are garbled.   Try to set env.java.opts: -Dfile.encoding=UTF-8 in flink-conf.yaml(Flink version \u0026lt; 1.19) or config.yaml(Flink version \u0026gt;= 1.19) (the option is changed to env.java.opts.all since Flink-1.17).  Synchronize MySQL table and column comment.   Synchronize MySQL create table comment to the paimon table, you need to configure --mysql_conf jdbc.properties.useInformationSchema=true. Synchronize MySQL alter table or column comment to the paimon table, you need to configure --mysql_conf debezium.include.schema.comments=true.  "});index.add({'id':24,'href':'/docs/1.1/primary-key-table/merge-engine/partial-update/','title':"Partial Update",'section':"Merge Engine",'content':"Partial Update #  By specifying 'merge-engine' = 'partial-update', users have the ability to update columns of a record through multiple updates until the record is complete. This is achieved by updating the value fields one by one, using the latest data under the same primary key. However, null values are not overwritten in the process.\nFor example, suppose Paimon receives three records:\n \u0026lt;1, 23.0, 10, NULL\u0026gt;- \u0026lt;1, NULL, NULL, 'This is a book'\u0026gt; \u0026lt;1, 25.2, NULL, NULL\u0026gt;  Assuming that the first column is the primary key, the final result would be \u0026lt;1, 25.2, 10, 'This is a book'\u0026gt;.\nFor streaming queries, partial-update merge engine must be used together with lookup or full-compaction changelog producer. (\u0026lsquo;input\u0026rsquo; changelog producer is also supported, but only returns input records.)  By default, Partial update can not accept delete records, you can choose one of the following solutions:\n Configure \u0026lsquo;ignore-delete\u0026rsquo; to ignore delete records. Configure \u0026lsquo;partial-update.remove-record-on-delete\u0026rsquo; to remove the whole row when receiving delete records. Configure \u0026lsquo;sequence-group\u0026rsquo;s to retract partial columns.  Configure \u0026lsquo;partial-update.remove-record-on-sequence-group\u0026rsquo; to remove the whole row when receiving delete records of specified sequence group.     Sequence Group #  A sequence-field may not solve the disorder problem of partial-update tables with multiple stream updates, because the sequence-field may be overwritten by the latest data of another stream during multi-stream update.\nSo we introduce sequence group mechanism for partial-update tables. It can solve:\n Disorder during multi-stream update. Each stream defines its own sequence-groups. A true partial-update, not just a non-null update.  See example:\nCREATE TABLE t ( k INT, a INT, b INT, g_1 INT, c INT, d INT, g_2 INT, PRIMARY KEY (k) NOT ENFORCED ) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;partial-update\u0026#39;, \u0026#39;fields.g_1.sequence-group\u0026#39; = \u0026#39;a,b\u0026#39;, \u0026#39;fields.g_2.sequence-group\u0026#39; = \u0026#39;c,d\u0026#39; ); INSERT INTO t VALUES (1, 1, 1, 1, 1, 1, 1); -- g_2 is null, c, d should not be updated INSERT INTO t VALUES (1, 2, 2, 2, 2, 2, CAST(NULL AS INT)); SELECT * FROM t; -- output 1, 2, 2, 2, 1, 1, 1  -- g_1 is smaller, a, b should not be updated INSERT INTO t VALUES (1, 3, 3, 1, 3, 3, 3); SELECT * FROM t; -- output 1, 2, 2, 2, 3, 3, 3 For fields.\u0026lt;field-name\u0026gt;.sequence-group, valid comparative data types include: DECIMAL, TINYINT, SMALLINT, INTEGER, BIGINT, FLOAT, DOUBLE, DATE, TIME, TIMESTAMP, and TIMESTAMP_LTZ.\nYou can also configure multiple sorted fields in a sequence-group, like fields.\u0026lt;field-name1\u0026gt;,\u0026lt;field-name2\u0026gt;.sequence-group, multiple fields will be compared in order.\nSee example:\nCREATE TABLE SG ( k INT, a INT, b INT, g_1 INT, c INT, d INT, g_2 INT, g_3 INT, PRIMARY KEY (k) NOT ENFORCED ) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;partial-update\u0026#39;, \u0026#39;fields.g_1.sequence-group\u0026#39; = \u0026#39;a,b\u0026#39;, \u0026#39;fields.g_2,g_3.sequence-group\u0026#39; = \u0026#39;c,d\u0026#39; ); INSERT INTO SG VALUES (1, 1, 1, 1, 1, 1, 1, 1); -- g_2, g_3 should not be updated INSERT INTO SG VALUES (1, 2, 2, 2, 2, 2, 1, CAST(NULL AS INT)); SELECT * FROM SG; -- output 1, 2, 2, 2, 1, 1, 1, 1  -- g_1 should not be updated INSERT INTO SG VALUES (1, 3, 3, 1, 3, 3, 3, 1); SELECT * FROM SG; -- output 1, 2, 2, 2, 3, 3, 3, 1 Aggregation For Partial Update #  You can specify aggregation function for the input field, all the functions in the Aggregation are supported.\nSee example:\nCREATE TABLE t ( k INT, a INT, b INT, c INT, d INT, PRIMARY KEY (k) NOT ENFORCED ) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;partial-update\u0026#39;, \u0026#39;fields.a.sequence-group\u0026#39; = \u0026#39;b\u0026#39;, \u0026#39;fields.b.aggregate-function\u0026#39; = \u0026#39;first_value\u0026#39;, \u0026#39;fields.c.sequence-group\u0026#39; = \u0026#39;d\u0026#39;, \u0026#39;fields.d.aggregate-function\u0026#39; = \u0026#39;sum\u0026#39; ); INSERT INTO t VALUES (1, 1, 1, CAST(NULL AS INT), CAST(NULL AS INT)); INSERT INTO t VALUES (1, CAST(NULL AS INT), CAST(NULL AS INT), 1, 1); INSERT INTO t VALUES (1, 2, 2, CAST(NULL AS INT), CAST(NULL AS INT)); INSERT INTO t VALUES (1, CAST(NULL AS INT), CAST(NULL AS INT), 2, 2); SELECT * FROM t; -- output 1, 2, 1, 2, 3 You can also configure an aggregation function for a sequence-group within multiple sorted fields.\nSee example:\nCREATE TABLE AGG ( k INT, a INT, b INT, g_1 INT, c VARCHAR, g_2 INT, g_3 INT, PRIMARY KEY (k) NOT ENFORCED ) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;partial-update\u0026#39;, \u0026#39;fields.a.aggregate-function\u0026#39; = \u0026#39;sum\u0026#39;, \u0026#39;fields.g_1,g_3.sequence-group\u0026#39; = \u0026#39;a\u0026#39;, \u0026#39;fields.g_2.sequence-group\u0026#39; = \u0026#39;c\u0026#39;); -- a in sequence-group g_1, g_3 with sum agg -- b not in sequence-group -- c in sequence-group g_2 without agg  INSERT INTO AGG VALUES (1, 1, 1, 1, \u0026#39;1\u0026#39;, 1, 1); -- g_2 should not be updated INSERT INTO AGG VALUES (1, 2, 2, 2, \u0026#39;2\u0026#39;, CAST(NULL AS INT), 2); SELECT * FROM AGG; -- output 1, 3, 2, 2, \u0026#34;1\u0026#34;, 1, 2  -- g_1, g_3 should not be updated INSERT INTO AGG VALUES (1, 3, 3, 2, \u0026#39;3\u0026#39;, 3, 1); SELECT * FROM AGG; -- output 1, 6, 3, 2, \u0026#34;3\u0026#34;, 3, 2 You can specify a default aggregation function for all the input fields with fields.default-aggregate-function, see example:\nCREATE TABLE t ( k INT, a INT, b INT, c INT, d INT, PRIMARY KEY (k) NOT ENFORCED ) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;partial-update\u0026#39;, \u0026#39;fields.a.sequence-group\u0026#39; = \u0026#39;b\u0026#39;, \u0026#39;fields.c.sequence-group\u0026#39; = \u0026#39;d\u0026#39;, \u0026#39;fields.default-aggregate-function\u0026#39; = \u0026#39;last_non_null_value\u0026#39;, \u0026#39;fields.d.aggregate-function\u0026#39; = \u0026#39;sum\u0026#39; ); INSERT INTO t VALUES (1, 1, 1, CAST(NULL AS INT), CAST(NULL AS INT)); INSERT INTO t VALUES (1, CAST(NULL AS INT), CAST(NULL AS INT), 1, 1); INSERT INTO t VALUES (1, 2, 2, CAST(NULL AS INT), CAST(NULL AS INT)); INSERT INTO t VALUES (1, CAST(NULL AS INT), CAST(NULL AS INT), 2, 2); SELECT * FROM t; -- output 1, 2, 2, 2, 3  "});index.add({'id':25,'href':'/docs/1.1/cdc-ingestion/postgres-cdc/','title':"Postgres CDC",'section':"CDC Ingestion",'content':"Postgres CDC #  Paimon supports synchronizing changes from different databases using change data capture (CDC). This feature requires Flink and its CDC connectors.\nPrepare CDC Bundled Jar #  flink-connector-postgres-cdc-*.jar Synchronizing Tables #  By using PostgresSyncTableAction in a Flink DataStream job or directly through flink run, users can synchronize one or multiple tables from PostgreSQL into one Paimon table.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  postgres_sync_table \\  --warehouse \u0026lt;warehouse_path\u0026gt; \\  --database \u0026lt;database_name\u0026gt; \\  --table \u0026lt;table_name\u0026gt; \\  [--partition_keys \u0026lt;partition_keys\u0026gt;] \\  [--primary_keys \u0026lt;primary_keys\u0026gt;] \\  [--type_mapping \u0026lt;option1,option2...\u0026gt;] \\  [--computed_column \u0026lt;\u0026#39;column-name=expr-name(args[, ...])\u0026#39;\u0026gt; [--computed_column ...]] \\  [--metadata_column \u0026lt;metadata_column\u0026gt;] \\  [--postgres_conf \u0026lt;postgres_cdc_source_conf\u0026gt; [--postgres_conf \u0026lt;postgres_cdc_source_conf\u0026gt; ...]] \\  [--catalog_conf \u0026lt;paimon_catalog_conf\u0026gt; [--catalog_conf \u0026lt;paimon_catalog_conf\u0026gt; ...]] \\  [--table_conf \u0026lt;paimon_table_sink_conf\u0026gt; [--table_conf \u0026lt;paimon_table_sink_conf\u0026gt; ...]]    Configuration Description     --warehouse The path to Paimon warehouse.   --database The database name in Paimon catalog.   --table The Paimon table name.   --partition_keys The partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example \"dt,hh,mm\".   --primary_keys The primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example \"buyer_id,seller_id\".   --type_mapping It is used to specify how to map PostgreSQL data type to Paimon type.\nSupported options:  \"to-string\": maps all PostgreSQL types to STRING.     --computed_column The definitions of computed columns. The argument field is from PostgreSQL table field name. See here for a complete list of configurations.    --metadata_column --metadata_column is used to specify which metadata columns to include in the output schema of the connector. Metadata columns provide additional information related to the source data, for example: --metadata_column table_name,database_name,schema_name,op_ts. See its document for a complete list of available metadata.   --postgres_conf The configuration for Flink CDC Postgres sources. Each configuration should be specified in the format \"key=value\". hostname, username, password, database-name, schema-name, table-name and slot.name are required configurations, others are optional. See its document for a complete list of configurations.   --catalog_conf The configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.   --table_conf The configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.    If the Paimon table you specify does not exist, this action will automatically create the table. Its schema will be derived from all specified PostgreSQL tables. If the Paimon table already exists, its schema will be compared against the schema of all specified PostgreSQL tables.\nExample 1: synchronize tables into one Paimon table\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  postgres_sync_table \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --partition_keys pt \\  --primary_keys pt,uid \\  --computed_column \u0026#39;_year=year(age)\u0026#39; \\  --postgres_conf hostname=127.0.0.1 \\  --postgres_conf username=root \\  --postgres_conf password=123456 \\  --postgres_conf database-name=\u0026#39;source_db\u0026#39; \\  --postgres_conf schema-name=\u0026#39;public\u0026#39; \\  --postgres_conf table-name=\u0026#39;source_table1|source_table2\u0026#39; \\  --postgres_conf slot.name=\u0026#39;paimon_cdc\u0026#39; \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --table_conf changelog-producer=input \\  --table_conf sink.parallelism=4 As example shows, the postgres_conf\u0026rsquo;s table-name supports regular expressions to monitor multiple tables that satisfy the regular expressions. The schemas of all the tables will be merged into one Paimon table schema.\nExample 2: synchronize shards into one Paimon table\nYou can also set \u0026lsquo;schema-name\u0026rsquo; with a regular expression to capture multiple schemas. A typical scenario is that a table \u0026lsquo;source_table\u0026rsquo; is split into schema \u0026lsquo;source_schema1\u0026rsquo;, \u0026lsquo;source_schema2\u0026rsquo; \u0026hellip;, then you can synchronize data of all the \u0026lsquo;source_table\u0026rsquo;s into one Paimon table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  postgres_sync_table \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --partition_keys pt \\  --primary_keys pt,uid \\  --computed_column \u0026#39;_year=year(age)\u0026#39; \\  --postgres_conf hostname=127.0.0.1 \\  --postgres_conf username=root \\  --postgres_conf password=123456 \\  --postgres_conf database-name=\u0026#39;source_db\u0026#39; \\  --postgres_conf schema-name=\u0026#39;source_schema.+\u0026#39; \\  --postgres_conf table-name=\u0026#39;source_table\u0026#39; \\  --postgres_conf slot.name=\u0026#39;paimon_cdc\u0026#39; \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --table_conf changelog-producer=input \\  --table_conf sink.parallelism=4 "});index.add({'id':26,'href':'/docs/1.1/concepts/spec/schema/','title':"Schema",'section':"Specification",'content':"Schema #  The version of the schema file starts from 0 and currently retains all versions of the schema. There may be old files that rely on the old schema version, so its deletion should be done with caution.\nSchema File is JSON, it includes:\n fields: data field list, data field contains id, name, type, field id is used to support schema evolution. partitionKeys: field name list, partition definition of the table, it cannot be modified. primaryKeys: field name list, primary key definition of the table, it cannot be modified. options: map\u0026lt;string, string\u0026gt;, no ordered, options of the table, including a lot of capabilities and optimizations.  Example #  { \u0026#34;version\u0026#34; : 3, \u0026#34;id\u0026#34; : 0, \u0026#34;fields\u0026#34; : [ { \u0026#34;id\u0026#34; : 0, \u0026#34;name\u0026#34; : \u0026#34;order_id\u0026#34;, \u0026#34;type\u0026#34; : \u0026#34;BIGINT NOT NULL\u0026#34; }, { \u0026#34;id\u0026#34; : 1, \u0026#34;name\u0026#34; : \u0026#34;order_name\u0026#34;, \u0026#34;type\u0026#34; : \u0026#34;STRING\u0026#34; }, { \u0026#34;id\u0026#34; : 2, \u0026#34;name\u0026#34; : \u0026#34;order_user_id\u0026#34;, \u0026#34;type\u0026#34; : \u0026#34;BIGINT\u0026#34; }, { \u0026#34;id\u0026#34; : 3, \u0026#34;name\u0026#34; : \u0026#34;order_shop_id\u0026#34;, \u0026#34;type\u0026#34; : \u0026#34;BIGINT\u0026#34; } ], \u0026#34;highestFieldId\u0026#34; : 3, \u0026#34;partitionKeys\u0026#34; : [ ], \u0026#34;primaryKeys\u0026#34; : [ \u0026#34;order_id\u0026#34; ], \u0026#34;options\u0026#34; : { \u0026#34;bucket\u0026#34; : \u0026#34;5\u0026#34; }, \u0026#34;comment\u0026#34; : \u0026#34;\u0026#34;, \u0026#34;timeMillis\u0026#34; : 1720496663041 } Compatibility #  For old versions:\n version 1: should put bucket -\u0026gt; 1 to options if there is no bucket key. version 1 \u0026amp; 2: should put file.format -\u0026gt; orc to options if there is no file.format key.  DataField #  DataField represents a column of the table.\n id: int, column id, automatic increment, it is used for schema evolution. name: string, column name. type: data type, it is very similar to SQL type string. description: string.  Update Schema #  Updating the schema should generate a new schema file.\nwarehouse └── default.db └── my_table ├── schema ├── schema-0 ├── schema-1 └── schema-2 There is a reference to schema in the snapshot. The schema file with the highest numerical value is usually the latest schema file.\nOld schema files cannot be directly deleted because there may be old data files that reference old schema files. When reading table, it is necessary to rely on them for schema evolution reading.\n"});index.add({'id':27,'href':'/docs/1.1/flink/sql-ddl/','title':"SQL DDL",'section':"Engine Flink",'content':"SQL DDL #  Create Catalog #  Paimon catalogs currently support three types of metastores:\n filesystem metastore (default), which stores both metadata and table files in filesystems. hive metastore, which additionally stores metadata in Hive metastore. Users can directly access the tables from Hive. jdbc metastore, which additionally stores metadata in relational databases such as MySQL, Postgres, etc.  See CatalogOptions for detailed options when creating a catalog.\nCreate Filesystem Catalog #  The following Flink SQL registers and uses a Paimon catalog named my_catalog. Metadata and table files are stored under hdfs:///path/to/warehouse.\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs:///path/to/warehouse\u0026#39; ); USE CATALOG my_catalog; You can define any default table options with the prefix table-default. for tables created in the catalog.\nCreating Hive Catalog #  By using Paimon Hive catalog, changes to the catalog will directly affect the corresponding Hive metastore. Tables created in such catalog can also be accessed directly from Hive.\nTo use Hive catalog, Database name, Table name and Field names should be lower case.\nPaimon Hive catalog in Flink relies on Flink Hive connector bundled jar. You should first download Hive connector bundled jar and add it to classpath.\n   Metastore version Bundle Name SQL Client JAR     2.3.0 - 3.1.3 Flink Bundle Download   1.2.0 - x.x.x Presto Bundle Download    The following Flink SQL registers and uses a Paimon Hive catalog named my_hive. Metadata and table files are stored under hdfs:///path/to/warehouse. In addition, metadata is also stored in Hive metastore.\nIf your Hive requires security authentication such as Kerberos, LDAP, Ranger or you want the paimon table to be managed by Apache Atlas(Setting \u0026lsquo;hive.metastore.event.listeners\u0026rsquo; in hive-site.xml). You can specify the hive-conf-dir and hadoop-conf-dir parameter to the hive-site.xml file path.\nCREATE CATALOG my_hive WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;hive\u0026#39;, -- \u0026#39;uri\u0026#39; = \u0026#39;thrift://\u0026lt;hive-metastore-host-name\u0026gt;:\u0026lt;port\u0026gt;\u0026#39;, default use \u0026#39;hive.metastore.uris\u0026#39; in HiveConf  -- \u0026#39;hive-conf-dir\u0026#39; = \u0026#39;...\u0026#39;, this is recommended in the kerberos environment  -- \u0026#39;hadoop-conf-dir\u0026#39; = \u0026#39;...\u0026#39;, this is recommended in the kerberos environment  -- \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs:///path/to/warehouse\u0026#39;, default use \u0026#39;hive.metastore.warehouse.dir\u0026#39; in HiveConf ); USE CATALOG my_hive; You can define any default table options with the prefix table-default. for tables created in the catalog.\nAlso, you can create FlinkGenericCatalog.\n When using hive catalog to change incompatible column types through alter table, you need to configure hive.metastore.disallow.incompatible.col.type.changes=false. see HIVE-17832.\n  If you are using Hive3, please disable Hive ACID:\nhive.strict.managed.tables=false hive.create.as.insert.only=false metastore.create.as.acid=false  Synchronizing Partitions into Hive Metastore #  By default, Paimon does not synchronize newly created partitions into Hive metastore. Users will see an unpartitioned table in Hive. Partition push-down will be carried out by filter push-down instead.\nIf you want to see a partitioned table in Hive and also synchronize newly created partitions into Hive metastore, please set the table property metastore.partitioned-table to true. Also see CoreOptions.\nAdding Parameters to a Hive Table #  Using the table option facilitates the convenient definition of Hive table parameters. Parameters prefixed with hive. will be automatically defined in the TBLPROPERTIES of the Hive table. For instance, using the option hive.table.owner=Jon will automatically add the parameter table.owner=Jon to the table properties during the creation process.\nSetting Location in Properties #  If you are using an object storage , and you don\u0026rsquo;t want that the location of paimon table/database is accessed by the filesystem of hive, which may lead to the error such as \u0026ldquo;No FileSystem for scheme: s3a\u0026rdquo;. You can set location in the properties of table/database by the config of location-in-properties. See setting the location of table/database in properties \nCreating JDBC Catalog #  By using the Paimon JDBC catalog, changes to the catalog will be directly stored in relational databases such as SQLite, MySQL, postgres, etc.\nCurrently, lock configuration is only supported for MySQL and SQLite. If you are using a different type of database for catalog storage, please do not configure lock.enabled.\nPaimon JDBC Catalog in Flink needs to correctly add the corresponding jar package for connecting to the database. You should first download JDBC connector bundled jar and add it to classpath. such as MySQL, postgres\n   database type Bundle Name SQL Client JAR     mysql mysql-connector-java Download   postgres postgresql Download    CREATE CATALOG my_jdbc WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;jdbc\u0026#39;, \u0026#39;uri\u0026#39; = \u0026#39;jdbc:mysql://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;databaseName\u0026gt;\u0026#39;, \u0026#39;jdbc.user\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;jdbc.password\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;catalog-key\u0026#39;=\u0026#39;jdbc\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs:///path/to/warehouse\u0026#39; ); USE CATALOG my_jdbc; You can configure any connection parameters that have been declared by JDBC through \u0026ldquo;jdbc.\u0026rdquo;, the connection parameters may be different between different databases, please configure according to the actual situation.\nYou can also perform logical isolation for databases under multiple catalogs by specifying \u0026ldquo;catalog-key\u0026rdquo;.\nAdditionally, when creating a JdbcCatalog, you can specify the maximum length for the lock key by configuring \u0026ldquo;lock-key-max-length,\u0026rdquo; which defaults to 255. Since this value is a combination of {catalog-key}.{database-name}.{table-name}, please adjust accordingly.\nYou can define any default table options with the prefix table-default. for tables created in the catalog.\nCreate Table #  After use Paimon catalog, you can create and drop tables. Tables created in Paimon Catalogs are managed by the catalog. When the table is dropped from catalog, its table files will also be deleted.\nThe following SQL assumes that you have registered and are using a Paimon catalog. It creates a managed table named my_table with five columns in the catalog\u0026rsquo;s default database, where dt, hh and user_id are the primary keys.\nCREATE TABLE my_table ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ); You can create partitioned table:\nCREATE TABLE my_table ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ) PARTITIONED BY (dt, hh); If you need cross partition upsert (primary keys not contain all partition fields), see Cross partition Upsert mode.  By configuring partition.expiration-time, expired partitions can be automatically deleted.  Specify Statistics Mode #  Paimon will automatically collect the statistics of the data file for speeding up the query process. There are four modes supported:\n full: collect the full metrics: null_count, min, max . truncate(length): length can be any positive number, the default mode is truncate(16), which means collect the null count, min/max value with truncated length of 16. This is mainly to avoid too big column which will enlarge the manifest file. counts: only collect the null count. none: disable the metadata stats collection.  The statistics collector mode can be configured by 'metadata.stats-mode', by default is 'truncate(16)'. You can configure the field level by setting 'fields.{field_name}.stats-mode'.\nFor the stats mode of none, by default metadata.stats-dense-store is true, which will significantly reduce the storage size of the manifest. But the Paimon sdk in reading engine requires at least version 0.9.1 or 1.0.0 or higher.\nField Default Value #  Paimon table currently supports setting default values for fields in table properties by 'fields.item_id.default-value', note that partition fields and primary key fields can not be specified.\nCreate Table As Select #  Table can be created and populated by the results of a query, for example, we have a sql like this: CREATE TABLE table_b AS SELECT id, name FORM table_a, The resulting table table_b will be equivalent to create the table and insert the data with the following statement: CREATE TABLE table_b (id INT, name STRING); INSERT INTO table_b SELECT id, name FROM table_a;\nWe can specify the primary key or partition when use CREATE TABLE AS SELECT, for syntax, please refer to the following sql.\n/* For streaming mode, you need to enable the checkpoint. */ CREATE TABLE my_table ( user_id BIGINT, item_id BIGINT ); CREATE TABLE my_table_as AS SELECT * FROM my_table; /* partitioned table */ CREATE TABLE my_table_partition ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh); CREATE TABLE my_table_partition_as WITH (\u0026#39;partition\u0026#39; = \u0026#39;dt\u0026#39;) AS SELECT * FROM my_table_partition; /* change options */ CREATE TABLE my_table_options ( user_id BIGINT, item_id BIGINT ) WITH (\u0026#39;file.format\u0026#39; = \u0026#39;orc\u0026#39;); CREATE TABLE my_table_options_as WITH (\u0026#39;file.format\u0026#39; = \u0026#39;parquet\u0026#39;) AS SELECT * FROM my_table_options; /* primary key */ CREATE TABLE my_table_pk ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ); CREATE TABLE my_table_pk_as WITH (\u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh\u0026#39;) AS SELECT * FROM my_table_pk; /* primary key + partition */ CREATE TABLE my_table_all ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ) PARTITIONED BY (dt, hh); CREATE TABLE my_table_all_as WITH (\u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh\u0026#39;, \u0026#39;partition\u0026#39; = \u0026#39;dt\u0026#39;) AS SELECT * FROM my_table_all; Create Table Like #  To create a table with the same schema, partition, and table properties as another table, use CREATE TABLE LIKE.\nCREATE TABLE my_table ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ); CREATE TABLE my_table_like LIKE my_table (EXCLUDING OPTIONS); Work with Flink Temporary Tables #  Flink Temporary tables are just recorded but not managed by the current Flink SQL session. If the temporary table is dropped, its resources will not be deleted. Temporary tables are also dropped when Flink SQL session is closed.\nIf you want to use Paimon catalog along with other tables but do not want to store them in other catalogs, you can create a temporary table. The following Flink SQL creates a Paimon catalog and a temporary table and also illustrates how to use both tables together.\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs:///path/to/warehouse\u0026#39; ); USE CATALOG my_catalog; -- Assume that there is already a table named my_table in my_catalog  CREATE TEMPORARY TABLE temp_table ( k INT, v STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;hdfs:///path/to/temp_table.csv\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ); SELECT my_table.k, my_table.v, temp_table.v FROM my_table JOIN temp_table ON my_table.k = temp_table.k; "});index.add({'id':28,'href':'/docs/1.1/spark/sql-ddl/','title':"SQL DDL",'section':"Engine Spark",'content':"SQL DDL #  Catalog #  Create Catalog #  Paimon catalogs currently support three types of metastores:\n filesystem metastore (default), which stores both metadata and table files in filesystems. hive metastore, which additionally stores metadata in Hive metastore. Users can directly access the tables from Hive. jdbc metastore, which additionally stores metadata in relational databases such as MySQL, Postgres, etc.  See CatalogOptions for detailed options when creating a catalog.\nCreate Filesystem Catalog #  The following Spark SQL registers and uses a Paimon catalog named my_catalog. Metadata and table files are stored under hdfs:///path/to/warehouse.\nThe following shell command registers a paimon catalog named paimon. Metadata and table files are stored under hdfs:///path/to/warehouse.\nspark-sql ... \\  --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\  --conf spark.sql.catalog.paimon.warehouse=hdfs:///path/to/warehouse You can define any default table options with the prefix spark.sql.catalog.paimon.table-default. for tables created in the catalog.\nAfter spark-sql is started, you can switch to the default database of the paimon catalog with the following SQL.\nUSE paimon.default; Creating Hive Catalog #  By using Paimon Hive catalog, changes to the catalog will directly affect the corresponding Hive metastore. Tables created in such catalog can also be accessed directly from Hive.\nTo use Hive catalog, Database name, Table name and Field names should be lower case.\nYour Spark installation should be able to detect, or already contains Hive dependencies. See here for more information.\nThe following shell command registers a Paimon Hive catalog named paimon. Metadata and table files are stored under hdfs:///path/to/warehouse. In addition, metadata is also stored in Hive metastore.\nspark-sql ... \\  --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\  --conf spark.sql.catalog.paimon.warehouse=hdfs:///path/to/warehouse \\  --conf spark.sql.catalog.paimon.metastore=hive \\  --conf spark.sql.catalog.paimon.uri=thrift://\u0026lt;hive-metastore-host-name\u0026gt;:\u0026lt;port\u0026gt; You can define any default table options with the prefix spark.sql.catalog.paimon.table-default. for tables created in the catalog.\nAfter spark-sql is started, you can switch to the default database of the paimon catalog with the following SQL.\nUSE paimon.default; Also, you can create SparkGenericCatalog.\nSynchronizing Partitions into Hive Metastore\nBy default, Paimon does not synchronize newly created partitions into Hive metastore. Users will see an unpartitioned table in Hive. Partition push-down will be carried out by filter push-down instead.\nIf you want to see a partitioned table in Hive and also synchronize newly created partitions into Hive metastore, please set the table property metastore.partitioned-table to true. Also see CoreOptions.\nCreating JDBC Catalog #  By using the Paimon JDBC catalog, changes to the catalog will be directly stored in relational databases such as SQLite, MySQL, postgres, etc.\nCurrently, lock configuration is only supported for MySQL and SQLite. If you are using a different type of database for catalog storage, please do not configure lock.enabled.\nPaimon JDBC Catalog in Spark needs to correctly add the corresponding jar package for connecting to the database. You should first download JDBC connector bundled jar and add it to classpath. such as MySQL, postgres\n   database type Bundle Name SQL Client JAR     mysql mysql-connector-java Download   postgres postgresql Download    spark-sql ... \\  --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\  --conf spark.sql.catalog.paimon.warehouse=hdfs:///path/to/warehouse \\  --conf spark.sql.catalog.paimon.metastore=jdbc \\  --conf spark.sql.catalog.paimon.uri=jdbc:mysql://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;databaseName\u0026gt; \\  --conf spark.sql.catalog.paimon.jdbc.user=... \\  --conf spark.sql.catalog.paimon.jdbc.password=... USE paimon.default; Creating REST Catalog #  By using the Paimon REST catalog, changes to the catalog will be directly stored in remote server.\nbear token #  spark-sql ... \\  --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\  --conf spark.sql.catalog.paimon.metastore=rest \\  --conf spark.sql.catalog.paimon.uri=\u0026lt;catalog server url\u0026gt; \\  --conf spark.sql.catalog.paimon.token.provider=bear \\  --conf spark.sql.catalog.paimon.token=\u0026lt;token\u0026gt; dlf ak #  spark-sql ... \\  --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\  --conf spark.sql.catalog.paimon.metastore=rest \\  --conf spark.sql.catalog.paimon.uri=\u0026lt;catalog server url\u0026gt; \\  --conf spark.sql.catalog.paimon.token.provider=dlf \\  --conf spark.sql.catalog.paimon.dlf.access-key-id=\u0026lt;access-key-id\u0026gt; \\  --conf spark.sql.catalog.paimon.dlf.access-key-secret=\u0026lt;security-token\u0026gt; dlf sts token #  spark-sql ... \\  --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\  --conf spark.sql.catalog.paimon.metastore=rest \\  --conf spark.sql.catalog.paimon.uri=\u0026lt;catalog server url\u0026gt; \\  --conf spark.sql.catalog.paimon.token.provider=dlf \\  --conf spark.sql.catalog.paimon.dlf.access-key-id=\u0026lt;access-key-id\u0026gt; \\  --conf spark.sql.catalog.paimon.dlf.access-key-secret=\u0026lt;access-key-secret\u0026gt; \\  --conf spark.sql.catalog.paimon.dlf.security-token=\u0026lt;security-token\u0026gt; USE paimon.default; Table #  Create Table #  After use Paimon catalog, you can create and drop tables. Tables created in Paimon Catalogs are managed by the catalog. When the table is dropped from catalog, its table files will also be deleted.\nThe following SQL assumes that you have registered and are using a Paimon catalog. It creates a managed table named my_table with five columns in the catalog\u0026rsquo;s default database, where dt, hh and user_id are the primary keys.\nCREATE TABLE my_table ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39; ); You can create partitioned table:\nCREATE TABLE my_table ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39; ); Create External Table #  When the catalog\u0026rsquo;s metastore type is hive, if the location is specified when creating a table, that table will be considered an external table; otherwise, it will be a managed table.\nWhen you drop an external table, only the metadata in Hive will be removed, and the actual data files will not be deleted; whereas dropping a managed table will also delete the data.\nCREATE TABLE my_table ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39; ) LOCATION \u0026#39;/path/to/table\u0026#39;; Furthermore, if there is already data stored in the specified location, you can create the table without explicitly specifying the fields, partitions and props or other information. In this case, the new table will inherit them all from the existing table’s metadata.\nHowever, if you manually specify them, you need to ensure that they are consistent with those of the existing table (props can be a subset). Therefore, it is strongly recommended not to specify them.\nCREATE TABLE my_table LOCATION \u0026#39;/path/to/table\u0026#39;; Create Table As Select #  Table can be created and populated by the results of a query, for example, we have a sql like this: CREATE TABLE table_b AS SELECT id, name FORM table_a, The resulting table table_b will be equivalent to create the table and insert the data with the following statement: CREATE TABLE table_b (id INT, name STRING); INSERT INTO table_b SELECT id, name FROM table_a;\nWe can specify the primary key or partition when use CREATE TABLE AS SELECT, for syntax, please refer to the following sql.\nCREATE TABLE my_table ( user_id BIGINT, item_id BIGINT ); CREATE TABLE my_table_as AS SELECT * FROM my_table; /* partitioned table*/ CREATE TABLE my_table_partition ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh); CREATE TABLE my_table_partition_as PARTITIONED BY (dt) AS SELECT * FROM my_table_partition; /* change TBLPROPERTIES */ CREATE TABLE my_table_options ( user_id BIGINT, item_id BIGINT ) TBLPROPERTIES (\u0026#39;file.format\u0026#39; = \u0026#39;orc\u0026#39;); CREATE TABLE my_table_options_as TBLPROPERTIES (\u0026#39;file.format\u0026#39; = \u0026#39;parquet\u0026#39;) AS SELECT * FROM my_table_options; /* primary key */ CREATE TABLE my_table_pk ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39; ); CREATE TABLE my_table_pk_as TBLPROPERTIES (\u0026#39;primary-key\u0026#39; = \u0026#39;dt\u0026#39;) AS SELECT * FROM my_table_pk; /* primary key + partition */ CREATE TABLE my_table_all ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39; ); CREATE TABLE my_table_all_as PARTITIONED BY (dt) TBLPROPERTIES (\u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh\u0026#39;) AS SELECT * FROM my_table_all; View #  Views are based on the result-set of an SQL query, when using org.apache.paimon.spark.SparkCatalog, views are managed by paimon itself. And in this case, views are supported when the metastore type is hive, and temporary views are not supported yet.\nCreate Or Replace View #  CREATE VIEW constructs a virtual table that has no physical data.\n-- create a view. CREATE VIEW v1 AS SELECT * FROM t1; -- create a view, if a view of same name already exists, it will be replaced. CREATE OR REPLACE VIEW v1 AS SELECT * FROM t1; Drop View #  DROP VIEW removes the metadata associated with a specified view from the catalog.\n-- drop a view DROP VIEW v1; Tag #  Create Or Replace Tag #  Create or replace a tag syntax with the following options.\n Create a tag with or without the snapshot id and time retention. Create an existed tag is not failed if using IF NOT EXISTS syntax. Update a tag using REPLACE TAG or CREATE OR REPLACE TAG syntax.  -- create a tag based on the latest snapshot and no retention. ALTER TABLE T CREATE TAG `TAG-1`; -- create a tag based on the latest snapshot and no retention if it doesn\u0026#39;t exist. ALTER TABLE T CREATE TAG IF NOT EXISTS `TAG-1`; -- create a tag based on the latest snapshot and retain it for 7 day. ALTER TABLE T CREATE TAG `TAG-2` RETAIN 7 DAYS; -- create a tag based on snapshot-1 and no retention. ALTER TABLE T CREATE TAG `TAG-3` AS OF VERSION 1; -- create a tag based on snapshot-2 and retain it for 12 hour. ALTER TABLE T CREATE TAG `TAG-4` AS OF VERSION 2 RETAIN 12 HOURS; -- replace a existed tag with new snapshot id and new retention ALTER TABLE T REPLACE TAG `TAG-4` AS OF VERSION 2 RETAIN 24 HOURS; -- create or replace a tag, create tag if it not exist, replace tag if it exists. ALTER TABLE T CREATE OR REPLACE TAG `TAG-5` AS OF VERSION 2 RETAIN 24 HOURS; Delete Tag #  Delete a tag or multiple tags of a table.\n-- delete a tag. ALTER TABLE T DELETE TAG `TAG-1`; -- delete a tag if it exists. ALTER TABLE T DELETE TAG IF EXISTS `TAG-1` -- delete multiple tags, delimiter is \u0026#39;,\u0026#39;. ALTER TABLE T DELETE TAG `TAG-1,TAG-2`; Rename Tag #  Rename an existing tag with a new tag name.\nALTER TABLE T RENAME TAG `TAG-1` TO `TAG-2`; Show Tags #  List all tags of a table.\nSHOW TAGS T; "});index.add({'id':29,'href':'/docs/1.1/spark/sql-functions/','title':"SQL Functions",'section':"Engine Spark",'content':"SQL Functions #  This section introduce all available Paimon Spark functions.\nmax_pt #  max_pt($table_name)\nIt accepts a string type literal to specify the table name and return a max-valid-toplevel partition value.\n valid: the partition which contains data files toplevel: only return the first partition value if the table has multi-partition columns  It would throw exception when:\n the table is not a partitioned table the partitioned table does not have partition all of the partitions do not contains data files  Example\n\u0026gt; SELECT max_pt(\u0026#39;t\u0026#39;); 20250101 \u0026gt; SELECT * FROM t where pt = max_pt(\u0026#39;t\u0026#39;); a, 20250101 Since: 1.1.0\n"});index.add({'id':30,'href':'/docs/1.1/flink/sql-write/','title':"SQL Write",'section':"Engine Flink",'content':"SQL Write #  Syntax #  INSERT { INTO | OVERWRITE } table_identifier [ part_spec ] [ column_list ] { value_expr | query }; For more information, please check the syntax document:\nFlink INSERT Statement\nINSERT INTO #  Use INSERT INTO to apply records and changes to tables.\nINSERT INTO my_table SELECT ... INSERT INTO supports both batch and streaming mode. In Streaming mode, by default, it will also perform compaction, snapshot expiration, and even partition expiration in Flink Sink (if it is configured).\nFor multiple jobs to write the same table, you can refer to dedicated compaction job for more info.\nClustering #  In Paimon, clustering is a feature that allows you to cluster data in your Append Table based on the values of certain columns during the write process. This organization of data can significantly enhance the efficiency of downstream tasks when reading the data, as it enables faster and more targeted data retrieval. This feature is only supported for Append Table(bucket = -1) and batch execution mode.\nTo utilize clustering, you can specify the columns you want to cluster when creating or writing to a table. Here\u0026rsquo;s a simple example of how to enable clustering:\nCREATE TABLE my_table ( a STRING, b STRING, c STRING, ) WITH ( \u0026#39;sink.clustering.by-columns\u0026#39; = \u0026#39;a,b\u0026#39;, ); You can also use SQL hints to dynamically set clustering options:\nINSERT INTO my_table /*+ OPTIONS(\u0026#39;sink.clustering.by-columns\u0026#39; = \u0026#39;a,b\u0026#39;) */ SELECT * FROM source; The data is clustered using an automatically chosen strategy (such as ORDER, ZORDER, or HILBERT), but you can manually specify the clustering strategy by setting the sink.clustering.strategy. Clustering relies on sampling and sorting. If the clustering process takes too much time, you can decrease the total sample number by setting the sink.clustering.sample-factor or disable the sorting step by setting the sink.clustering.sort-in-cluster to false.\nYou can refer to FlinkConnectorOptions for more info about the configurations above.\nOverwriting the Whole Table #  For unpartitioned tables, Paimon supports overwriting the whole table. (or for partitioned table which disables dynamic-partition-overwrite option).\nUse INSERT OVERWRITE to overwrite the whole unpartitioned table.\nINSERT OVERWRITE my_table SELECT ... Overwriting a Partition #  For partitioned tables, Paimon supports overwriting a partition.\nUse INSERT OVERWRITE to overwrite a partition.\nINSERT OVERWRITE my_table PARTITION (key1 = value1, key2 = value2, ...) SELECT ... Dynamic Overwrite #  Flink\u0026rsquo;s default overwrite mode is dynamic partition overwrite (that means Paimon only deletes the partitions appear in the overwritten data). You can configure dynamic-partition-overwrite to change it to static overwritten.\n-- MyTable is a Partitioned Table  -- Dynamic overwrite INSERT OVERWRITE my_table SELECT ... -- Static overwrite (Overwrite whole table) INSERT OVERWRITE my_table /*+ OPTIONS(\u0026#39;dynamic-partition-overwrite\u0026#39; = \u0026#39;false\u0026#39;) */ SELECT ... Truncate tables #  Flink 1.17- You can use INSERT OVERWRITE to purge tables by inserting empty value.\nINSERT OVERWRITE my_table /*+ OPTIONS(\u0026#39;dynamic-partition-overwrite\u0026#39;=\u0026#39;false\u0026#39;) */ SELECT * FROM my_table WHERE false; Flink 1.18\u0026#43; TRUNCATE TABLE my_table;  Purging Partitions #  Currently, Paimon supports two ways to purge partitions.\n  Like purging tables, you can use INSERT OVERWRITE to purge data of partitions by inserting empty value to them.\n  Method #1 does not support to drop multiple partitions. In case that you need to drop multiple partitions, you can submit the drop_partition job through flink run.\n  -- Syntax INSERT OVERWRITE my_table /*+ OPTIONS(\u0026#39;dynamic-partition-overwrite\u0026#39;=\u0026#39;false\u0026#39;) */ PARTITION (key1 = value1, key2 = value2, ...) SELECT selectSpec FROM my_table WHERE false; -- The following SQL is an example: -- table definition CREATE TABLE my_table ( k0 INT, k1 INT, v STRING ) PARTITIONED BY (k0, k1); -- you can use INSERT OVERWRITE my_table /*+ OPTIONS(\u0026#39;dynamic-partition-overwrite\u0026#39;=\u0026#39;false\u0026#39;) */ PARTITION (k0 = 0) SELECT k1, v FROM my_table WHERE false; -- or INSERT OVERWRITE my_table /*+ OPTIONS(\u0026#39;dynamic-partition-overwrite\u0026#39;=\u0026#39;false\u0026#39;) */ PARTITION (k0 = 0, k1 = 0) SELECT v FROM my_table WHERE false; Updating tables #  Important table properties setting:\n Only primary key table supports this feature. MergeEngine needs to be deduplicate or partial-update to support this feature. Do not support updating primary keys.   Currently, Paimon supports updating records by using UPDATE in Flink 1.17 and later versions. You can perform UPDATE in Flink\u0026rsquo;s batch mode.\n-- Syntax UPDATE table_identifier SET column1 = value1, column2 = value2, ... WHERE condition; -- The following SQL is an example: -- table definition CREATE TABLE my_table ( a STRING, b INT, c INT, PRIMARY KEY (a) NOT ENFORCED ) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;deduplicate\u0026#39; ); -- you can use UPDATE my_table SET b = 1, c = 2 WHERE a = \u0026#39;myTable\u0026#39;; Deleting from table #  Flink 1.17\u0026#43; Important table properties setting:\n Only primary key tables support this feature. If the table has primary keys, the following MergeEngine support this feature:  deduplicate. partial-update with option \u0026lsquo;partial-update.remove-record-on-delete\u0026rsquo; enabled.   Do not support deleting from table in streaming mode.   -- Syntax DELETE FROM table_identifier WHERE conditions; -- The following SQL is an example: -- table definition CREATE TABLE my_table ( id BIGINT NOT NULL, currency STRING, rate BIGINT, dt String, PRIMARY KEY (id, dt) NOT ENFORCED ) PARTITIONED BY (dt) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;deduplicate\u0026#39; ); -- you can use DELETE FROM my_table WHERE currency = \u0026#39;UNKNOWN\u0026#39;;  Partition Mark Done #  For partitioned tables, each partition may need to be scheduled to trigger downstream batch computation. Therefore, it is necessary to choose this timing to indicate that it is ready for scheduling and to minimize the amount of data drift during scheduling. We call this process: \u0026ldquo;Partition Mark Done\u0026rdquo;.\nExample to mark done:\nCREATE TABLE my_partitioned_table ( f0 INT, f1 INT, f2 INT, ... dt STRING ) PARTITIONED BY (dt) WITH ( \u0026#39;partition.timestamp-formatter\u0026#39;=\u0026#39;yyyyMMdd\u0026#39;, \u0026#39;partition.timestamp-pattern\u0026#39;=\u0026#39;$dt\u0026#39;, \u0026#39;partition.time-interval\u0026#39;=\u0026#39;1 d\u0026#39;, \u0026#39;partition.idle-time-to-done\u0026#39;=\u0026#39;15 m\u0026#39;, \u0026#39;partition.mark-done-action\u0026#39;=\u0026#39;done-partition\u0026#39; ); You can also customize a PartitionMarkDoneAction to mark the partition completed.\n partition.mark-done-action: custom partition.mark-done-action.custom.class: The partition mark done class for implement PartitionMarkDoneAction interface (e.g. org.apache.paimon.CustomPartitionMarkDoneAction).  Define a class CustomPartitionMarkDoneAction to implement the PartitionMarkDoneAction interface.\npackage org.apache.paimon; public class CustomPartitionMarkDoneAction implements PartitionMarkDoneAction { @Override public void markDone(String partition) { // do something.  } @Override public void close() {} } Paimon also support http-report partition mark done action, this action will report the partition to the remote http server.\n partition.mark-done-action: http-report partition.mark-done-action.http.url : Action will report the partition to the remote http server. partition.mark-done-action.http.timeout : Http client connection timeout and default is 5s. partition.mark-done-action.http.params : Http client request params in the request body json.  Http Post request body :\n{ \u0026#34;table\u0026#34;: \u0026#34;table fullName\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;table location path\u0026#34;, \u0026#34;partition\u0026#34;: \u0026#34;mark done partition\u0026#34;, \u0026#34;params\u0026#34; : \u0026#34;custom params\u0026#34; } Http Response body :\n{ \u0026#34;result\u0026#34;: \u0026#34;success\u0026#34; }  Firstly, you need to define the time parser of the partition and the time interval between partitions in order to determine when the partition can be properly marked done. Secondly, you need to define idle-time, which determines how long it takes for the partition to have no new data, and then it will be marked as done. Thirdly, by default, partition mark done will create _SUCCESS file, the content of _SUCCESS file is a json, contains creationTime and modificationTime, they can help you understand if there is any delayed data. You can also configure other actions, like 'done-partition', for example, partition 'dt=20240501' with produce 'dt=20240501.done' done partition.  "});index.add({'id':31,'href':'/docs/1.1/spark/sql-write/','title':"SQL Write",'section':"Engine Spark",'content':"SQL Write #  Insert Table #  The INSERT statement inserts new rows into a table or overwrites the existing data in the table. The inserted rows can be specified by value expressions or result from a query.\nSyntax\nINSERT { INTO | OVERWRITE } table_identifier [ part_spec ] [ column_list ] { value_expr | query }; Parameters\n  table_identifier: Specifies a table name, which may be optionally qualified with a database name.\n  part_spec: An optional parameter that specifies a comma-separated list of key and value pairs for partitions.\n  column_list: An optional parameter that specifies a comma-separated list of columns belonging to the table_identifier table. Spark will reorder the columns of the input query to match the table schema according to the specified column list.\nNote: Since Spark 3.4, INSERT INTO commands with explicit column lists comprising fewer columns than the target table will automatically add the corresponding default values for the remaining columns (or NULL for any column lacking an explicitly-assigned default value). In Spark 3.3 or earlier, column_list\u0026rsquo;s size must be equal to the target table\u0026rsquo;s column size, otherwise these commands would have failed.\n  value_expr ( { value | NULL } [ , … ] ) [ , ( … ) ]: Specifies the values to be inserted. Either an explicitly specified value or a NULL can be inserted. A comma must be used to separate each value in the clause. More than one set of values can be specified to insert multiple rows.\n  For more information, please check the syntax document: Spark INSERT Statement\nInsert Into #  Use INSERT INTO to apply records and changes to tables.\nINSERT INTO my_table SELECT ... Insert Overwrite #  Use INSERT OVERWRITE to overwrite the whole table.\nINSERT OVERWRITE my_table SELECT ... Insert Overwrite Partition #  Use INSERT OVERWRITE to overwrite a partition.\nINSERT OVERWRITE my_table PARTITION (key1 = value1, key2 = value2, ...) SELECT ... Dynamic Overwrite Partition #  Spark\u0026rsquo;s default overwrite mode is static partition overwrite. To enable dynamic overwritten you need to set the Spark session configuration spark.sql.sources.partitionOverwriteMode to dynamic\nFor example:\nCREATE TABLE my_table (id INT, pt STRING) PARTITIONED BY (pt); INSERT INTO my_table VALUES (1, \u0026#39;p1\u0026#39;), (2, \u0026#39;p2\u0026#39;); -- Static overwrite (Overwrite the whole table) INSERT OVERWRITE my_table VALUES (3, \u0026#39;p1\u0026#39;); -- or INSERT OVERWRITE my_table PARTITION (pt) VALUES (3, \u0026#39;p1\u0026#39;); SELECT * FROM my_table; /* +---+---+ | id| pt| +---+---+ | 3| p1| +---+---+ */ -- Static overwrite with specified partitions (Only overwrite pt=\u0026#39;p1\u0026#39;) INSERT OVERWRITE my_table PARTITION (pt=\u0026#39;p1\u0026#39;) VALUES (3); SELECT * FROM my_table; /* +---+---+ | id| pt| +---+---+ | 2| p2| | 3| p1| +---+---+ */ -- Dynamic overwrite (Only overwrite pt=\u0026#39;p1\u0026#39;) SET spark.sql.sources.partitionOverwriteMode=dynamic; INSERT OVERWRITE my_table VALUES (3, \u0026#39;p1\u0026#39;); SELECT * FROM my_table; /* +---+---+ | id| pt| +---+---+ | 2| p2| | 3| p1| +---+---+ */ Truncate Table #  The TRUNCATE TABLE statement removes all the rows from a table or partition(s).\nTRUNCATE TABLE my_table; Update Table #  Updates the column values for the rows that match a predicate. When no predicate is provided, update the column values for all rows.\nNote:\nUpdate primary key columns is not supported when the target table is a primary key table.  Spark supports update PrimitiveType and StructType, for example:\n-- Syntax UPDATE table_identifier SET column1 = value1, column2 = value2, ... WHERE condition; CREATE TABLE t ( id INT, s STRUCT\u0026lt;c1: INT, c2: STRING\u0026gt;, name STRING) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;id\u0026#39;, \u0026#39;merge-engine\u0026#39; = \u0026#39;deduplicate\u0026#39; ); -- you can use UPDATE t SET name = \u0026#39;a_new\u0026#39; WHERE id = 1; UPDATE t SET s.c2 = \u0026#39;a_new\u0026#39; WHERE s.c1 = 1; Delete From Table #  Deletes the rows that match a predicate. When no predicate is provided, deletes all rows.\nDELETE FROM my_table WHERE currency = \u0026#39;UNKNOWN\u0026#39;; Merge Into Table #  Merges a set of updates, insertions and deletions based on a source table into a target table.\nNote:\nIn update clause, to update primary key columns is not supported when the target table is a primary key table.  Example: One\nThis is a simple demo that, if a row exists in the target table update it, else insert it.\n-- Here both source and target tables have the same schema: (a INT, b INT, c STRING), and a is a primary key.  MERGE INTO target USING source ON target.a = source.a WHEN MATCHED THEN UPDATE SET * WHEN NOT MATCHED THEN INSERT * Example: Two\nThis is a demo with multiple, conditional clauses.\n-- Here both source and target tables have the same schema: (a INT, b INT, c STRING), and a is a primary key.  MERGE INTO target USING source ON target.a = source.a WHEN MATCHED AND target.a = 5 THEN UPDATE SET b = source.b + target.b -- when matched and meet the condition 1, then update b; WHEN MATCHED AND source.c \u0026gt; \u0026#39;c2\u0026#39; THEN UPDATE SET * -- when matched and meet the condition 2, then update all the columns; WHEN MATCHED THEN DELETE -- when matched, delete this row in target table; WHEN NOT MATCHED AND c \u0026gt; \u0026#39;c9\u0026#39; THEN INSERT (a, b, c) VALUES (a, b * 1.1, c) -- when not matched but meet the condition 3, then transform and insert this row; WHEN NOT MATCHED THEN INSERT * -- when not matched, insert this row without any transformation; Streaming Write #  Paimon Structured Streaming only supports the two append and complete modes.  // Create a paimon table if not exists. spark.sql(s\u0026#34;\u0026#34;\u0026#34; |CREATE TABLE T (k INT, v STRING) |TBLPROPERTIES (\u0026#39;primary-key\u0026#39;=\u0026#39;k\u0026#39;, \u0026#39;bucket\u0026#39;=\u0026#39;3\u0026#39;) |\u0026#34;\u0026#34;\u0026#34;.stripMargin) // Here we use MemoryStream to fake a streaming source. val inputData = MemoryStream[(Int, String)] val df = inputData.toDS().toDF(\u0026#34;k\u0026#34;, \u0026#34;v\u0026#34;) // Streaming Write to paimon table. val stream = df .writeStream .outputMode(\u0026#34;append\u0026#34;) .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;/path/to/checkpoint\u0026#34;) .format(\u0026#34;paimon\u0026#34;) .start(\u0026#34;/path/to/paimon/sink/table\u0026#34;) Schema Evolution #  Schema evolution is a feature that allows users to easily modify the current schema of a table to adapt to existing data, or new data that changes over time, while maintaining data integrity and consistency.\nPaimon supports automatic schema merging of source data and current table data while data is being written, and uses the merged schema as the latest schema of the table, and it only requires configuring write.merge-schema.\ndata.write .format(\u0026#34;paimon\u0026#34;) .mode(\u0026#34;append\u0026#34;) .option(\u0026#34;write.merge-schema\u0026#34;, \u0026#34;true\u0026#34;) .save(location) When enable write.merge-schema, Paimon can allow users to perform the following actions on table schema by default:\n Adding columns Up-casting the type of column(e.g. Int -\u0026gt; Long)  Paimon also supports explicit type conversions between certain types (e.g. String -\u0026gt; Date, Long -\u0026gt; Int), it requires an explicit configuration write.merge-schema.explicit-cast.\nSchema evolution can be used in streaming mode at the same time.\nval inputData = MemoryStream[(Int, String)] inputData .toDS() .toDF(\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;) .writeStream .format(\u0026#34;paimon\u0026#34;) .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;/path/to/checkpoint\u0026#34;) .option(\u0026#34;write.merge-schema\u0026#34;, \u0026#34;true\u0026#34;) .option(\u0026#34;write.merge-schema.explicit-cast\u0026#34;, \u0026#34;true\u0026#34;) .start(location) Here list the configurations.\n  Scan Mode Description     write.merge-schema If true, merge the data schema and the table schema automatically before write data.   write.merge-schema.explicit-cast If true, allow to merge data types if the two types meet the rules for explicit casting.    "});index.add({'id':32,'href':'/docs/1.1/ecosystem/starrocks/','title':"StarRocks",'section':"Ecosystem",'content':"StarRocks #  This documentation is a guide for using Paimon in StarRocks.\nVersion #  Paimon currently supports StarRocks 3.1 and above. Recommended version is StarRocks 3.2.6 or above.\nCreate Paimon Catalog #  Paimon catalogs are registered by executing a CREATE EXTERNAL CATALOG SQL in StarRocks. For example, you can use the following SQL to create a Paimon catalog named paimon_catalog.\nCREATE EXTERNAL CATALOG paimon_catalog PROPERTIES( \u0026#34;type\u0026#34; = \u0026#34;paimon\u0026#34;, \u0026#34;paimon.catalog.type\u0026#34; = \u0026#34;filesystem\u0026#34;, \u0026#34;paimon.catalog.warehouse\u0026#34; = \u0026#34;oss://\u0026lt;your_bucket\u0026gt;/user/warehouse/\u0026#34; ); More catalog types and configures can be seen in Paimon catalog.\nQuery #  Suppose there already exists a database named test_db and a table named test_tbl in paimon_catalog, you can query this table using the following SQL:\nSELECT * FROM paimon_catalog.test_db.test_tbl; Query System Tables #  You can access all kinds of Paimon system tables by StarRocks. For example, you can read the ro (read-optimized) system table to improve reading performance of primary-key tables.\nSELECT * FROM paimon_catalog.test_db.test_tbl$ro; For another example, you can query partition files of the table using the following SQL:\nSELECT * FROM paimon_catalog.test_db.partition_tbl$partitions; /* +-----------+--------------+--------------------+------------+----------------------------+ | partition | record_count | file_size_in_bytes | file_count | last_update_time | +-----------+--------------+--------------------+------------+----------------------------+ | [1] | 1 | 645 | 1 | 2024-01-01 00:00:00.000000 | +-----------+--------------+--------------------+------------+----------------------------+ */ StarRocks to Paimon type mapping #  This section lists all supported type conversion between StarRocks and Paimon. All StarRocks’s data types can be found in this doc StarRocks Data type overview.\n  StarRocks Data Type Paimon Data Type Atomic Type     STRUCT RowType false   MAP MapType false   ARRAY ArrayType false   BOOLEAN BooleanType true   TINYINT TinyIntType true   SMALLINT SmallIntType true   INT IntType true   BIGINT BigIntType true   FLOAT FloatType true   DOUBLE DoubleType true   CHAR(length) CharType(length) true   VARCHAR(MAX_VARCHAR_LENGTH) VarCharType(VarCharType.MAX_LENGTH) true   VARCHAR(length) VarCharType(length), length is less than VarCharType.MAX_LENGTH true   DATE DateType true   DATETIME TimestampType true   DECIMAL(precision, scale) DecimalType(precision, scale) true   VARBINARY(length) VarBinaryType(length) true   DATETIME LocalZonedTimestampType true    "});index.add({'id':33,'href':'/docs/1.1/append-table/streaming/','title':"Streaming",'section':"Table w/o PK",'content':"Streaming #  You can stream write to the Append table in a very flexible way through Flink, or read the Append table through Flink, using it like a queue. The only difference is that its latency is in minutes. Its advantages are very low cost and the ability to push down filters and projection.\nPre small files merging #  \u0026ldquo;Pre\u0026rdquo; means that this compact occurs before committing files to the snapshot.\nIf Flink\u0026rsquo;s checkpoint interval is short (for example, 30 seconds), each snapshot may produce lots of small changelog files. Too many files may put a burden on the distributed storage cluster.\nIn order to compact small changelog files into large ones, you can set the table option precommit-compact = true. Default value of this option is false, if true, it will add a compact coordinator and worker operator after the writer operator, which copies changelog files into large ones.\nPost small files merging #  \u0026ldquo;Post\u0026rdquo; means that this compact occurs after committing files to the snapshot.\nIn streaming write job, without bucket definition, there is no compaction in writer, instead, will use Compact Coordinator to scan the small files and pass compaction task to Compact Worker. In streaming mode, if you run insert sql in flink, the topology will be like this:\nDo not worry about backpressure, compaction never backpressure.\nIf you set write-only to true, the Compact Coordinator and Compact Worker will be removed in the topology.\nThe auto compaction is only supported in Flink engine streaming mode. You can also start a compaction job in Flink by Flink action in Paimon and disable all the other compactions by setting write-only.\nStreaming Query #  You can stream the Append table and use it like a Message Queue. As with primary key tables, there are two options for streaming reads:\n By default, Streaming read produces the latest snapshot on the table upon first startup, and continue to read the latest incremental records. You can specify scan.mode, scan.snapshot-id, scan.timestamp-millis and/or scan.file-creation-time-millis to stream read incremental only.  Similar to flink-kafka, order is not guaranteed by default, if your data has some sort of order requirement, you also need to consider defining a bucket-key, see Bucketed Append\n"});index.add({'id':34,'href':'/docs/1.1/primary-key-table/','title':"Table with PK",'section':"Apache Paimon",'content':""});index.add({'id':35,'href':'/docs/1.1/primary-key-table/merge-engine/aggregation/','title':"Aggregation",'section':"Merge Engine",'content':"Aggregation #  NOTE: Always set table.exec.sink.upsert-materialize to NONE in Flink SQL TableConfig.  Sometimes users only care about aggregated results. The aggregation merge engine aggregates each value field with the latest data one by one under the same primary key according to the aggregate function.\nEach field not part of the primary keys can be given an aggregate function, specified by the fields.\u0026lt;field-name\u0026gt;.aggregate-function table property, otherwise it will use last_non_null_value aggregation as default. For example, consider the following table definition.\nFlink CREATE TABLE my_table ( product_id BIGINT, price DOUBLE, sales BIGINT, PRIMARY KEY (product_id) NOT ENFORCED ) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;aggregation\u0026#39;, \u0026#39;fields.price.aggregate-function\u0026#39; = \u0026#39;max\u0026#39;, \u0026#39;fields.sales.aggregate-function\u0026#39; = \u0026#39;sum\u0026#39; );  Field price will be aggregated by the max function, and field sales will be aggregated by the sum function. Given two input records \u0026lt;1, 23.0, 15\u0026gt; and \u0026lt;1, 30.2, 20\u0026gt;, the final result will be \u0026lt;1, 30.2, 35\u0026gt;.\nAggregation Functions #  Current supported aggregate functions and data types are:\nsum #  The sum function aggregates the values across multiple rows. It supports DECIMAL, TINYINT, SMALLINT, INTEGER, BIGINT, FLOAT, and DOUBLE data types.\nproduct #  The product function can compute product values across multiple lines. It supports DECIMAL, TINYINT, SMALLINT, INTEGER, BIGINT, FLOAT, and DOUBLE data types.\ncount #  In scenarios where counting rows that match a specific condition is required, you can use the SUM function to achieve this. By expressing a condition as a Boolean value (TRUE or FALSE) and converting it into a numerical value, you can effectively count the rows. In this approach, TRUE is converted to 1, and FALSE is converted to 0.\nFor example, if you have a table orders and want to count the number of rows that meet a specific condition, you can use the following query:\nSELECT SUM(CASE WHEN condition THEN 1 ELSE 0 END) AS count FROM orders; max #  The max function identifies and retains the maximum value. It supports CHAR, VARCHAR, DECIMAL, TINYINT, SMALLINT, INTEGER, BIGINT, FLOAT, DOUBLE, DATE, TIME, TIMESTAMP, and TIMESTAMP_LTZ data types.\nmin #  The min function identifies and retains the minimum value. It supports CHAR, VARCHAR, DECIMAL, TINYINT, SMALLINT, INTEGER, BIGINT, FLOAT, DOUBLE, DATE, TIME, TIMESTAMP, and TIMESTAMP_LTZ data types.\nlast_value #  The last_value function replaces the previous value with the most recently imported value. It supports all data types.\nlast_non_null_value #  The last_non_null_value function replaces the previous value with the latest non-null value. It supports all data types.\nlistagg #  The listagg function concatenates multiple string values into a single string. It supports STRING data type. Each field not part of the primary keys can be given a list agg delimiter, specified by the fields..list-agg-delimiter table property, otherwise it will use \u0026ldquo;,\u0026rdquo; as default.\nbool_and #  The bool_and function evaluates whether all values in a boolean set are true. It supports BOOLEAN data type.\nbool_or #  The bool_or function checks if at least one value in a boolean set is true. It supports BOOLEAN data type.\nfirst_value #  The first_value function retrieves the first null value from a data set. It supports all data types.\nfirst_non_null_value #  The first_non_null_value function selects the first non-null value in a data set. It supports all data types.\nrbm32 #  The rbm32 function aggregates multiple serialized 32-bit RoaringBitmap into a single RoaringBitmap. It supports VARBINARY data type.\nrbm64 #  The rbm64 function aggregates multiple serialized 64-bit Roaring64Bitmap into a single Roaring64Bitmap. It supports VARBINARY data type.\nnested_update #  The nested_update function collects multiple rows into one array (so-called \u0026lsquo;nested table\u0026rsquo;). It supports ARRAY data types.\nUse fields.\u0026lt;field-name\u0026gt;.nested-key=pk0,pk1,... to specify the primary keys of the nested table. If no keys, row will be appended to array.\nAn example:\nFlink -- orders table CREATE TABLE orders ( order_id BIGINT PRIMARY KEY NOT ENFORCED, user_name STRING, address STRING ); -- sub orders that have the same order_id -- belongs to the same order CREATE TABLE sub_orders ( order_id BIGINT, sub_order_id INT, product_name STRING, price BIGINT, PRIMARY KEY (order_id, sub_order_id) NOT ENFORCED ); -- wide table CREATE TABLE order_wide ( order_id BIGINT PRIMARY KEY NOT ENFORCED, user_name STRING, address STRING, sub_orders ARRAY\u0026lt;ROW\u0026lt;sub_order_id BIGINT, product_name STRING, price BIGINT\u0026gt;\u0026gt; ) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;aggregation\u0026#39;, \u0026#39;fields.sub_orders.aggregate-function\u0026#39; = \u0026#39;nested_update\u0026#39;, \u0026#39;fields.sub_orders.nested-key\u0026#39; = \u0026#39;sub_order_id\u0026#39; ); -- widen INSERT INTO order_wide SELECT order_id, user_name, address, CAST (NULL AS ARRAY\u0026lt;ROW\u0026lt;sub_order_id BIGINT, product_name STRING, price BIGINT\u0026gt;\u0026gt;) FROM orders UNION ALL SELECT order_id, CAST (NULL AS STRING), CAST (NULL AS STRING), ARRAY[ROW(sub_order_id, product_name, price)] FROM sub_orders; -- query using UNNEST SELECT order_id, user_name, address, sub_order_id, product_name, price FROM order_wide, UNNEST(sub_orders) AS so(sub_order_id, product_name, price)  collect #  The collect function collects elements into an Array. You can set fields.\u0026lt;field-name\u0026gt;.distinct=true to deduplicate elements. It only supports ARRAY type.\nmerge_map #  The merge_map function merge input maps. It only supports MAP type.\nTypes of cardinality sketches #  Paimon uses the Apache DataSketches library of stochastic streaming algorithms to implement sketch modules. The DataSketches library includes various types of sketches, each one designed to solve a different sort of problem. Paimon supports HyperLogLog (HLL) and Theta cardinality sketches.\nHyperLogLog #  The HyperLogLog (HLL) sketch aggregator is a very compact sketch algorithm for approximate distinct counting. You can also use the HLL aggregator to calculate a union of HLL sketches.\nTheta #  The Theta sketch is a sketch algorithm for approximate distinct counting with set operations. Theta sketches let you count the overlap between sets, so that you can compute the union, intersection, or set difference between sketch objects.\nChoosing a sketch type #  HLL and Theta sketches both support approximate distinct counting; however, the HLL sketch produces more accurate results and consumes less storage space. Theta sketches are more flexible but require significantly more memory.\nWhen choosing an approximation algorithm for your use case, consider the following:\nIf your use case entails distinct counting and merging sketch objects, use the HLL sketch. If you need to evaluate union, intersection, or difference set operations, use the Theta sketch. You cannot merge HLL sketches with Theta sketches.\nhll_sketch #  The hll_sketch function aggregates multiple serialized Sketch objects into a single Sketch. It supports VARBINARY data type.\nAn example:\nFlink -- source table CREATE TABLE VISITS ( id INT PRIMARY KEY NOT ENFORCED, user_id STRING ); -- agg table CREATE TABLE UV_AGG ( id INT PRIMARY KEY NOT ENFORCED, uv VARBINARY ) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;aggregation\u0026#39;, \u0026#39;fields.uv.aggregate-function\u0026#39; = \u0026#39;hll_sketch\u0026#39; ); -- Register the following class as a Flink function with the name \u0026#34;HLL_SKETCH\u0026#34; -- for example: create TEMPORARY function HLL_SKETCH as \u0026#39;HllSketchFunction\u0026#39;; -- which is used to transform input to sketch bytes array: -- -- public static class HllSketchFunction extends ScalarFunction { -- public byte[] eval(String user_id) { -- HllSketch hllSketch = new HllSketch(); -- hllSketch.update(user_id); -- return hllSketch.toCompactByteArray(); -- } -- } -- INSERT INTO UV_AGG SELECT id, HLL_SKETCH(user_id) FROM VISITS; -- Register the following class as a Flink function with the name \u0026#34;HLL_SKETCH_COUNT\u0026#34; -- for example: create TEMPORARY function HLL_SKETCH_COUNT as \u0026#39;HllSketchCountFunction\u0026#39;; -- which is used to get cardinality from sketch bytes array: -- -- public static class HllSketchCountFunction extends ScalarFunction { -- public Double eval(byte[] sketchBytes) { -- if (sketchBytes == null) { -- return 0d; -- } -- return HllSketch.heapify(sketchBytes).getEstimate(); -- } -- } -- -- Then we can get user cardinality based on the aggregated field. SELECT id, HLL_SKETCH_COUNT(UV) as uv FROM UV_AGG;  theta_sketch #  The theta_sketch function aggregates multiple serialized Sketch objects into a single Sketch. It supports VARBINARY data type.\nAn example:\nFlink -- source table CREATE TABLE VISITS ( id INT PRIMARY KEY NOT ENFORCED, user_id STRING ); -- agg table CREATE TABLE UV_AGG ( id INT PRIMARY KEY NOT ENFORCED, uv VARBINARY ) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;aggregation\u0026#39;, \u0026#39;fields.uv.aggregate-function\u0026#39; = \u0026#39;theta_sketch\u0026#39; ); -- Register the following class as a Flink function with the name \u0026#34;THETA_SKETCH\u0026#34; -- for example: create TEMPORARY function THETA_SKETCH as \u0026#39;ThetaSketchFunction\u0026#39;; -- which is used to transform input to sketch bytes array: -- -- public static class ThetaSketchFunction extends ScalarFunction { -- public byte[] eval(String user_id) { -- UpdateSketch updateSketch = UpdateSketch.builder().build(); -- updateSketch.update(user_id); -- return updateSketch.compact().toByteArray(); -- } -- } -- INSERT INTO UV_AGG SELECT id, THETA_SKETCH(user_id) FROM VISITS; -- Register the following class as a Flink function with the name \u0026#34;THETA_SKETCH_COUNT\u0026#34; -- for example: create TEMPORARY function THETA_SKETCH_COUNT as \u0026#39;ThetaSketchCountFunction\u0026#39;; -- which is used to get cardinality from sketch bytes array: -- -- public static class ThetaSketchCountFunction extends ScalarFunction { -- public Double eval(byte[] sketchBytes) { -- if (sketchBytes == null) { -- return 0d; -- } -- return Sketches.wrapCompactSketch(Memory.wrap(sketchBytes)).getEstimate(); -- } -- } -- -- Then we can get user cardinality based on the aggregated field. SELECT id, THETA_SKETCH_COUNT(UV) as uv FROM UV_AGG;  For streaming queries, aggregation merge engine must be used together with lookup or full-compaction changelog producer. (\u0026lsquo;input\u0026rsquo; changelog producer is also supported, but only returns input records.)  Retraction #  Only sum, product, collect, merge_map, nested_update, last_value and last_non_null_value supports retraction (UPDATE_BEFORE and DELETE), others aggregate functions do not support retraction. If you allow some functions to ignore retraction messages, you can configure: 'fields.${field_name}.ignore-retract'='true'.\nThe last_value and last_non_null_value just set field to null when accept retract messages.\nThe product will return null for retraction message when accumulator is null.\nThe collect and merge_map make a best-effort attempt to handle retraction messages, but the results are not guaranteed to be accurate. The following behaviors may occur when processing retraction messages:\n  It might fail to handle retraction messages if records are disordered. For example, the table uses collect, and the upstreams send +I['A', 'B'] and -U['A'] respectively. If the table receives -U['A'] first, it can do nothing; then it receives +I['A', 'B'], the merge result will be +I['A', 'B'] instead of +I['B'].\n  The retract message from one upstream will retract the result merged from multiple upstreams. For example, the table uses merge_map, and one upstream sends +I[1-\u0026gt;A], another upstream sends +I[1-\u0026gt;B], -D[1-\u0026gt;B] later. The table will merge two insert values to +I[1-\u0026gt;B] first, and then the -D[1-\u0026gt;B] will retract the whole result, so the final result is an empty map instead of +I[1-\u0026gt;A]\n  "});index.add({'id':36,'href':'/docs/1.1/program-api/catalog-api/','title':"Catalog API",'section':"Program API",'content':"Catalog API #  Create Database #  You can use the catalog to create databases. The created databases are persistence in the file system.\nimport org.apache.paimon.catalog.Catalog; public class CreateDatabase { public static void main(String[] args) { try { Catalog catalog = CreateCatalog.createFilesystemCatalog(); catalog.createDatabase(\u0026#34;my_db\u0026#34;, false); } catch (Catalog.DatabaseAlreadyExistException e) { // do something  } } } Determine Whether Database Exists #  You can use the catalog to determine whether the database exists\nimport org.apache.paimon.catalog.Catalog; public class DatabaseExists { public static void main(String[] args) { Catalog catalog = CreateCatalog.createFilesystemCatalog(); boolean exists = catalog.databaseExists(\u0026#34;my_db\u0026#34;); } } List Databases #  You can use the catalog to list databases.\nimport org.apache.paimon.catalog.Catalog; import java.util.List; public class ListDatabases { public static void main(String[] args) { Catalog catalog = CreateCatalog.createFilesystemCatalog(); List\u0026lt;String\u0026gt; databases = catalog.listDatabases(); } } Drop Database #  You can use the catalog to drop database.\nimport org.apache.paimon.catalog.Catalog; public class DropDatabase { public static void main(String[] args) { try { Catalog catalog = CreateCatalog.createFilesystemCatalog(); catalog.dropDatabase(\u0026#34;my_db\u0026#34;, false, true); } catch (Catalog.DatabaseNotEmptyException e) { // do something  } catch (Catalog.DatabaseNotExistException e) { // do something  } } } Alter Database #  You can use the catalog to alter database\u0026rsquo;s properties.(ps: only support hive and jdbc catalog)\nimport java.util.ArrayList; import org.apache.paimon.catalog.Catalog; public class AlterDatabase { public static void main(String[] args) { try { Catalog catalog = CreateCatalog.createHiveCatalog(); List\u0026lt;DatabaseChange\u0026gt; changes = new ArrayList\u0026lt;\u0026gt;(); changes.add(DatabaseChange.setProperty(\u0026#34;k1\u0026#34;, \u0026#34;v1\u0026#34;)); changes.add(DatabaseChange.removeProperty(\u0026#34;k2\u0026#34;)); catalog.alterDatabase(\u0026#34;my_db\u0026#34;, changes, true); } catch (Catalog.DatabaseNotExistException e) { // do something  } } } Determine Whether Table Exists #  You can use the catalog to determine whether the table exists\nimport org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; public class TableExists { public static void main(String[] args) { Identifier identifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;my_table\u0026#34;); Catalog catalog = CreateCatalog.createFilesystemCatalog(); boolean exists = catalog.tableExists(identifier); } } List Tables #  You can use the catalog to list tables.\nimport org.apache.paimon.catalog.Catalog; import java.util.List; public class ListTables { public static void main(String[] args) { try { Catalog catalog = CreateCatalog.createFilesystemCatalog(); List\u0026lt;String\u0026gt; tables = catalog.listTables(\u0026#34;my_db\u0026#34;); } catch (Catalog.DatabaseNotExistException e) { // do something  } } } Drop Table #  You can use the catalog to drop table.\nimport org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; public class DropTable { public static void main(String[] args) { Identifier identifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;my_table\u0026#34;); try { Catalog catalog = CreateCatalog.createFilesystemCatalog(); catalog.dropTable(identifier, false); } catch (Catalog.TableNotExistException e) { // do something  } } } Rename Table #  You can use the catalog to rename a table.\nimport org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; public class RenameTable { public static void main(String[] args) { Identifier fromTableIdentifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;my_table\u0026#34;); Identifier toTableIdentifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;test_table\u0026#34;); try { Catalog catalog = CreateCatalog.createFilesystemCatalog(); catalog.renameTable(fromTableIdentifier, toTableIdentifier, false); } catch (Catalog.TableAlreadyExistException e) { // do something  } catch (Catalog.TableNotExistException e) { // do something  } } } Alter Table #  You can use the catalog to alter a table, but you need to pay attention to the following points.\n Column %s cannot specify NOT NULL in the %s table. Cannot update partition column type in the table. Cannot change nullability of primary key. If the type of the column is nested row type, update the column type is not supported. Update column to nested row type is not supported.  import org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; import org.apache.paimon.schema.Schema; import org.apache.paimon.schema.SchemaChange; import org.apache.paimon.types.DataField; import org.apache.paimon.types.DataTypes; import com.google.common.collect.Lists; import java.util.Arrays; import java.util.HashMap; import java.util.Map; public class AlterTable { public static void main(String[] args) { Identifier identifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;my_table\u0026#34;); Map\u0026lt;String, String\u0026gt; options = new HashMap\u0026lt;\u0026gt;(); options.put(\u0026#34;bucket\u0026#34;, \u0026#34;4\u0026#34;); Catalog catalog = CreateCatalog.createFilesystemCatalog(); catalog.createDatabase(\u0026#34;my_db\u0026#34;, false); try { catalog.createTable( identifier, new Schema( Lists.newArrayList( new DataField(0, \u0026#34;col1\u0026#34;, DataTypes.STRING(), \u0026#34;field1\u0026#34;), new DataField(1, \u0026#34;col2\u0026#34;, DataTypes.STRING(), \u0026#34;field2\u0026#34;), new DataField(2, \u0026#34;col3\u0026#34;, DataTypes.STRING(), \u0026#34;field3\u0026#34;), new DataField(3, \u0026#34;col4\u0026#34;, DataTypes.BIGINT(), \u0026#34;field4\u0026#34;), new DataField( 4, \u0026#34;col5\u0026#34;, DataTypes.ROW( new DataField( 5, \u0026#34;f1\u0026#34;, DataTypes.STRING(), \u0026#34;f1\u0026#34;), new DataField( 6, \u0026#34;f2\u0026#34;, DataTypes.STRING(), \u0026#34;f2\u0026#34;), new DataField( 7, \u0026#34;f3\u0026#34;, DataTypes.STRING(), \u0026#34;f3\u0026#34;)), \u0026#34;field5\u0026#34;), new DataField(8, \u0026#34;col6\u0026#34;, DataTypes.STRING(), \u0026#34;field6\u0026#34;)), Lists.newArrayList(\u0026#34;col1\u0026#34;), // partition keys  Lists.newArrayList(\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;), // primary key  options, \u0026#34;table comment\u0026#34;), false); } catch (Catalog.TableAlreadyExistException e) { // do something  } catch (Catalog.DatabaseNotExistException e) { // do something  } // add option  SchemaChange addOption = SchemaChange.setOption(\u0026#34;snapshot.time-retained\u0026#34;, \u0026#34;2h\u0026#34;); // add column  SchemaChange addColumn = SchemaChange.addColumn(\u0026#34;col1_after\u0026#34;, DataTypes.STRING()); // add a column after col1  SchemaChange.Move after = SchemaChange.Move.after(\u0026#34;col1_after\u0026#34;, \u0026#34;col1\u0026#34;); SchemaChange addColumnAfterField = SchemaChange.addColumn(\u0026#34;col7\u0026#34;, DataTypes.STRING(), \u0026#34;\u0026#34;, after); // rename column  SchemaChange renameColumn = SchemaChange.renameColumn(\u0026#34;col3\u0026#34;, \u0026#34;col3_new_name\u0026#34;); // drop column  SchemaChange dropColumn = SchemaChange.dropColumn(\u0026#34;col6\u0026#34;); // update column comment  SchemaChange updateColumnComment = SchemaChange.updateColumnComment(new String[] {\u0026#34;col4\u0026#34;}, \u0026#34;col4 field\u0026#34;); // update nested column comment  SchemaChange updateNestedColumnComment = SchemaChange.updateColumnComment(new String[] {\u0026#34;col5\u0026#34;, \u0026#34;f1\u0026#34;}, \u0026#34;col5 f1 field\u0026#34;); // update column type  SchemaChange updateColumnType = SchemaChange.updateColumnType(\u0026#34;col4\u0026#34;, DataTypes.DOUBLE()); // update column position, you need to pass in a parameter of type Move  SchemaChange updateColumnPosition = SchemaChange.updateColumnPosition(SchemaChange.Move.first(\u0026#34;col4\u0026#34;)); // update column nullability  SchemaChange updateColumnNullability = SchemaChange.updateColumnNullability(new String[] {\u0026#34;col4\u0026#34;}, false); // update nested column nullability  SchemaChange updateNestedColumnNullability = SchemaChange.updateColumnNullability(new String[] {\u0026#34;col5\u0026#34;, \u0026#34;f2\u0026#34;}, false); SchemaChange[] schemaChanges = new SchemaChange[] { addOption, removeOption, addColumn, addColumnAfterField, renameColumn, dropColumn, updateColumnComment, updateNestedColumnComment, updateColumnType, updateColumnPosition, updateColumnNullability, updateNestedColumnNullability }; try { catalog.alterTable(identifier, Arrays.asList(schemaChanges), false); } catch (Catalog.TableNotExistException e) { // do something  } catch (Catalog.ColumnAlreadyExistException e) { // do something  } catch (Catalog.ColumnNotExistException e) { // do something  } } } "});index.add({'id':37,'href':'/docs/1.1/concepts/concurrency-control/','title':"Concurrency Control",'section':"Concepts",'content':"Concurrency Control #  Paimon supports optimistic concurrency for multiple concurrent write jobs.\nEach job writes data at its own pace and generates a new snapshot based on the current snapshot by applying incremental files (deleting or adding files) at the time of committing.\nThere may be two types of commit failures here:\n Snapshot conflict: the snapshot id has been preempted, the table has generated a new snapshot from another job. OK, let\u0026rsquo;s commit again. Files conflict: The file that this job wants to delete has been deleted by another jobs. At this point, the job can only fail. (For streaming jobs, it will fail and restart, intentionally failover once)  Snapshot conflict #  Paimon\u0026rsquo;s snapshot ID is unique, so as long as the job writes its snapshot file to the file system, it is considered successful.\nPaimon uses the file system\u0026rsquo;s renaming mechanism to commit snapshots, which is secure for HDFS as it ensures transactional and atomic renaming.\nBut for object storage such as OSS and S3, their 'RENAME' does not have atomic semantic. We need to configure Hive or jdbc metastore and enable 'lock.enabled' option for the catalog. Otherwise, there may be a chance of losing the snapshot.\nFiles conflict #  When Paimon commits a file deletion (which is only a logical deletion), it checks for conflicts with the latest snapshot. If there are conflicts (which means the file has been logically deleted), it can no longer continue on this commit node, so it can only intentionally trigger a failover to restart, and the job will retrieve the latest status from the filesystem in the hope of resolving this conflict.\nPaimon will ensure that there is no data loss or duplication here, but if two streaming jobs are writing at the same time and there are conflicts, you will see that they are constantly restarting, which is not a good thing.\nThe essence of conflict lies in deleting files (logically), and deleting files is born from compaction, so as long as we close the compaction of the writing job (Set \u0026lsquo;write-only\u0026rsquo; to true) and start a separate job to do the compaction work, everything is very good.\nSee dedicated compaction job for more info.\n"});index.add({'id':38,'href':'/docs/1.1/project/contributing/','title':"Contributing",'section':"Project",'content':"Contributing #  Apache Paimon is developed by an open and friendly community. Everybody is cordially welcome to join the community and contribute to Apache Paimon. There are several ways to interact with the community and contribute to Paimon including asking questions, filing bug reports, proposing new features, joining discussions on the mailing lists, contributing code or documentation, improving website, testing release candidates and writing corresponding blog etc.\nWhat do you want to do? #  Contributing to Apache Paimon goes beyond writing code for the project. Below, we list different opportunities to help the project:\n  Area Further information      Report Bug To report a problem with Paimon, open Paimon’s issues.  Please give detailed information about the problem you encountered and, if possible, add a description that helps to reproduce the problem.    Contribute Code Read the Code Contribution Guide    Code Reviews Read the Code Review Guide    Release Version Releasing a new Paimon version.    Support Users Reply to questions on the user mailing list, check the latest issues in Issues for tickets which are actually user questions.     Spread the Word About Paimon Organize or attend a Paimon Meetup, contribute to the Paimon blog, share your conference, meetup or blog post on the dev@paimon.apache.org mailing list.     Any other question? Reach out to the dev@paimon.apache.org mailing list to get help!     Code Contribution Guide #  Apache Paimon is maintained, improved, and extended by code contributions of volunteers. We welcome contributions to Paimon.\nPlease feel free to ask questions at any time. Either send a mail to the Dev mailing list or comment on the issue you are working on.\n .contribute-grid { margin-bottom: 10px; display: flex; flex-direction: column; margin-left: -2px; margin-right: -2px; } .contribute-grid .column { margin-top: 4px; padding: 0 2px; } @media only screen and (min-width: 480px) { .contribute-grid { flex-direction: row; flex-wrap: wrap; } .contribute-grid .column { flex: 0 0 50%; } .contribute-grid .column { margin-top: 4px; } } @media only screen and (min-width: 960px) { .contribute-grid { flex-wrap: nowrap; } .contribute-grid .column { flex: 0 0 25%; } } .contribute-grid .panel { height: 100%; margin: 0; } .contribute-grid .panel-body { padding: 10px; } .contribute-grid h2 { margin: 0 0 10px 0; padding: 0; display: flex; align-items: flex-start; } .contribute-grid .number { margin-right: 0.25em; font-size: 1.5em; line-height: 0.9; }  1Discuss Create an Issue or mailing list discussion and reach consensus\nTo request an issue, please note that it is not just a \"please assign it to me\", you need to explain your understanding of the issue, and your design, and if possible, you need to provide your POC code.\n   2Implement Create the Pull Request and the approach agreed upon in the issue.\n1.Only create the PR if you are assigned to the issue. 2.Please associate an issue (if any), e.g. fix #123. 3.Please enable the actions of your own clone project.\n   3Review Work with the reviewer.\n1.Make sure no unrelated or unnecessary reformatting changes are included. 2.Please ensure that the test passing. 3.Please don't resolve conversation.\n   4Merge A committer of Paimon checks if the contribution fulfills the requirements and merges the code to the codebase.\n    Code Review Guide #  Every review needs to check the following six aspects. We encourage to check these aspects in order, to avoid spending time on detailed code quality reviews when formal requirements are not met or there is no consensus in the community to accept the change.\n1. Is the Contribution Well-Described? #  Check whether the contribution is sufficiently well-described to support a good review. Trivial changes and fixes do not need a long description. If the implementation is exactly according to a prior discussion on issue or the development mailing list, only a short reference to that discussion is needed.\nIf the implementation is different from the agreed approach in the consensus discussion, a detailed description of the implementation is required for any further review of the contribution.\n2. Does the Contribution Need Attention from some Specific Committers? #  Some changes require attention and approval from specific committers.\nIf the pull request needs specific attention, one of the tagged committers/contributors should give the final approval.\n3. Is the Overall Code Quality Good, Meeting Standard we Want to Maintain in Paimon? #   Does the code follow the right software engineering practices? Is the code correct, robust, maintainable, testable? Are the changes performance aware, when changing a performance sensitive part? Are the changes sufficiently covered by tests? Are the tests executing fast? If dependencies have been changed, were the NOTICE files updated?  Code guidelines can be found in the Flink Java Code Style and Quality Guide.\n4. Are the documentation updated? #  If the pull request introduces a new feature, the feature should be documented.\nBecome a Committer #  When you have made enough contributions, you can be nominated as Paimon\u0026rsquo;s Committer. See Committer.\n"});index.add({'id':39,'href':'/docs/1.1/concepts/rest/dlf/','title':"DLF Token",'section':"RESTCatalog",'content':"DLF Token #  DLF (Data Lake Formation) building is a fully-managed platform for unified metadata and data storage and management, aiming to provide customers with functions such as metadata management, storage management, permission management, storage analysis, and storage optimization.\nDLF provides multiple authentication methods for different environments.\nThe 'warehouse' is your catalog instance name on the server, not the path.  Use the access key #  CREATE CATALOG `paimon-rest-catalog` WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;uri\u0026#39; = \u0026#39;\u0026lt;catalog server url\u0026gt;\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;rest\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;my_instance_name\u0026#39;, \u0026#39;token.provider\u0026#39; = \u0026#39;dlf\u0026#39;, \u0026#39;dlf.access-key-id\u0026#39;=\u0026#39;\u0026lt;access-key-id\u0026gt;\u0026#39;, \u0026#39;dlf.access-key-secret\u0026#39;=\u0026#39;\u0026lt;access-key-secret\u0026gt;\u0026#39;, ); You can grant specific permissions to a RAM user and use the RAM user\u0026rsquo;s access key for long-term access to your DLF resources. Compared to using the Alibaba Cloud account access key, accessing DLF resources with a RAM user access key is more secure.\nUse the STS temporary access token #  Through the STS service, you can generate temporary access tokens for users, allowing them to access DLF resources restricted by policies within the validity period.\nCREATE CATALOG `paimon-rest-catalog` WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;uri\u0026#39; = \u0026#39;\u0026lt;catalog server url\u0026gt;\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;rest\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;my_instance_name\u0026#39;, \u0026#39;token.provider\u0026#39; = \u0026#39;dlf\u0026#39;, \u0026#39;dlf.access-key-id\u0026#39;=\u0026#39;\u0026lt;access-key-id\u0026gt;\u0026#39;, \u0026#39;dlf.access-key-secret\u0026#39;=\u0026#39;\u0026lt;access-key-secret\u0026gt;\u0026#39;, \u0026#39;dlf.security-token\u0026#39;=\u0026#39;\u0026lt;security-token\u0026gt;\u0026#39; ); In some environments, temporary access token can be periodically refreshed by using a local file:\nCREATE CATALOG `paimon-rest-catalog` WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;uri\u0026#39; = \u0026#39;\u0026lt;catalog server url\u0026gt;\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;rest\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;my_instance_name\u0026#39;, \u0026#39;token.provider\u0026#39; = \u0026#39;dlf\u0026#39;, \u0026#39;dlf.token-path\u0026#39; = \u0026#39;my_token_path_in_disk\u0026#39; ); Use the STS token from aliyun ecs role #  An instance RAM role refers to a RAM role granted to an ECS instance. This RAM role is a standard service role with the trusted entity being the cloud server. By using an instance RAM role, it is possible to obtain temporary access token (STS Token) within the ECS instance without configuring an AccessKey.\nCREATE CATALOG `paimon-rest-catalog` WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;uri\u0026#39; = \u0026#39;\u0026lt;catalog server url\u0026gt;\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;rest\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;my_instance_name\u0026#39;, \u0026#39;token.provider\u0026#39; = \u0026#39;dlf\u0026#39;, \u0026#39;dlf.token-loader\u0026#39; = \u0026#39;ecs\u0026#39; -- optional, loader can obtain it through ecs metadata service  -- \u0026#39;dlf.token-ecs-role-name\u0026#39; = \u0026#39;my_ecs_role_name\u0026#39; ); "});index.add({'id':40,'href':'/docs/1.1/ecosystem/doris/','title':"Doris",'section':"Ecosystem",'content':"Doris #  This documentation is a guide for using Paimon in Doris.\n More details can be found in Apache Doris Website\n Version #  Paimon currently supports Apache Doris 2.0.6 and above.\nCreate Paimon Catalog #  Use CREATE CATALOG statement in Apache Doris to create Paimon Catalog.\nDoris support multi types of Paimon Catalogs. Here are some examples:\n-- HDFS based Paimon Catalog CREATE CATALOG `paimon_hdfs` PROPERTIES ( \u0026#34;type\u0026#34; = \u0026#34;paimon\u0026#34;, \u0026#34;warehouse\u0026#34; = \u0026#34;hdfs://172.21.0.1:8020/user/paimon\u0026#34;, \u0026#34;hadoop.username\u0026#34; = \u0026#34;hadoop\u0026#34; ); -- Aliyun OSS based Paimon Catalog CREATE CATALOG `paimon_oss` PROPERTIES ( \u0026#34;type\u0026#34; = \u0026#34;paimon\u0026#34;, \u0026#34;warehouse\u0026#34; = \u0026#34;oss://paimon-bucket/paimonoss\u0026#34;, \u0026#34;oss.endpoint\u0026#34; = \u0026#34;oss-cn-beijing.aliyuncs.com\u0026#34;, \u0026#34;oss.access_key\u0026#34; = \u0026#34;ak\u0026#34;, \u0026#34;oss.secret_key\u0026#34; = \u0026#34;sk\u0026#34; ); -- Hive Metastore based Paimon Catalog CREATE CATALOG `paimon_hms` PROPERTIES ( \u0026#34;type\u0026#34; = \u0026#34;paimon\u0026#34;, \u0026#34;paimon.catalog.type\u0026#34; = \u0026#34;hms\u0026#34;, \u0026#34;warehouse\u0026#34; = \u0026#34;hdfs://172.21.0.1:8020/user/zhangdong/paimon2\u0026#34;, \u0026#34;hive.metastore.uris\u0026#34; = \u0026#34;thrift://172.21.0.44:7004\u0026#34;, \u0026#34;hadoop.username\u0026#34; = \u0026#34;hadoop\u0026#34; ); -- Integrate with Aliyun DLF CREATE CATALOG paimon_dlf PROPERTIES ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;paimon.catalog.type\u0026#39; = \u0026#39;dlf\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;oss://paimon-bucket/paimonoss/\u0026#39;, \u0026#39;dlf.proxy.mode\u0026#39; = \u0026#39;DLF_ONLY\u0026#39;, \u0026#39;dlf.uid\u0026#39; = \u0026#39;xxxxx\u0026#39;, \u0026#39;dlf.region\u0026#39; = \u0026#39;cn-beijing\u0026#39;, \u0026#39;dlf.access_key\u0026#39; = \u0026#39;ak\u0026#39;, \u0026#39;dlf.secret_key\u0026#39; = \u0026#39;sk\u0026#39; ); See Apache Doris Website for more examples.\nAccess Paimon Catalog #    Query Paimon table with full qualified name\nSELECT * FROM paimon_hdfs.paimon_db.paimon_table;   Switch to Paimon Catalog and query\nSWITCH paimon_hdfs; USE paimon_db; SELECT * FROM paimon_table;   Query Optimization #    Read optimized for Primary Key Table\nDoris can utilize the Read optimized feature for Primary Key Table(release in Paimon 0.6), by reading base data files using native Parquet/ORC reader and delta file using JNI.\n  Deletion Vectors\nDoris(2.1.4+) natively supports Deletion Vectors(released in Paimon 0.8).\n  Doris to Paimon type mapping #    Doris Data Type Paimon Data Type Atomic Type     Boolean BooleanType true   TinyInt TinyIntType true   SmallInt SmallIntType true   Int IntType true   BigInt BigIntType true   Float FloatType true   Double DoubleType true   Varchar VarCharType true   Char CharType true   Binary VarBinaryType, BinaryType true   Decimal(precision, scale) DecimalType(precision, scale) true   Datetime TimestampType,LocalZonedTimestampType true   Date DateType true   Array ArrayType false   Map MapType false   Struct RowType false    "});index.add({'id':41,'href':'/docs/1.1/cdc-ingestion/kafka-cdc/','title':"Kafka CDC",'section':"CDC Ingestion",'content':"Kafka CDC #  Prepare Kafka Bundled Jar #  flink-sql-connector-kafka-*.jar Supported Formats #  Flink provides several Kafka CDC formats: Canal Json, Debezium Json, Debezium Avro, Ogg Json, Maxwell Json and Normal Json. If a message in a Kafka topic is a change event captured from another database using the Change Data Capture (CDC) tool, then you can use the Paimon Kafka CDC. Write the INSERT, UPDATE, DELETE messages parsed into the paimon table.\n  Formats Supported     Canal CDC True   Debezium CDC True   Maxwell CDC True   OGG CDC True   JSON True   aws-dms-json True   debezium-bson True    The JSON sources possibly missing some information. For example, Ogg and Maxwell format standards don\u0026rsquo;t contain field types; When you write JSON sources into Flink Kafka sink, it will only reserve data and row type and drop other information. The synchronization job will try best to handle the problem as follows:\n Usually, debezium-json contains \u0026lsquo;schema\u0026rsquo; field, from which Paimon will retrieve data types. Make sure your debezium json has this field, or Paimon will use \u0026lsquo;STRING\u0026rsquo; type. If missing field types, Paimon will use \u0026lsquo;STRING\u0026rsquo; type as default. If missing database name or table name, you cannot do database synchronization, but you can still do table synchronization. If missing primary keys, the job might create non primary key table. You can set primary keys when submit job in table synchronization.   Synchronizing Tables #  By using KafkaSyncTableAction in a Flink DataStream job or directly through flink run, users can synchronize one or multiple tables from Kafka\u0026rsquo;s one topic into one Paimon table.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  kafka_sync_table \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  [--partition_keys \u0026lt;partition_keys\u0026gt;] \\  [--primary_keys \u0026lt;primary-keys\u0026gt;] \\  [--type_mapping to-string] \\  [--computed_column \u0026lt;\u0026#39;column-name=expr-name(args[, ...])\u0026#39;\u0026gt; [--computed_column ...]] \\  [--kafka_conf \u0026lt;kafka-source-conf\u0026gt; [--kafka_conf \u0026lt;kafka-source-conf\u0026gt; ...]] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; ...]]    Configuration Description     --warehouse The path to Paimon warehouse.   --database The database name in Paimon catalog.   --table The Paimon table name.   --partition_keys The partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example \"dt,hh,mm\".   --primary_keys The primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example \"buyer_id,seller_id\".   --type_mapping It is used to specify how to map MySQL data type to Paimon type.\nSupported options:  \"tinyint1-not-bool\": maps MySQL TINYINT(1) to TINYINT instead of BOOLEAN. \"to-nullable\": ignores all NOT NULL constraints (except for primary keys). This is used to solve the problem that Flink cannot accept the MySQL 'ALTER TABLE ADD COLUMN column type NOT NULL DEFAULT x' operation.  \"to-string\": maps all MySQL types to STRING. \"char-to-string\": maps MySQL CHAR(length)/VARCHAR(length) types to STRING. \"longtext-to-bytes\": maps MySQL LONGTEXT types to BYTES. \"bigint-unsigned-to-bigint\": maps MySQL BIGINT UNSIGNED, BIGINT UNSIGNED ZEROFILL, SERIAL to BIGINT. You should ensure overflow won't occur when using this option. \"decimal-no-change\": Ignore decimal type change.     --computed_column The definitions of computed columns. The argument field is from Kafka topic's table field name. See here for a complete list of configurations.    --kafka_conf The configuration for Flink Kafka sources. Each configuration should be specified in the format `key=value`. `properties.bootstrap.servers`, `topic/topic-pattern`, `properties.group.id`, and `value.format` are required configurations, others are optional.See its document for a complete list of configurations.   --catalog_conf The configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.   --table_conf The configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.    If the Paimon table you specify does not exist, this action will automatically create the table. Its schema will be derived from all specified Kafka topic\u0026rsquo;s tables,it gets the earliest non-DDL data parsing schema from topic. If the Paimon table already exists, its schema will be compared against the schema of all specified Kafka topic\u0026rsquo;s tables.\nExample 1:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  kafka_sync_table \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --partition_keys pt \\  --primary_keys pt,uid \\  --computed_column \u0026#39;_year=year(age)\u0026#39; \\  --kafka_conf properties.bootstrap.servers=127.0.0.1:9020 \\  --kafka_conf topic=order \\  --kafka_conf properties.group.id=123456 \\  --kafka_conf value.format=canal-json \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --table_conf changelog-producer=input \\  --table_conf sink.parallelism=4 If the kafka topic doesn\u0026rsquo;t contain message when you start the synchronization job, you must manually create the table before submitting the job. You can define the partition keys and primary keys only, and the left columns will be added by the synchronization job.\nNOTE: In this case you shouldn\u0026rsquo;t use \u0026ndash;partition_keys or \u0026ndash;primary_keys, because those keys are defined when creating the table and can not be modified. Additionally, if you specified computed columns, you should also define all the argument columns used for computed columns.\nExample 2: If you want to synchronize a table which has primary key \u0026lsquo;id INT\u0026rsquo;, and you want to compute a partition key \u0026lsquo;part=date_format(create_time,yyyy-MM-dd)\u0026rsquo;, you can create a such table first (the other columns can be omitted):\nCREATE TABLE test_db.test_table ( id INT, -- primary key  create_time TIMESTAMP, -- the argument of computed column part  part STRING, -- partition key  PRIMARY KEY (id, part) NOT ENFORCED ) PARTITIONED BY (part); Then you can submit synchronization job:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  kafka_sync_table \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --computed_column \u0026#39;part=date_format(create_time,yyyy-MM-dd)\u0026#39; \\  ... (other conf) Example 3: For some append data (such as log data), it can be treated as special CDC data with only INSERT operation type, so you can use \u0026lsquo;format=json\u0026rsquo; to synchronize such data to the Paimon table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  kafka_sync_table \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --partition_keys pt \\  --computed_column \u0026#39;pt=date_format(event_tm, yyyyMMdd)\u0026#39; \\  --kafka_conf properties.bootstrap.servers=127.0.0.1:9020 \\  --kafka_conf topic=test_log \\  --kafka_conf properties.group.id=123456 \\  --kafka_conf value.format=json \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf sink.parallelism=4 Synchronizing Databases #  By using KafkaSyncDatabaseAction in a Flink DataStream job or directly through flink run, users can synchronize the multi topic or one topic into one Paimon database.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  kafka_sync_database \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  [--table_mapping \u0026lt;table-name\u0026gt;=\u0026lt;paimon-table-name1\u0026gt; [--table_mapping \u0026lt;table-name2\u0026gt;=\u0026lt;paimon-table-name2\u0026gt; ...]] \\  [--table_prefix \u0026lt;paimon-table-prefix\u0026gt;] \\  [--table_suffix \u0026lt;paimon-table-suffix\u0026gt;] \\  [--table_prefix_db \u0026lt;db-name1\u0026gt;=\u0026lt;table-prefix1\u0026gt; [--table_prefix_db \u0026lt;db-name2\u0026gt;=\u0026lt;table-prefix2\u0026gt; ...]] \\  [--table_suffix_db \u0026lt;db-name1\u0026gt;=\u0026lt;table-suffix1\u0026gt; [--table_suffix_db \u0026lt;db-name2\u0026gt;=\u0026lt;table-suffix2\u0026gt; ...]] \\  [--including_tables \u0026lt;table-name|name-regular-expr\u0026gt;] \\  [--excluding_tables \u0026lt;table-name|name-regular-expr\u0026gt;] \\  [--including_dbs \u0026lt;database-name|name-regular-expr\u0026gt;] \\  [--excluding_dbs \u0026lt;database-name|name-regular-expr\u0026gt;] \\  [--type_mapping to-string] \\  [--partition_keys \u0026lt;partition_keys\u0026gt;] \\  [--primary_keys \u0026lt;primary-keys\u0026gt;] \\  [--kafka_conf \u0026lt;kafka-source-conf\u0026gt; [--kafka_conf \u0026lt;kafka-source-conf\u0026gt; ...]] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; ...]]    Configuration Description     --warehouse The path to Paimon warehouse.   --database The database name in Paimon catalog.   --ignore_incompatible It is default false, in this case, if MySQL table name exists in Paimon and their schema is incompatible,an exception will be thrown. You can specify it to true explicitly to ignore the incompatible tables and exception.   --table_mapping The table name mapping between source database and Paimon. For example, if you want to synchronize a source table named \"test\" to a Paimon table named \"paimon_test\", you can specify \"--table_mapping test=paimon_test\". Multiple mappings could be specified with multiple \"--table_mapping\" options. \"--table_mapping\" has higher priority than \"--table_prefix\" and \"--table_suffix\".   --table_prefix The prefix of all Paimon tables to be synchronized except those specified by \"--table_mapping\" or \"--table_prefix_db\". For example, if you want all synchronized tables to have \"ods_\" as prefix, you can specify \"--table_prefix ods_\".   --table_suffix The suffix of all Paimon tables to be synchronized except those specified by \"--table_mapping\" or \"--table_suffix_db\". The usage is same as \"--table_prefix\".   --table_prefix_db The prefix of the Paimon tables to be synchronized from the specified db. For example, if you want to prefix the tables from db1 with \"ods_db1_\", you can specify \"--table_prefix_db db1=ods_db1_\". Multiple mappings could be specified multiple \"--table_prefix_db\" options. \"--table_prefix_db\" has higher priority than \"--table_prefix\".   --table_suffix_db The suffix of the Paimon tables to be synchronized from the specified db. The usage is same as \"--table_prefix_db\".   --including_tables It is used to specify which source tables are to be synchronized. You must use '|' to separate multiple tables.Because '|' is a special character, a comma is required, for example: 'a|b|c'.Regular expression is supported, for example, specifying \"--including_tables test|paimon.*\" means to synchronize table 'test' and all tables start with 'paimon'.   --excluding_tables It is used to specify which source tables are not to be synchronized. The usage is same as \"--including_tables\". \"--excluding_tables\" has higher priority than \"--including_tables\" if you specified both.   --including_dbs It is used to specify the databases within which the tables are to be synchronized. The usage is same as \"--including_tables\".   --excluding_dbs It is used to specify the databases within which the tables are not to be synchronized. The usage is same as \"--excluding_tables\". \"--excluding_dbs\" has higher priority than \"--including_dbs\" if you specified both.   --type_mapping It is used to specify how to map MySQL data type to Paimon type.\nSupported options:  \"tinyint1-not-bool\": maps MySQL TINYINT(1) to TINYINT instead of BOOLEAN. \"to-nullable\": ignores all NOT NULL constraints (except for primary keys). This is used to solve the problem that Flink cannot accept the MySQL 'ALTER TABLE ADD COLUMN column type NOT NULL DEFAULT x' operation.  \"to-string\": maps all MySQL types to STRING. \"char-to-string\": maps MySQL CHAR(length)/VARCHAR(length) types to STRING. \"longtext-to-bytes\": maps MySQL LONGTEXT types to BYTES. \"bigint-unsigned-to-bigint\": maps MySQL BIGINT UNSIGNED, BIGINT UNSIGNED ZEROFILL, SERIAL to BIGINT. You should ensure overflow won't occur when using this option. \"decimal-no-change\": Ignore decimal type change.     --computed_column The definitions of computed columns. The argument field is from Kafka topic's table field name. See here for a complete list of configurations.    --eager_init It is default false. If true, all relevant tables commiter will be initialized eagerly, which means those tables could be forced to create snapshot.    --partition_keys The partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example \"dt,hh,mm\". If the keys are not in source table, the sink table won't set partition keys.   --multiple_table_partition_keys The partition keys for each different Paimon table. If there are multiple partition keys, connect them with comma, for example --multiple_table_partition_keys tableName1=col1,col2.col3 --multiple_table_partition_keys tableName2=col4,col5.col6 --multiple_table_partition_keys tableName3=col7,col8.col9 If the keys are not in source table, the sink table won't set partition keys.   --primary_keys The primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example \"buyer_id,seller_id\". If the keys are not in source table, but the source table has primary keys, the sink table will use source table's primary keys. Otherwise, the sink table won't set primary keys.    --kafka_conf The configuration for Flink Kafka sources. Each configuration should be specified in the format `key=value`. `properties.bootstrap.servers`, `topic/topic-pattern`, `properties.group.id`, and `value.format` are required configurations, others are optional.See its document for a complete list of configurations.   --catalog_conf The configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.   --table_conf The configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.    This action will build a single combined sink for all tables. For each Kafka topic\u0026rsquo;s table to be synchronized, if the corresponding Paimon table does not exist, this action will automatically create the table, and its schema will be derived from all specified Kafka topic\u0026rsquo;s tables. If the Paimon table already exists and its schema is different from that parsed from Kafka record, this action will try to preform schema evolution.\nExample\nSynchronization from one Kafka topic to Paimon database.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  kafka_sync_database \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --kafka_conf properties.bootstrap.servers=127.0.0.1:9020 \\  --kafka_conf topic=order \\  --kafka_conf properties.group.id=123456 \\  --kafka_conf value.format=canal-json \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --table_conf changelog-producer=input \\  --table_conf sink.parallelism=4 Synchronization from multiple Kafka topics to Paimon database.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  kafka_sync_database \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --kafka_conf properties.bootstrap.servers=127.0.0.1:9020 \\  --kafka_conf topic=order\\;logistic_order\\;user \\  --kafka_conf properties.group.id=123456 \\  --kafka_conf value.format=canal-json \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --table_conf changelog-producer=input \\  --table_conf sink.parallelism=4 Additional kafka_config #  There are some useful options to build Flink Kafka Source, but they are not provided by flink-kafka-connector document. They are:\n  Key Default Type Description     schema.registry.url (none) String When configuring \"value.format=debezium-avro\" which requires using the Confluence schema registry model for Apache Avro serialization, you need to provide the schema registry URL.    "});index.add({'id':42,'href':'/docs/1.1/append-table/query-performance/','title':"Query Performance",'section':"Table w/o PK",'content':"Query Performance #  Data Skipping By Order #  Paimon by default records the maximum and minimum values of each field in the manifest file.\nIn the query, according to the WHERE condition of the query, together with the statistics in the manifest we can perform file filtering. If the filtering effect is good, the query that would have cost minutes will be accelerated to milliseconds to complete the execution.\nOften the data distribution is not always ideal for filtering, so can we sort the data by the field in WHERE condition? You can take a look at Flink COMPACT Action, Flink COMPACT Procedure or Spark COMPACT Procedure.\nData Skipping By File Index #  You can use file index too, it filters files by indexing on the reading side.\nCREATE TABLE \u0026lt;PAIMON_TABLE\u0026gt; (\u0026lt;COLUMN\u0026gt; \u0026lt;COLUMN_TYPE\u0026gt; , ...) WITH ( \u0026#39;file-index.bloom-filter.columns\u0026#39; = \u0026#39;c1,c2\u0026#39;, \u0026#39;file-index.bloom-filter.c1.items\u0026#39; = \u0026#39;200\u0026#39; ); Define file-index.bloom-filter.columns, Data file index is an external index file and Paimon will create its corresponding index file for each file. If the index file is too small, it will be stored directly in the manifest, otherwise in the directory of the data file. Each data file corresponds to an index file, which has a separate file definition and can contain different types of indexes with multiple columns.\nDifferent file indexes may be efficient in different scenarios. For example bloom filter may speed up query in point lookup scenario. Using a bitmap may consume more space but can result in greater accuracy.\nBloom Filter:\n file-index.bloom-filter.columns: specify the columns that need bloom filter index. file-index.bloom-filter.\u0026lt;column_name\u0026gt;.fpp to config false positive probability. file-index.bloom-filter.\u0026lt;column_name\u0026gt;.items to config the expected distinct items in one data file.  Bitmap:\n file-index.bitmap.columns: specify the columns that need bitmap index. See Index Bitmap.  Bit-Slice Index Bitmap\n file-index.bsi.columns: specify the columns that need bsi index.  More filter types will be supported\u0026hellip;\nIf you want to add file index to existing table, without any rewrite, you can use rewrite_file_index procedure. Before we use the procedure, you should config appropriate configurations in target table. You can use ALTER clause to config file-index.\u0026lt;filter-type\u0026gt;.columns to the table.\nHow to invoke: see flink procedures\n"});index.add({'id':43,'href':'/docs/1.1/concepts/spec/snapshot/','title':"Snapshot",'section':"Specification",'content':"Snapshot #  Each commit generates a snapshot file, and the version of the snapshot file starts from 1 and must be continuous. EARLIEST and LATEST are hint files at the beginning and end of the snapshot list, and they can be inaccurate. When hint files are inaccurate, the read will scan all snapshot files to determine the beginning and end.\nwarehouse └── default.db └── my_table ├── snapshot ├── EARLIEST ├── LATEST ├── snapshot-1 ├── snapshot-2 └── snapshot-3 Writing commit will preempt the next snapshot id, and once the snapshot file is successfully written, this commit will be visible.\nSnapshot File is JSON, it includes:\n version: Snapshot file version, current is 3. id: snapshot id, same to file name. schemaId: the corresponding schema version for this commit. baseManifestList: a manifest list recording all changes from the previous snapshots. deltaManifestList: a manifest list recording all new changes occurred in this snapshot. changelogManifestList: a manifest list recording all changelog produced in this snapshot, null if no changelog is produced. indexManifest: a manifest recording all index files of this table, null if no table index file. commitUser: usually generated by UUID, it is used for recovery of streaming writes, one stream write job with one user. commitIdentifier: transaction id corresponding to streaming write, each transaction may result in multiple commits for different commitKinds. commitKind: type of changes in this snapshot, including append, compact, overwrite and analyze. timeMillis: commit time millis. logOffsets: commit log offsets. totalRecordCount: record count of all changes occurred in this snapshot. deltaRecordCount: record count of all new changes occurred in this snapshot. changelogRecordCount: record count of all changelog produced in this snapshot. watermark: watermark for input records, from Flink watermark mechanism, Long.MIN_VALUE if there is no watermark. statistics: stats file name for statistics of this table.  "});index.add({'id':44,'href':'/docs/1.1/primary-key-table/table-mode/','title':"Table Mode",'section':"Table with PK",'content':"Table Mode #  The file structure of the primary key table is roughly shown in the above figure. The table or partition contains multiple buckets, and each bucket is a separate LSM tree structure that contains multiple files.\nThe writing process of LSM is roughly as follows: Flink checkpoint flush L0 files, and trigger a compaction as needed to merge the data. According to the different processing ways during writing, there are three modes:\n MOR (Merge On Read): Default mode, only minor compactions are performed, and merging are required for reading. COW (Copy On Write): Using 'full-compaction.delta-commits' = '1', full compaction will be synchronized, which means the merge is completed on write. MOW (Merge On Write): Using 'deletion-vectors.enabled' = 'true', in writing phase, LSM will be queried to generate the deletion vector file for the data file, which directly filters out unnecessary lines during reading.  The Merge On Write mode is recommended for general primary key tables (merge-engine is default deduplicate).\nMerge On Read #  MOR is the default mode of primary key table.\nWhen the mode is MOR, it is necessary to merge all files for reading, as all files are ordered and undergo multi way merging, which includes a comparison calculation of the primary key.\nThere is an obvious issue here, where a single LSM tree can only have a single thread to read, so the read parallelism is limited. If the amount of data in the bucket is too large, it can lead to poor read performance. So in order to read performance, it is recommended to analyze the query requirements table and set the data volume in the bucket to be between 200MB and 1GB. But if the bucket is too small, there will be a lot of small file reads and writes, causing pressure on the file system.\nIn addition, due to the merging process, Filter based data skipping cannot be performed on non primary key columns, otherwise new data will be filtered out, resulting in incorrect old data.\n Write performance: very good. Read performance: not so good.  Copy On Write #  ALTER TABLE orders SET (\u0026#39;full-compaction.delta-commits\u0026#39; = \u0026#39;1\u0026#39;); Set full-compaction.delta-commits to 1, which means that every write will be fully merged, and all data will be merged to the highest level. When reading, merging is not necessary at this time, and the reading performance is the highest. But every write requires full merging, and write amplification is very severe.\n Write performance: very bad. Read performance: very good.  Merge On Write #  ALTER TABLE orders SET (\u0026#39;deletion-vectors.enabled\u0026#39; = \u0026#39;true\u0026#39;); Thanks to Paimon\u0026rsquo;s LSM structure, it has the ability to be queried by primary key. We can generate deletion vectors files when writing, representing which data in the file has been deleted. This directly filters out unnecessary rows during reading, which is equivalent to merging and does not affect reading performance.\nA simple example just like:\nUpdates data by deleting old record first and then adding new one.\n Write performance: good. Read performance: good.  Visibility guarantee: Tables in deletion vectors mode, the files with level 0 will only be visible after compaction. So by default, compaction is synchronous, and if asynchronous is turned on, there may be delays in the data.  MOR Read Optimized #  If you don\u0026rsquo;t want to use Deletion Vectors mode, you want to query fast enough in MOR mode, but can only find older data, you can also:\n Configure \u0026lsquo;compaction.optimization-interval\u0026rsquo; when writing data. Query from read-optimized system table. Reading from results of optimized files avoids merging records with the same key, thus improving reading performance.  You can flexibly balance query performance and data latency when reading.\n"});index.add({'id':45,'href':'/docs/1.1/append-table/','title':"Table w/o PK",'section':"Apache Paimon",'content':""});index.add({'id':46,'href':'/docs/1.1/maintenance/write-performance/','title':"Write Performance",'section':"Maintenance",'content':"Write Performance #  Paimon\u0026rsquo;s write performance is closely related to checkpoint, so if you need greater write throughput:\n Flink Configuration ('flink-conf.yaml'/'config.yaml' or SET in SQL): Increase the checkpoint interval ('execution.checkpointing.interval'), increase max concurrent checkpoints to 3 ('execution.checkpointing.max-concurrent-checkpoints'), or just use batch mode. Increase write-buffer-size. Enable write-buffer-spillable. Rescale bucket number if you are using Fixed-Bucket mode.  Option 'changelog-producer' = 'lookup' or 'full-compaction', and option 'full-compaction.delta-commits' have a large impact on write performance, if it is a snapshot / full synchronization phase you can unset these options and then enable them again in the incremental phase.\nIf you find that the input of the job shows a jagged pattern in the case of backpressure, it may be imbalanced work nodes. You can consider turning on Asynchronous Compaction to observe if the throughput is increased.\nParallelism #  It is recommended that the parallelism of sink should be less than or equal to the number of buckets, preferably equal. You can control the parallelism of the sink with the sink.parallelism table property.\n  Option Required Default Type Description     sink.parallelism No (none) Integer Defines the parallelism of the sink operator. By default, the parallelism is determined by the framework using the same parallelism of the upstream chained operator.    Local Merging #  If your job suffers from primary key data skew (for example, you want to count the number of views for each page in a website, and some particular pages are very popular among the users), you can set 'local-merge-buffer-size' so that input records will be buffered and merged before they\u0026rsquo;re shuffled by bucket and written into sink. This is particularly useful when the same primary key is updated frequently between snapshots.\nThe buffer will be flushed when it is full. We recommend starting with 64 mb when you are faced with data skew but don\u0026rsquo;t know where to start adjusting buffer size.\n(Currently, Local merging not works for CDC ingestion)\nFile Format #  If you want to achieve ultimate compaction performance, you can consider using row storage file format AVRO.\n The advantage is that you can achieve high write throughput and compaction performance. The disadvantage is that your analysis queries will be slow, and the biggest problem with row storage is that it does not have the query projection. For example, if the table have 100 columns but only query a few columns, the IO of row storage cannot be ignored. Additionally, compression efficiency will decrease and storage costs will increase.  This a tradeoff.\nEnable row storage through the following options:\nfile.format = avro metadata.stats-mode = none The collection of statistical information for row storage is a bit expensive, so I suggest turning off statistical information as well.\nIf you don\u0026rsquo;t want to modify all files to Avro format, at least you can consider modifying the files in the previous layers to Avro format. You can use 'file.format.per.level' = '0:avro,1:avro' to specify the files in the first two layers to be in Avro format.\nFile Compression #  By default, Paimon uses zstd with level 1, you can modify the compression algorithm:\n'file.compression.zstd-level': Default zstd level is 1. For higher compression rates, it can be configured to 9, but the read and write speed will significantly decrease.\nStability #  If there are too few buckets or resources, full-compaction may cause the checkpoint timeout, Flink\u0026rsquo;s default checkpoint timeout is 10 minutes.\nIf you expect stability even in this case, you can turn up the checkpoint timeout, for example:\nexecution.checkpointing.timeout = 60 min Write Initialize #  In the initialization of write, the writer of the bucket needs to read all historical files. If there is a bottleneck here (For example, writing a large number of partitions simultaneously), you can use write-manifest-cache to cache the read manifest data to accelerate initialization.\nWrite Memory #  There are three main places in Paimon writer that takes up memory:\n Writer\u0026rsquo;s memory buffer, shared and preempted by all writers of a single task. This memory value can be adjusted by the write-buffer-size table property. Memory consumed when merging several sorted runs for compaction. Can be adjusted by the num-sorted-run.compaction-trigger option to change the number of sorted runs to be merged. If the row is very large, reading too many lines of data at once will consume a lot of memory when making a compaction. Reducing the read.batch-size option can alleviate the impact of this case. The memory consumed by writing columnar ORC file. Decreasing the orc.write.batch-size option can reduce the consumption of memory for ORC format. If files are automatically compaction in the write task, dictionaries for certain large columns can significantly consume memory during compaction.  To disable dictionary encoding for all fields in Parquet format, set 'parquet.enable.dictionary'= 'false'. To disable dictionary encoding for all fields in ORC format, set orc.dictionary.key.threshold='0'. Additionally,set orc.column.encoding.direct='field1,field2' to disable dictionary encoding for specific columns.    If your Flink job does not rely on state, please avoid using managed memory, which you can control with the following Flink parameter:\ntaskmanager.memory.managed.size=1m Or you can use Flink managed memory for your write buffer to avoid OOM, set table property:\nsink.use-managed-memory-allocator=true Commit Memory #  Committer node may use a large memory if the amount of data written to the table is particularly large, OOM may occur if the memory is too small. In this case, you need to increase the Committer heap memory, but you may not want to increase the memory of Flink\u0026rsquo;s TaskManager uniformly, which may lead to a waste of memory.\nYou can use fine-grained-resource-management of Flink to increase committer heap memory only:\n Configure Flink Configuration cluster.fine-grained-resource-management.enabled: true. (This is default after Flink 1.18) Configure Paimon Table Options: sink.committer-memory, for example 300 MB, depends on your TaskManager. (sink.committer-cpu is also supported) If you use Flink batch job write data into Paimon or run dedicated compaction, Configure Flink Configuration fine-grained.shuffle-mode.all-blocking: true.  "});index.add({'id':47,'href':'/docs/1.1/concepts/catalog/','title':"Catalog",'section':"Concepts",'content':"Catalog #  Paimon provides a Catalog abstraction to manage the table of contents and metadata. The Catalog abstraction provides a series of ways to help you better integrate with computing engines. We always recommend that you use Catalog to access the Paimon table.\nCatalogs #  Paimon catalogs currently support four types of metastores:\n filesystem metastore (default), which stores both metadata and table files in filesystems. hive metastore, which additionally stores metadata in Hive metastore. Users can directly access the tables from Hive. jdbc metastore, which additionally stores metadata in relational databases such as MySQL, Postgres, etc. rest metastore, which is designed to provide a lightweight way to access any catalog backend from a single client.  Filesystem Catalog #  Metadata and table files are stored under hdfs:///path/to/warehouse.\n-- Flink SQL CREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs:///path/to/warehouse\u0026#39; ); Hive Catalog #  By using Paimon Hive catalog, changes to the catalog will directly affect the corresponding Hive metastore. Tables created in such catalog can also be accessed directly from Hive. Metadata and table files are stored under hdfs:///path/to/warehouse. In addition, schema is also stored in Hive metastore.\n-- Flink SQL CREATE CATALOG my_hive WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;hive\u0026#39;, -- \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs:///path/to/warehouse\u0026#39;, default use \u0026#39;hive.metastore.warehouse.dir\u0026#39; in HiveConf ); By default, Paimon does not synchronize newly created partitions into Hive metastore. Users will see an unpartitioned table in Hive. Partition push-down will be carried out by filter push-down instead.\nIf you want to see a partitioned table in Hive and also synchronize newly created partitions into Hive metastore, please set the table option metastore.partitioned-table to true.\nJDBC Catalog #  By using the Paimon JDBC catalog, changes to the catalog will be directly stored in relational databases such as SQLite, MySQL, postgres, etc.\n-- Flink SQL CREATE CATALOG my_jdbc WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;jdbc\u0026#39;, \u0026#39;uri\u0026#39; = \u0026#39;jdbc:mysql://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;databaseName\u0026gt;\u0026#39;, \u0026#39;jdbc.user\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;jdbc.password\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;catalog-key\u0026#39;=\u0026#39;jdbc\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs:///path/to/warehouse\u0026#39; ); REST Catalog #  By using the Paimon REST catalog, changes to the catalog will be directly stored in a remote catalog server which exposed through REST API. See Paimon REST Catalog.\n"});index.add({'id':48,'href':'/docs/1.1/project/committer/','title':"Committer",'section':"Project",'content':"Committer #  Become a Committer #  How to become a committer #  There is no strict protocol for becoming a committer. Candidates for new committers are typically people that are active contributors and community members. Candidates are suggested by current committers or PPMC members, and voted upon by the PPMC.\nIf you would like to become a committer, you should engage with the community and start contributing to Apache Paimon in any of the above ways. You might also want to talk to other committers and ask for their advice and guidance.\n  Community contributions include helping to answer user questions on the mailing list, verifying release candidates, giving talks, organizing community events, and other forms of evangelism and community building. The \u0026ldquo;Apache Way\u0026rdquo; has a strong focus on the project community, and committers can be recognized for outstanding community contributions even without any code contributions.\n  Code/technology contributions include contributed pull requests (patches), design discussions, reviews, testing, and other help in identifying and fixing bugs. Especially constructive and high quality design discussions, as well as helping other contributors, are strong indicators.\n  Identify promising candidates #  While the prior points give ways to identify promising candidates, the following are \u0026ldquo;must haves\u0026rdquo; for any committer candidate:\n  Being community minded: The candidate understands the meritocratic principles of community management. They do not always optimize for as much as possible personal contribution, but will help and empower others where it makes sense.\n  We trust that a committer candidate will use their write access to the repositories responsibly, and if in doubt, conservatively. It is important that committers are aware of what they know and what they don\u0026rsquo;t know. In doubt, committers should ask for a second pair of eyes rather than commit to parts that they are not well familiar with.\n  They have shown to be respectful towards other community members and constructive in discussions.\n  Committer Rights #  JetBrains provides a free license to Apache Committers, allowing them to access all JetBrains IDEs, such as IntelliJ IDEA, PyCharm, and other desktop tools.\nPlease use your @apache.org email address to All Products Packs for Apache committers.\n"});index.add({'id':49,'href':'/docs/1.1/maintenance/dedicated-compaction/','title':"Dedicated Compaction",'section':"Maintenance",'content':"Dedicated Compaction #  Paimon\u0026rsquo;s snapshot management supports writing with multiple writers.\nFor S3-like object store, its 'RENAME' does not have atomic semantic. We need to configure Hive metastore and enable 'lock.enabled' option for the catalog.  By default, Paimon supports concurrent writing to different partitions. A recommended mode is that streaming job writes records to Paimon\u0026rsquo;s latest partition, Simultaneously batch job (overwrite) writes records to the historical partition.\nSo far, everything works very well, but if you need multiple writers to write records to the same partition, it will be a bit more complicated. For example, you don\u0026rsquo;t want to use UNION ALL, you have multiple streaming jobs to write records to a 'partial-update' table. Please refer to the 'Dedicated Compaction Job' below.\nDedicated Compaction Job #  By default, Paimon writers will perform compaction as needed during writing records. This is sufficient for most use cases.\nCompaction will mark some data files as \u0026ldquo;deleted\u0026rdquo; (not really deleted, see expiring snapshots for more info). If multiple writers mark the same file, a conflict will occur when committing the changes. Paimon will automatically resolve the conflict, but this may result in job restarts.\nTo avoid these downsides, users can also choose to skip compactions in writers, and run a dedicated job only for compaction. As compactions are performed only by the dedicated job, writers can continuously write records without pausing and no conflicts will ever occur.\nTo skip compactions in writers, set the following table property to true.\n  Option Required Default Type Description     write-only No false Boolean If set to true, compactions and snapshot expiration will be skipped. This option is used along with dedicated compact jobs.    To run a dedicated job for compaction, follow these instructions.\nFlink SQL Run the following sql:\nCALL sys.compact( `table` =\u0026gt; \u0026#39;default.T\u0026#39;, partitions =\u0026gt; \u0026#39;p=0\u0026#39;, options =\u0026gt; \u0026#39;sink.parallelism=4\u0026#39;, `where` =\u0026gt; \u0026#39;dt\u0026gt;10 and h\u0026lt;20\u0026#39; ); Flink Action Jar Run the following command to submit a compaction job for the table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  compact \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  [--partition \u0026lt;partition-name\u0026gt;] \\  [--compact_strategy \u0026lt;minor / full\u0026gt;] \\  [--table_conf \u0026lt;table_conf\u0026gt;] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] Example: compact table\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  compact \\  --warehouse s3:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --partition dt=20221126,hh=08 \\  --partition dt=20221127,hh=09 \\  --table_conf sink.parallelism=10 \\  --compact_strategy minor \\  --catalog_conf s3.endpoint=https://****.com \\  --catalog_conf s3.access-key=***** \\  --catalog_conf s3.secret-key=*****  --compact_strategy Determines how to pick files to be merged, the default is determined by the runtime execution mode, streaming-mode use minor strategy and batch-mode use full strategy.  full : Only supports batch mode. All files will be selected for merging. minor : Pick the set of files that need to be merged based on specified conditions.    You can use -D execution.runtime-mode=batch or -yD execution.runtime-mode=batch (for the ON-YARN scenario) to control batch or streaming mode. If you submit a batch job, all current table files will be compacted. If you submit a streaming job, the job will continuously monitor new changes to the table and perform compactions as needed.\nFor more usage of the compact action, see\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  compact --help  Similarly, the default is synchronous compaction, which may cause checkpoint timeouts. You can configure table_conf to use Asynchronous Compaction.  Database Compaction Job #  You can run the following command to submit a compaction job for multiple database.\nFlink SQL Run the following sql:\nCALL sys.compact_database( including_databases =\u0026gt; \u0026#39;includingDatabases\u0026#39;, mode =\u0026gt; \u0026#39;mode\u0026#39;, including_tables =\u0026gt; \u0026#39;includingTables\u0026#39;, excluding_tables =\u0026gt; \u0026#39;excludingTables\u0026#39;, table_options =\u0026gt; \u0026#39;tableOptions\u0026#39; ) -- example CALL sys.compact_database( including_databases =\u0026gt; \u0026#39;db1|db2\u0026#39;, mode =\u0026gt; \u0026#39;combined\u0026#39;, including_tables =\u0026gt; \u0026#39;table_.*\u0026#39;, excluding_tables =\u0026gt; \u0026#39;ignore\u0026#39;, table_options =\u0026gt; \u0026#39;sink.parallelism=4\u0026#39; ) Flink Action Jar \u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  compact_database \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --including_databases \u0026lt;database-name|name-regular-expr\u0026gt; \\  [--including_tables \u0026lt;paimon-table-name|name-regular-expr\u0026gt;] \\  [--excluding_tables \u0026lt;paimon-table-name|name-regular-expr\u0026gt;] \\  [--mode \u0026lt;compact-mode\u0026gt;] \\  [--compact_strategy \u0026lt;minor / full\u0026gt;] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table_conf \u0026lt;paimon-table_conf\u0026gt; [--table_conf \u0026lt;paimon-table_conf\u0026gt; ...]]  --including_databases is used to specify which database is to be compacted. In compact mode, you need to specify a database name, in compact_database mode, you could specify multiple database, regular expression is supported. --including_tables is used to specify which source tables are to be compacted, you must use \u0026lsquo;|\u0026rsquo; to separate multiple tables, the format is databaseName.tableName, regular expression is supported. For example, specifying \u0026ldquo;\u0026ndash;including_tables db1.t1|db2.+\u0026rdquo; means to compact table \u0026lsquo;db1.t1\u0026rsquo; and all tables in the db2 database. --excluding_tables is used to specify which source tables are not to be compacted. The usage is same as \u0026ldquo;\u0026ndash;including_tables\u0026rdquo;. \u0026ldquo;\u0026ndash;excluding_tables\u0026rdquo; has higher priority than \u0026ldquo;\u0026ndash;including_tables\u0026rdquo; if you specified both. --mode is used to specify compaction mode. Possible values:  \u0026ldquo;divided\u0026rdquo; (the default mode if you haven\u0026rsquo;t specified one): start a sink for each table, the compaction of the new table requires restarting the job. \u0026ldquo;combined\u0026rdquo;: start a single combined sink for all tables, the new table will be automatically compacted.   --catalog_conf is the configuration for Paimon catalog. Each configuration should be specified in the format key=value. See here for a complete list of catalog configurations. --table_conf is the configuration for compaction. Each configuration should be specified in the format key=value. Pivotal configuration is listed below:     Key Default Type Description     continuous.discovery-interval 10 s Duration The discovery interval of continuous reading.   sink.parallelism (none) Integer Defines a custom parallelism for the sink. By default, if this option is not defined, the planner will derive the parallelism for each statement individually by also considering the global configuration.    You can use -D execution.runtime-mode=batch to control batch or streaming mode. If you submit a batch job, all current table files will be compacted. If you submit a streaming job, the job will continuously monitor new changes to the table and perform compactions as needed.\nIf you only want to submit the compaction job and don\u0026rsquo;t want to wait until the job is done, you should submit in detached mode.  You can set --mode combined to enable compacting newly added tables without restarting job.  Example1: compact database\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  compact_database \\  --warehouse s3:///path/to/warehouse \\  --including_databases test_db \\  --catalog_conf s3.endpoint=https://****.com \\  --catalog_conf s3.access-key=***** \\  --catalog_conf s3.secret-key=***** Example2: compact database in combined mode\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  compact_database \\  --warehouse s3:///path/to/warehouse \\  --including_databases test_db \\  --mode combined \\  --catalog_conf s3.endpoint=https://****.com \\  --catalog_conf s3.access-key=***** \\  --catalog_conf s3.secret-key=***** \\  --table_conf continuous.discovery-interval=***** For more usage of the compact_database action, see\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  compact_database --help  Sort Compact #  If your table is configured with dynamic bucket primary key table or append table , you can trigger a compact with specified column sort to speed up queries.\nFlink SQL Run the following sql:\n-- sort compact table CALL sys.compact(`table` =\u0026gt; \u0026#39;default.T\u0026#39;, order_strategy =\u0026gt; \u0026#39;zorder\u0026#39;, order_by =\u0026gt; \u0026#39;a,b\u0026#39;) Flink Action Jar \u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  -D execution.runtime-mode=batch \\  /path/to/paimon-flink-action-1.1.1.jar \\  compact \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  --order_strategy \u0026lt;orderType\u0026gt; \\  --order_by \u0026lt;col1,col2,...\u0026gt; \\  [--partition \u0026lt;partition-name\u0026gt;] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table_conf \u0026lt;paimon-table-dynamic-conf\u0026gt; [--table_conf \u0026lt;paimon-table-dynamic-conf\u0026gt;] ...] There are two new configuration in Sort Compact\n  Configuration Description     --order_strategy the order strategy now support \"zorder\" and \"hilbert\" and \"order\". For example: --order_strategy zorder   --order_by Specify the order columns. For example: --order_by col0, col1    The sort parallelism is the same as the sink parallelism, you can dynamically specify it by add conf --table_conf sink.parallelism=\u0026lt;value\u0026gt;.\n Historical Partition Compact #  You can run the following command to submit a compaction job for partition which has not received any new data for a period of time. Small files in those partitions will be full compacted.\nThis feature now is only used in batch mode.  For Table #  This is for one table. Flink SQL Run the following sql:\n-- history partition compact table CALL sys.compact(`table` =\u0026gt; \u0026#39;default.T\u0026#39;, \u0026#39;partition_idle_time\u0026#39; =\u0026gt; \u0026#39;1 d\u0026#39;) Flink Action Jar \u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  -D execution.runtime-mode=batch \\  /path/to/paimon-flink-action-1.1.1.jar \\  compact \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  --partition_idle_time \u0026lt;partition-idle-time\u0026gt; \\  [--partition \u0026lt;partition-name\u0026gt;] \\  [--compact_strategy \u0026lt;minor / full\u0026gt;] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table_conf \u0026lt;paimon-table-dynamic-conf\u0026gt; [--table_conf \u0026lt;paimon-table-dynamic-conf\u0026gt;] ...] There are one new configuration in Historical Partition Compact\n --partition_idle_time: this is used to do a full compaction for partition which had not received any new data for \u0026lsquo;partition_idle_time\u0026rsquo;. And only these partitions will be compacted.   For Databases #  This is for multiple tables in different databases. Flink SQL Run the following sql:\n-- history partition compact table CALL sys.compact_database( including_databases =\u0026gt; \u0026#39;includingDatabases\u0026#39;, mode =\u0026gt; \u0026#39;mode\u0026#39;, including_tables =\u0026gt; \u0026#39;includingTables\u0026#39;, excluding_tables =\u0026gt; \u0026#39;excludingTables\u0026#39;, table_options =\u0026gt; \u0026#39;tableOptions\u0026#39;, partition_idle_time =\u0026gt; \u0026#39;partition_idle_time\u0026#39; ); Example: compact historical partitions for tables in database\n-- history partition compact table CALL sys.compact_database( includingDatabases =\u0026gt; \u0026#39;test_db\u0026#39;, mode =\u0026gt; \u0026#39;combined\u0026#39;, partition_idle_time =\u0026gt; \u0026#39;1 d\u0026#39; ); Flink Action Jar \u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  -D execution.runtime-mode=batch \\  /path/to/paimon-flink-action-1.1.1.jar \\  compact_database \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --including_databases \u0026lt;database-name|name-regular-expr\u0026gt; \\  --partition_idle_time \u0026lt;partition-idle-time\u0026gt; \\  [--including_tables \u0026lt;paimon-table-name|name-regular-expr\u0026gt;] \\  [--excluding_tables \u0026lt;paimon-table-name|name-regular-expr\u0026gt;] \\  [--mode \u0026lt;compact-mode\u0026gt;] \\  [--compact_strategy \u0026lt;minor / full\u0026gt;] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table_conf \u0026lt;paimon-table_conf\u0026gt; [--table_conf \u0026lt;paimon-table_conf\u0026gt; ...]] Example: compact historical partitions for tables in database\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  compact_database \\  --warehouse s3:///path/to/warehouse \\  --including_databases test_db \\  --partition_idle_time 1d \\  --catalog_conf s3.endpoint=https://****.com \\  --catalog_conf s3.access-key=***** \\  --catalog_conf s3.secret-key=*****  "});index.add({'id':50,'href':'/docs/1.1/primary-key-table/merge-engine/first-row/','title':"First Row",'section':"Merge Engine",'content':"First Row #  By specifying 'merge-engine' = 'first-row', users can keep the first row of the same primary key. It differs from the deduplicate merge engine that in the first-row merge engine, it will generate insert only changelog.\nfirst-row merge engine only supports none and lookup changelog producer. For streaming queries must be used with the lookup changelog producer.   You can not specify sequence.field. Not accept DELETE and UPDATE_BEFORE message. You can config ignore-delete to ignore these two kinds records. Visibility guarantee: Tables with First Row engine, the files with level 0 will only be visible after compaction. So by default, compaction is synchronous, and if asynchronous is turned on, there may be delays in the data.   This is of great help in replacing log deduplication in streaming computation.\n"});index.add({'id':51,'href':'/docs/1.1/ecosystem/hive/','title':"Hive",'section':"Ecosystem",'content':"Hive #  This documentation is a guide for using Paimon in Hive.\nVersion #  Paimon currently supports Hive 3.1, 2.3, 2.2, 2.1 and 2.1-cdh-6.3.\nExecution Engine #  Paimon currently supports MR and Tez execution engine for Hive Read, and MR execution engine for Hive Write. Note If you use beeline, please restart the hive cluster.\nInstallation #  Download the jar file with corresponding version.\n    Jar     Hive 3.1 paimon-hive-connector-3.1-1.1.1.jar   Hive 2.3 paimon-hive-connector-2.3-1.1.1.jar   Hive 2.2 paimon-hive-connector-2.2-1.1.1.jar   Hive 2.1 paimon-hive-connector-2.1-1.1.1.jar   Hive 2.1-cdh-6.3 paimon-hive-connector-2.1-cdh-6.3-1.1.1.jar    You can also manually build bundled jar from the source code.\nTo build from source code, clone the git repository.\nBuild bundled jar with the following command. mvn clean install -DskipTests\nYou can find Hive connector jar in ./paimon-hive/paimon-hive-connector-\u0026lt;hive-version\u0026gt;/target/paimon-hive-connector-\u0026lt;hive-version\u0026gt;-1.1.1.jar.\nThere are several ways to add this jar to Hive.\n You can create an auxlib folder under the root directory of Hive, and copy paimon-hive-connector-1.1.1.jar into auxlib. You can also copy this jar to a path accessible by Hive, then use add jar /path/to/paimon-hive-connector-1.1.1.jar to enable paimon support in Hive. Note that this method is not recommended. If you\u0026rsquo;re using the MR execution engine and running a join statement, you may be faced with the exception org.apache.hive.com.esotericsoftware.kryo.kryoexception: unable to find class.  NOTE:\n If you are using HDFS :  Make sure that the environment variable HADOOP_HOME or HADOOP_CONF_DIR is set. You can set paimon.hadoop-load-default-config =false to disable loading the default value from core-default.xml、hdfs-default.xml, which may lead smaller size for split.   With hive cbo, it may lead to some incorrect query results, such as to query struct type with not null predicate, you can disable the cbo by set hive.cbo.enable=false; command.  Hive SQL: access Paimon Tables already in Hive metastore #  Run the following Hive SQL in Hive CLI to access the created table.\n-- Assume that paimon-hive-connector-\u0026lt;hive-version\u0026gt;-1.1.1.jar is already in auxlib directory. -- List tables in Hive -- (you might need to switch to \u0026#34;default\u0026#34; database if you\u0026#39;re not there by default)  SHOW TABLES; /* OK test_table */ -- Read records from test_table  SELECT a, b FROM test_table ORDER BY a; /* OK 1\tTable 2\tStore */ -- Insert records into test table -- Limitations: -- Only support INSERT INTO, not support INSERT OVERWRITE -- It is recommended to write to a non primary key table -- Writing to a primary key table may result in a large number of small files  INSERT INTO test_table VALUES (3, \u0026#39;Paimon\u0026#39;); SELECT a, b FROM test_table ORDER BY a; /* OK 1\tTable 2\tStore 3\tPaimon */ -- time travel  SET paimon.scan.snapshot-id=1; SELECT a, b FROM test_table ORDER BY a; /* OK 1\tTable 2\tStore 3\tPaimon */ SET paimon.scan.snapshot-id=null; Hive SQL: create new Paimon Tables #  You can create new paimon tables in Hive. Run the following Hive SQL in Hive CLI.\n-- Assume that paimon-hive-connector-1.1.1.jar is already in auxlib directory. -- Let\u0026#39;s create a new paimon table.  SET hive.metastore.warehouse.dir=warehouse_path; CREATE TABLE hive_test_table( a INT COMMENT \u0026#39;The a field\u0026#39;, b STRING COMMENT \u0026#39;The b field\u0026#39; ) STORED BY \u0026#39;org.apache.paimon.hive.PaimonStorageHandler\u0026#39;; Hive SQL: access Paimon Tables by External Table #  To access existing paimon table, you can also register them as external tables in Hive. Run the following Hive SQL in Hive CLI.\n-- Assume that paimon-hive-connector-1.1.1.jar is already in auxlib directory. -- Let\u0026#39;s use the test_table created in the above section. -- To create an external table, you don\u0026#39;t need to specify any column or table properties. -- Pointing the location to the path of table is enough.  CREATE EXTERNAL TABLE external_test_table STORED BY \u0026#39;org.apache.paimon.hive.PaimonStorageHandler\u0026#39; LOCATION \u0026#39;/path/to/table/store/warehouse/default.db/test_table\u0026#39;; -- In addition to the way setting location above, you can also place the location setting in TBProperties -- to avoid Hive accessing Paimon\u0026#39;s location through its own file system when creating tables. -- This method is effective in scenarios using Object storage,such as s3.  CREATE EXTERNAL TABLE external_test_table STORED BY \u0026#39;org.apache.paimon.hive.PaimonStorageHandler\u0026#39; TBLPROPERTIES ( \u0026#39;paimon_location\u0026#39; =\u0026#39;s3://xxxxx/path/to/table/store/warehouse/default.db/test_table\u0026#39; ); -- Read records from external_test_table  SELECT a, b FROM external_test_table ORDER BY a; /* OK 1\tTable 2\tStore */ -- Insert records into test table  INSERT INTO external_test_table VALUES (3, \u0026#39;Paimon\u0026#39;); SELECT a, b FROM external_test_table ORDER BY a; /* OK 1\tTable 2\tStore 3\tPaimon */ Hive Type Conversion #  This section lists all supported type conversion between Hive and Paimon. All Hive\u0026rsquo;s data types are available in package org.apache.hadoop.hive.serde2.typeinfo.\n  Hive Data Type Paimon Data Type Atomic Type     StructTypeInfo RowType false   MapTypeInfo MapType false   ListTypeInfo ArrayType false   PrimitiveTypeInfo(\"boolean\") BooleanType true   PrimitiveTypeInfo(\"tinyint\") TinyIntType true   PrimitiveTypeInfo(\"smallint\") SmallIntType true   PrimitiveTypeInfo(\"int\") IntType true   PrimitiveTypeInfo(\"bigint\") BigIntType true   PrimitiveTypeInfo(\"float\") FloatType true   PrimitiveTypeInfo(\"double\") DoubleType true   CharTypeInfo(length) CharType(length) true   PrimitiveTypeInfo(\"string\") VarCharType(VarCharType.MAX_LENGTH) true   VarcharTypeInfo(length) VarCharType(length), length is less than VarCharType.MAX_LENGTH true   PrimitiveTypeInfo(\"date\") DateType true   PrimitiveTypeInfo(\"timestamp\") TimestampType true   DecimalTypeInfo(precision, scale) DecimalType(precision, scale) true   PrimitiveTypeInfo(\"binary\") VarBinaryType, BinaryType true    "});index.add({'id':52,'href':'/docs/1.1/migration/iceberg-compatibility/','title':"Iceberg Compatibility",'section':"Migration",'content':"Iceberg Compatibility #  Paimon supports generating Iceberg compatible metadata, so that Paimon tables can be consumed directly by Iceberg readers.\nSet the following table options, so that Paimon tables can generate Iceberg compatible metadata.\n  Option Default Type Description     metadata.iceberg.storage disabled Enum  When set, produce Iceberg metadata after a snapshot is committed, so that Iceberg readers can read Paimon's raw data files.  disabled: Disable Iceberg compatibility support. table-location: Store Iceberg metadata in each table's directory. hadoop-catalog: Store Iceberg metadata in a separate directory. This directory can be specified as the warehouse directory of an Iceberg Hadoop catalog. hive-catalog: Not only store Iceberg metadata like hadoop-catalog, but also create Iceberg external table in Hive.      For most SQL users, we recommend setting 'metadata.iceberg.storage' = 'hadoop-catalog' or 'metadata.iceberg.storage' = 'hive-catalog', so that all tables can be visited as an Iceberg warehouse. For Iceberg Java API users, you might consider setting 'metadata.iceberg.storage' = 'table-location', so you can visit each table with its table path.\nAppend Tables #  Let\u0026rsquo;s walk through a simple example, where we query Paimon tables with Iceberg connectors in Flink and Spark. Before trying out this example, make sure that your compute engine already supports Iceberg. Please refer to Iceberg\u0026rsquo;s document if you haven\u0026rsquo;t set up Iceberg.\n Flink: Preparation when using Flink SQL Client Spark: Using Iceberg in Spark 3  Let\u0026rsquo;s now create a Paimon append only table with Iceberg compatibility enabled and insert some data.\nFlink SQL CREATE CATALOG paimon_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;\u0026lt;path-to-warehouse\u0026gt;\u0026#39; ); CREATE TABLE paimon_catalog.`default`.cities ( country STRING, name STRING ) WITH ( \u0026#39;metadata.iceberg.storage\u0026#39; = \u0026#39;hadoop-catalog\u0026#39; ); INSERT INTO paimon_catalog.`default`.cities VALUES (\u0026#39;usa\u0026#39;, \u0026#39;new york\u0026#39;), (\u0026#39;germany\u0026#39;, \u0026#39;berlin\u0026#39;), (\u0026#39;usa\u0026#39;, \u0026#39;chicago\u0026#39;), (\u0026#39;germany\u0026#39;, \u0026#39;hamburg\u0026#39;); Spark SQL Start spark-sql with the following command line.\nspark-sql --jars \u0026lt;path-to-paimon-jar\u0026gt; \\  --conf spark.sql.catalog.paimon_catalog=org.apache.paimon.spark.SparkCatalog \\  --conf spark.sql.catalog.paimon_catalog.warehouse=\u0026lt;path-to-warehouse\u0026gt; \\  --packages org.apache.iceberg:iceberg-spark-runtime-\u0026lt;iceberg-version\u0026gt; \\  --conf spark.sql.catalog.iceberg_catalog=org.apache.iceberg.spark.SparkCatalog \\  --conf spark.sql.catalog.iceberg_catalog.type=hadoop \\  --conf spark.sql.catalog.iceberg_catalog.warehouse=\u0026lt;path-to-warehouse\u0026gt;/iceberg \\  --conf spark.sql.catalog.iceberg_catalog.cache-enabled=false \\ # disable iceberg catalog caching to quickly see the result --conf spark.sql.extensions=org.apache.paimon.spark.extensions.PaimonSparkSessionExtensions,org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions Run the following Spark SQL to create Paimon table and insert data.\nCREATE TABLE paimon_catalog.`default`.cities ( country STRING, name STRING ) TBLPROPERTIES ( \u0026#39;metadata.iceberg.storage\u0026#39; = \u0026#39;hadoop-catalog\u0026#39; ); INSERT INTO paimon_catalog.`default`.cities VALUES (\u0026#39;usa\u0026#39;, \u0026#39;new york\u0026#39;), (\u0026#39;germany\u0026#39;, \u0026#39;berlin\u0026#39;), (\u0026#39;usa\u0026#39;, \u0026#39;chicago\u0026#39;), (\u0026#39;germany\u0026#39;, \u0026#39;hamburg\u0026#39;);  Now let\u0026rsquo;s query this Paimon table with Iceberg connector.\nFlink SQL CREATE CATALOG iceberg_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;iceberg\u0026#39;, \u0026#39;catalog-type\u0026#39; = \u0026#39;hadoop\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;\u0026lt;path-to-warehouse\u0026gt;/iceberg\u0026#39;, \u0026#39;cache-enabled\u0026#39; = \u0026#39;false\u0026#39; -- disable iceberg catalog caching to quickly see the result ); SELECT * FROM iceberg_catalog.`default`.cities WHERE country = \u0026#39;germany\u0026#39;; /* +----+--------------------------------+--------------------------------+ | op | country | name | +----+--------------------------------+--------------------------------+ | +I | germany | berlin | | +I | germany | hamburg | +----+--------------------------------+--------------------------------+ */ Spark SQL SELECT * FROM iceberg_catalog.`default`.cities WHERE country = \u0026#39;germany\u0026#39;; /* germany berlin germany hamburg */  Let\u0026rsquo;s insert more data and query again.\nFlink SQL INSERT INTO paimon_catalog.`default`.cities VALUES (\u0026#39;usa\u0026#39;, \u0026#39;houston\u0026#39;), (\u0026#39;germany\u0026#39;, \u0026#39;munich\u0026#39;); SELECT * FROM iceberg_catalog.`default`.cities WHERE country = \u0026#39;germany\u0026#39;; /* +----+--------------------------------+--------------------------------+ | op | country | name | +----+--------------------------------+--------------------------------+ | +I | germany | munich | | +I | germany | berlin | | +I | germany | hamburg | +----+--------------------------------+--------------------------------+ */ Spark SQL INSERT INTO paimon_catalog.`default`.cities VALUES (\u0026#39;usa\u0026#39;, \u0026#39;houston\u0026#39;), (\u0026#39;germany\u0026#39;, \u0026#39;munich\u0026#39;); SELECT * FROM iceberg_catalog.`default`.cities WHERE country = \u0026#39;germany\u0026#39;; /* germany munich germany berlin germany hamburg */  Primary Key Tables #  Flink SQL CREATE CATALOG paimon_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;\u0026lt;path-to-warehouse\u0026gt;\u0026#39; ); CREATE TABLE paimon_catalog.`default`.orders ( order_id BIGINT, status STRING, payment DOUBLE, PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( \u0026#39;metadata.iceberg.storage\u0026#39; = \u0026#39;hadoop-catalog\u0026#39;, \u0026#39;compaction.optimization-interval\u0026#39; = \u0026#39;1ms\u0026#39; -- ATTENTION: this option is only for testing, see \u0026#34;timeliness\u0026#34; section below for more information ); INSERT INTO paimon_catalog.`default`.orders VALUES (1, \u0026#39;SUBMITTED\u0026#39;, CAST(NULL AS DOUBLE)), (2, \u0026#39;COMPLETED\u0026#39;, 200.0), (3, \u0026#39;SUBMITTED\u0026#39;, CAST(NULL AS DOUBLE)); CREATE CATALOG iceberg_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;iceberg\u0026#39;, \u0026#39;catalog-type\u0026#39; = \u0026#39;hadoop\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;\u0026lt;path-to-warehouse\u0026gt;/iceberg\u0026#39;, \u0026#39;cache-enabled\u0026#39; = \u0026#39;false\u0026#39; -- disable iceberg catalog caching to quickly see the result ); SELECT * FROM iceberg_catalog.`default`.orders WHERE status = \u0026#39;COMPLETED\u0026#39;; /* +----+----------------------+--------------------------------+--------------------------------+ | op | order_id | status | payment | +----+----------------------+--------------------------------+--------------------------------+ | +I | 2 | COMPLETED | 200.0 | +----+----------------------+--------------------------------+--------------------------------+ */ INSERT INTO paimon_catalog.`default`.orders VALUES (1, \u0026#39;COMPLETED\u0026#39;, 100.0); SELECT * FROM iceberg_catalog.`default`.orders WHERE status = \u0026#39;COMPLETED\u0026#39;; /* +----+----------------------+--------------------------------+--------------------------------+ | op | order_id | status | payment | +----+----------------------+--------------------------------+--------------------------------+ | +I | 1 | COMPLETED | 100.0 | | +I | 2 | COMPLETED | 200.0 | +----+----------------------+--------------------------------+--------------------------------+ */ Spark SQL Start spark-sql with the following command line.\nspark-sql --jars \u0026lt;path-to-paimon-jar\u0026gt; \\  --conf spark.sql.catalog.paimon_catalog=org.apache.paimon.spark.SparkCatalog \\  --conf spark.sql.catalog.paimon_catalog.warehouse=\u0026lt;path-to-warehouse\u0026gt; \\  --packages org.apache.iceberg:iceberg-spark-runtime-\u0026lt;iceberg-version\u0026gt; \\  --conf spark.sql.catalog.iceberg_catalog=org.apache.iceberg.spark.SparkCatalog \\  --conf spark.sql.catalog.iceberg_catalog.type=hadoop \\  --conf spark.sql.catalog.iceberg_catalog.warehouse=\u0026lt;path-to-warehouse\u0026gt;/iceberg \\  --conf spark.sql.catalog.iceberg_catalog.cache-enabled=false \\ # disable iceberg catalog caching to quickly see the result --conf spark.sql.extensions=org.apache.paimon.spark.extensions.PaimonSparkSessionExtensions,org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions Run the following Spark SQL to create Paimon table, insert/update data, and query with Iceberg catalog.\nCREATE TABLE paimon_catalog.`default`.orders ( order_id BIGINT, status STRING, payment DOUBLE ) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;order_id\u0026#39;, \u0026#39;metadata.iceberg.storage\u0026#39; = \u0026#39;hadoop-catalog\u0026#39;, \u0026#39;compaction.optimization-interval\u0026#39; = \u0026#39;1ms\u0026#39; -- ATTENTION: this option is only for testing, see \u0026#34;timeliness\u0026#34; section below for more information ); INSERT INTO paimon_catalog.`default`.orders VALUES (1, \u0026#39;SUBMITTED\u0026#39;, CAST(NULL AS DOUBLE)), (2, \u0026#39;COMPLETED\u0026#39;, 200.0), (3, \u0026#39;SUBMITTED\u0026#39;, CAST(NULL AS DOUBLE)); SELECT * FROM iceberg_catalog.`default`.orders WHERE status = \u0026#39;COMPLETED\u0026#39;; /* 2 COMPLETED 200.0 */ INSERT INTO paimon_catalog.`default`.orders VALUES (1, \u0026#39;COMPLETED\u0026#39;, 100.0); SELECT * FROM iceberg_catalog.`default`.orders WHERE status = \u0026#39;COMPLETED\u0026#39;; /* 2 COMPLETED 200.0 1 COMPLETED 100.0 */  Paimon primary key tables organize data files as LSM trees, so data files must be merged in memory before querying. However, Iceberg readers are not able to merge data files, so they can only query data files on the highest level of LSM trees. Data files on the highest level are produced by the full compaction process. So to conclude, for primary key tables, Iceberg readers can only query data after full compaction.\nBy default, there is no guarantee on how frequently Paimon will perform full compaction. You can configure the following table option, so that Paimon is forced to perform full compaction after several commits.\n  Option Default Type Description     compaction.optimization-interval (none) Duration Full compaction will be constantly triggered per time interval. First compaction after the job starts will always be full compaction.   full-compaction.delta-commits (none) Integer Full compaction will be constantly triggered after delta commits. Only implemented in Flink.    Note that full compaction is a resource-consuming process, so the value of this table option should not be too small. We recommend full compaction to be performed once or twice per hour.\nHive Catalog #  When creating Paimon table, set 'metadata.iceberg.storage' = 'hive-catalog'. This option value not only store Iceberg metadata like hadoop-catalog, but also create Iceberg external table in Hive. This Paimon table can be accessed from Iceberg Hive catalog later.\nTo provide information about Hive metastore, you also need to set some (or all) of the following table options when creating Paimon table.\n  Option Default Type Description     metadata.iceberg.uri  String Hive metastore uri for Iceberg Hive catalog.   metadata.iceberg.hive-conf-dir  String hive-conf-dir for Iceberg Hive catalog.   metadata.iceberg.hadoop-conf-dir  String hadoop-conf-dir for Iceberg Hive catalog.   metadata.iceberg.manifest-compression snappy String Compression for Iceberg manifest files.   metadata.iceberg.manifest-legacy-version false Boolean Should use the legacy manifest version to generate Iceberg's 1.4 manifest files.   metadata.iceberg.hive-client-class org.apache.hadoop.hive.metastore.HiveMetaStoreClient String Hive client class name for Iceberg Hive Catalog.   metadata.iceberg.glue.skip-archive false Boolean Skip archive for AWS Glue catalog.    AWS Glue Catalog #  You can use Hive Catalog to connect AWS Glue metastore, you can use set 'metadata.iceberg.hive-client-class' to 'com.amazonaws.glue.catalog.metastore.AWSCatalogMetastoreClient'.\n Note: You can use this repo to build the required jar, include it in your path and configure the AWSCatalogMetastoreClient.\n AWS Athena #  AWS Athena may use old manifest reader to read Iceberg manifest by names, we should let Paimon producing legacy Iceberg manifest list file, you can enable: 'metadata.iceberg.manifest-legacy-version'.\nDuckDB #  Duckdb may rely on files placed in the root/data directory, while Paimon is usually placed directly in the root directory, so you can configure this parameter for the table to achieve compatibility: 'data-file.path-directory' = 'data'.\nTrino Iceberg #  In this example, we use Trino Iceberg connector to access Paimon table through Iceberg Hive catalog. Before trying out this example, make sure that you have configured Trino Iceberg connector. See Trino\u0026rsquo;s document for more information.\nLet\u0026rsquo;s first create a Paimon table with Iceberg compatibility enabled.\nFlink SQL CREATE CATALOG paimon_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;\u0026lt;path-to-warehouse\u0026gt;\u0026#39; ); CREATE TABLE paimon_catalog.`default`.animals ( kind STRING, name STRING ) WITH ( \u0026#39;metadata.iceberg.storage\u0026#39; = \u0026#39;hive-catalog\u0026#39;, \u0026#39;metadata.iceberg.uri\u0026#39; = \u0026#39;thrift://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;\u0026#39; ); INSERT INTO paimon_catalog.`default`.animals VALUES (\u0026#39;mammal\u0026#39;, \u0026#39;cat\u0026#39;), (\u0026#39;mammal\u0026#39;, \u0026#39;dog\u0026#39;), (\u0026#39;reptile\u0026#39;, \u0026#39;snake\u0026#39;), (\u0026#39;reptile\u0026#39;, \u0026#39;lizard\u0026#39;); Spark SQL Start spark-sql with the following command line.\nspark-sql --jars \u0026lt;path-to-paimon-jar\u0026gt; \\  --conf spark.sql.catalog.paimon_catalog=org.apache.paimon.spark.SparkCatalog \\  --conf spark.sql.catalog.paimon_catalog.warehouse=\u0026lt;path-to-warehouse\u0026gt; \\  --packages org.apache.iceberg:iceberg-spark-runtime-\u0026lt;iceberg-version\u0026gt; \\  --conf spark.sql.catalog.iceberg_catalog=org.apache.iceberg.spark.SparkCatalog \\  --conf spark.sql.catalog.iceberg_catalog.type=hadoop \\  --conf spark.sql.catalog.iceberg_catalog.warehouse=\u0026lt;path-to-warehouse\u0026gt;/iceberg \\  --conf spark.sql.catalog.iceberg_catalog.cache-enabled=false \\ # disable iceberg catalog caching to quickly see the result --conf spark.sql.extensions=org.apache.paimon.spark.extensions.PaimonSparkSessionExtensions,org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions Run the following Spark SQL to create Paimon table, insert/update data, and query with Iceberg catalog.\nCREATE TABLE paimon_catalog.`default`.animals ( kind STRING, name STRING ) TBLPROPERTIES ( \u0026#39;metadata.iceberg.storage\u0026#39; = \u0026#39;hive-catalog\u0026#39;, \u0026#39;metadata.iceberg.uri\u0026#39; = \u0026#39;thrift://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;\u0026#39; ); INSERT INTO paimon_catalog.`default`.animals VALUES (\u0026#39;mammal\u0026#39;, \u0026#39;cat\u0026#39;), (\u0026#39;mammal\u0026#39;, \u0026#39;dog\u0026#39;), (\u0026#39;reptile\u0026#39;, \u0026#39;snake\u0026#39;), (\u0026#39;reptile\u0026#39;, \u0026#39;lizard\u0026#39;);  Start Trino using Iceberg catalog and query from Paimon table.\nSELECT * FROM animals WHERE class = \u0026#39;mammal\u0026#39;; /* kind | name --------+------ mammal | cat mammal | dog */ Supported Types #  Paimon Iceberg compatibility currently supports the following data types.\n   Paimon Data Type Iceberg Data Type     BOOLEAN boolean   INT int   BIGINT long   FLOAT float   DOUBLE double   DECIMAL decimal   CHAR string   VARCHAR string   BINARY binary   VARBINARY binary   DATE date   TIMESTAMP* timestamp   TIMESTAMP_LTZ* timestamptz   ARRAY list   MAP map   ROW struct    *: TIMESTAMP and TIMESTAMP_LTZ type only support precision from 4 to 6\nTable Options #  Options for Iceberg Compatibility.\n  Key Default Type Description     metadata.iceberg.compaction.max.file-num 50 Integer If number of small Iceberg manifest metadata files exceeds this limit, always trigger manifest metadata compaction regardless of their total size.   metadata.iceberg.compaction.min.file-num 10 Integer Minimum number of Iceberg manifest metadata files to trigger manifest metadata compaction.   metadata.iceberg.database (none) String Metastore database name for Iceberg Catalog. Set this as an iceberg database alias if using a centralized Catalog.   metadata.iceberg.delete-after-commit.enabled true Boolean Whether to delete old metadata files after each table commit   metadata.iceberg.glue.skip-archive false Boolean Skip archive for AWS Glue catalog.   metadata.iceberg.hadoop-conf-dir (none) String hadoop-conf-dir for Iceberg Hive catalog.   metadata.iceberg.hive-client-class \"org.apache.hadoop.hive.metastore.HiveMetaStoreClient\" String Hive client class name for Iceberg Hive Catalog.   metadata.iceberg.hive-conf-dir (none) String hive-conf-dir for Iceberg Hive catalog.   metadata.iceberg.manifest-compression \"snappy\" String Compression for Iceberg manifest files.   metadata.iceberg.manifest-legacy-version false Boolean Should use the legacy manifest version to generate Iceberg's 1.4 manifest files.   metadata.iceberg.previous-versions-max 0 Integer The number of old metadata files to keep after each table commit   metadata.iceberg.storage disabled Enum\n When set, produce Iceberg metadata after a snapshot is committed, so that Iceberg readers can read Paimon's raw data files.\nPossible values:\"disabled\": Disable Iceberg compatibility support.\"table-location\": Store Iceberg metadata in each table's directory.\"hadoop-catalog\": Store Iceberg metadata in a separate directory. This directory can be specified as the warehouse directory of an Iceberg Hadoop catalog.\"hive-catalog\": Not only store Iceberg metadata like hadoop-catalog, but also create Iceberg external table in Hive.   metadata.iceberg.table (none) String Metastore table name for Iceberg Catalog.Set this as an iceberg table alias if using a centralized Catalog.   metadata.iceberg.uri (none) String Hive metastore uri for Iceberg Hive catalog.    "});index.add({'id':53,'href':'/docs/1.1/concepts/spec/manifest/','title':"Manifest",'section':"Specification",'content':"Manifest #  Manifest List #  ├── manifest └── manifest-list-51c16f7b-421c-4bc0-80a0-17677f343358-1 Manifest List includes meta of several manifest files. Its name contains UUID, it is a avro file, the schema is:\n _FILE_NAME: STRING, manifest file name. _FILE_SIZE: BIGINT, manifest file size. _NUM_ADDED_FILES: BIGINT, number added files in manifest. _NUM_DELETED_FILES: BIGINT, number deleted files in manifest. _PARTITION_STATS: SimpleStats, partition stats, the minimum and maximum values of partition fields in this manifest are beneficial for skipping certain manifest files during queries, it is a SimpleStats. _SCHEMA_ID: BIGINT, schema id when writing this manifest file.  Manifest #  Manifest includes meta of several data files or changelog files or table-index files. Its name contains UUID, it is an avro file.\nThe changes of the file are saved in the manifest, and the file can be added or deleted. Manifests should be in an orderly manner, and the same file may be added or deleted multiple times. The last version should be read. This design can make commit lighter to support file deletion generated by compaction.\nData Manifest #  Data Manifest includes meta of several data files or changelog files.\n├── manifest └── manifest-6758823b-2010-4d06-aef0-3b1b597723d6-0 The schema is:\n _KIND: TINYINT, ADD or DELETE, _PARTITION: BYTES, partition spec, a BinaryRow. _BUCKET: INT, bucket of this file. _TOTAL_BUCKETS: INT, total buckets when write this file, it is used for verification after bucket changes. _FILE: data file meta.  The data file meta is:\n _FILE_NAME: STRING, file name. _FILE_SIZE: BIGINT, file size. _ROW_COUNT: BIGINT, total number of rows (including add \u0026amp; delete) in this file. _MIN_KEY: STRING, the minimum key of this file. _MAX_KEY: STRING, the maximum key of this file. _KEY_STATS: SimpleStats, the statistics of the key. _VALUE_STATS: SimpleStats, the statistics of the value. _MIN_SEQUENCE_NUMBER: BIGINT, the minimum sequence number. _MAX_SEQUENCE_NUMBER: BIGINT, the maximum sequence number. _SCHEMA_ID: BIGINT, schema id when write this file. _LEVEL: INT, level of this file, in LSM. _EXTRA_FILES: ARRAY, extra files for this file, for example, data file index file. _CREATION_TIME: TIMESTAMP_MILLIS, creation time of this file. _DELETE_ROW_COUNT: BIGINT, rowCount = addRowCount + deleteRowCount. _EMBEDDED_FILE_INDEX: BYTES, if data file index is too small, store the index in manifest. _FILE_SOURCE: TINYINT, indicate whether this file is generated as an APPEND or COMPACT file. _VALUE_STATS_COLS: ARRAY, statistical column in metadata. _EXTERNAL_PATH: external path of this file, null if it is in warehouse.  Index Manifest #  Index Manifest includes meta of several table-index files.\n├── manifest └── index-manifest-5d670043-da25-4265-9a26-e31affc98039-0 The schema is:\n _KIND: TINYINT, ADD or DELETE, _PARTITION: BYTES, partition spec, a BinaryRow. _BUCKET: INT, bucket of this file. _INDEX_TYPE: STRING, \u0026ldquo;HASH\u0026rdquo; or \u0026ldquo;DELETION_VECTORS\u0026rdquo;. _FILE_NAME: STRING, file name. _FILE_SIZE: BIGINT, file size. _ROW_COUNT: BIGINT, total number of rows. _DELETIONS_VECTORS_RANGES: Metadata only used by \u0026ldquo;DELETION_VECTORS\u0026rdquo;, is an array of deletion vector meta, the schema of each deletion vector meta is:  f0: the data file name corresponding to this deletion vector. f1: the starting offset of this deletion vector in the index file. f2: the length of this deletion vector in the index file. _CARDINALITY: the number of deleted rows.    Appendix #  SimpleStats #  SimpleStats is nested row, the schema is:\n _MIN_VALUES: BYTES, BinaryRow, the minimum values of the columns. _MAX_VALUES: BYTES, BinaryRow, the maximum values of the columns. _NULL_COUNTS: ARRAY, the number of nulls of the columns.  BinaryRow #  BinaryRow is backed by bytes instead of Object. It can significantly reduce the serialization/deserialization of Java objects.\nA Row has two part: Fixed-length part and variable-length part. Fixed-length part contains 1 byte header and null bit set and field values. Null bit set is used for null tracking and is aligned to 8-byte word boundaries. Field values holds fixed-length primitive types and variable-length values which can be stored in 8 bytes inside. If it do not fit the variable-length field, then store the length and offset of variable-length part.\n"});index.add({'id':54,'href':'/docs/1.1/primary-key-table/merge-engine/','title':"Merge Engine",'section':"Table with PK",'content':""});index.add({'id':55,'href':'/docs/1.1/cdc-ingestion/mongo-cdc/','title':"Mongo CDC",'section':"CDC Ingestion",'content':"Mongo CDC #  Prepare MongoDB Bundled Jar #  flink-sql-connector-mongodb-cdc-*.jar only cdc 3.1+ is supported\nSynchronizing Tables #  By using MongoDBSyncTableAction in a Flink DataStream job or directly through flink run, users can synchronize one collection from MongoDB into one Paimon table.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  mongodb_sync_table \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  [--partition_keys \u0026lt;partition_keys\u0026gt;] \\  [--computed_column \u0026lt;\u0026#39;column-name=expr-name(args[, ...])\u0026#39;\u0026gt; [--computed_column ...]] \\  [--mongodb_conf \u0026lt;mongodb-cdc-source-conf\u0026gt; [--mongodb_conf \u0026lt;mongodb-cdc-source-conf\u0026gt; ...]] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; ...]]    Configuration Description     --warehouse The path to Paimon warehouse.   --database The database name in Paimon catalog.   --table The Paimon table name.   --partition_keys The partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example \"dt,hh,mm\".   --computed_column The definitions of computed columns. The argument field is from MongoDB collection field name. See here for a complete list of configurations.    --mongodb_conf The configuration for Flink CDC MongoDB sources. Each configuration should be specified in the format \"key=value\". hosts, username, password, database and collection are required configurations, others are optional. See its document for a complete list of configurations.   --catalog_conf The configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.   --table_conf The configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.    Here are a few points to take note of:\n The mongodb_conf introduces the schema.start.mode parameter on top of the MongoDB CDC source configuration.schema.start.mode provides two modes: dynamic (default) and specified. In dynamic mode, MongoDB schema information is parsed at one level, which forms the basis for schema change evolution. In specified mode, synchronization takes place according to specified criteria. This can be done by configuring field.name to specify the synchronization fields and parser.path to specify the JSON parsing path for those fields. The difference between the two is that the specify mode requires the user to explicitly identify the fields to be used and create a mapping table based on those fields. Dynamic mode, on the other hand, ensures that Paimon and MongoDB always keep the top-level fields consistent, eliminating the need to focus on specific fields. Further processing of the data table is required when using values from nested fields. The mongodb_conf introduces the default.id.generation parameter as an enhancement to the MongoDB CDC source configuration. The default.id.generation setting offers two distinct behaviors: when set to true and when set to false. When default.id.generation is set to true, the MongoDB CDC source adheres to the default _id generation strategy, which involves stripping the outer $oid nesting to provide a more straightforward identifier. This mode simplifies the _id representation, making it more direct and user-friendly. On the contrary, when default.id.generation is set to false, the MongoDB CDC source retains the original _id structure, without any additional processing. This mode offers users the flexibility to work with the raw _id format as provided by MongoDB, preserving any nested elements like $oid. The choice between the two hinges on the user\u0026rsquo;s preference: the former for a cleaner, simplified _id and the latter for a direct representation of MongoDB\u0026rsquo;s _id structure.    Operator Description     $ The root element to query. This starts all path expressions.   @ The current node being processed by a filter predicate.   * Wildcard. Available anywhere a name or numeric are required.   .. Deep scan. Available anywhere a name is required.   . Dot-notated child.   ['{name}' (, '{name}')] Bracket-notated child or children.   [{number} (, {number})] Bracket-notated child or children.   [start:end] Array index or indexes.   [?({expression})] Filter expression. Expression must evaluate to a boolean value.    Functions can be invoked at the tail end of a path - the input to a function is the output of the path expression. The function output is dictated by the function itself.\n  Function Description Output type     min() Provides the min value of an array of numbers. Double   max() Provides the max value of an array of numbers. Double   avg() Provides the average value of an array of numbers. Double   stddev() Provides the standard deviation value of an array of numbers Double   length() Provides the length of an array Integer   sum() Provides the sum value of an array of numbers. Double   keys() Provides the property keys (An alternative for terminal tilde ~) Set   concat(X) Provides a concatinated version of the path output with a new item. like input   append(X) add an item to the json path output array like input   append(X) add an item to the json path output array like input   first() Provides the first item of an array Depends on the array   last() Provides the last item of an array Depends on the array   index(X) Provides the item of an array of index: X, if the X is negative, take from backwards Depends on the array    Path Examples\n{ \u0026#34;store\u0026#34;: { \u0026#34;book\u0026#34;: [ { \u0026#34;category\u0026#34;: \u0026#34;reference\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Nigel Rees\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Sayings of the Century\u0026#34;, \u0026#34;price\u0026#34;: 8.95 }, { \u0026#34;category\u0026#34;: \u0026#34;fiction\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Evelyn Waugh\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Sword of Honour\u0026#34;, \u0026#34;price\u0026#34;: 12.99 }, { \u0026#34;category\u0026#34;: \u0026#34;fiction\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Herman Melville\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Moby Dick\u0026#34;, \u0026#34;isbn\u0026#34;: \u0026#34;0-553-21311-3\u0026#34;, \u0026#34;price\u0026#34;: 8.99 }, { \u0026#34;category\u0026#34;: \u0026#34;fiction\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;J. R. R. Tolkien\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;The Lord of the Rings\u0026#34;, \u0026#34;isbn\u0026#34;: \u0026#34;0-395-19395-8\u0026#34;, \u0026#34;price\u0026#34;: 22.99 } ], \u0026#34;bicycle\u0026#34;: { \u0026#34;color\u0026#34;: \u0026#34;red\u0026#34;, \u0026#34;price\u0026#34;: 19.95 } }, \u0026#34;expensive\u0026#34;: 10 }    JsonPath Result     $.store.book[*].author Provides the min value of an array of numbers.   $..author All authors.   $.store.* All things, both books and bicycles.   $.store..price Provides the standard deviation value of an array of numbers.   $..book[2] The third book.   $..book[-2] The second to last book.   $..book[0,1] The first two books.   $..book[:2] All books from index 0 (inclusive) until index 2 (exclusive).   $..book[1:2] All books from index 1 (inclusive) until index 2 (exclusive)   $..book[-2:] Last two books   $..book[2:] All books from index 2 (inclusive) to last   $..book[?(@.isbn)] All books with an ISBN number   $.store.book[?(@.price \u0026lt; 10)] All books in store cheaper than 10   $..book[?(@.price \u0026lt;= $['expensive'])] All books in store that are not \"expensive\"   $..book[?(@.author =~ /.*REES/i)] All books matching regex (ignore case)   $..* Give me every thing   $..book.length() The number of books      The synchronized table is required to have its primary key set as _id. This is because MongoDB\u0026rsquo;s change events are recorded before updates in messages. Consequently, we can only convert them into Flink\u0026rsquo;s UPSERT change log stream. The upstart stream demands a unique key, which is why we must declare _id as the primary key. Declaring other columns as primary keys is not feasible, as delete operations only encompass the _id and sharding key, excluding other keys and values.\n  MongoDB Change Streams are designed to return simple JSON documents without any data type definitions. This is because MongoDB is a document-oriented database, and one of its core features is the dynamic schema, where documents can contain different fields, and the data types of fields can be flexible. Therefore, the absence of data type definitions in Change Streams is to maintain this flexibility and extensibility. For this reason, we have set all field data types for synchronizing MongoDB to Paimon as String to address the issue of not being able to obtain data types.\n  If the Paimon table you specify does not exist, this action will automatically create the table. Its schema will be derived from MongoDB collection.\nExample 1: synchronize collection into one Paimon table\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  mongodb_sync_table \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --partition_keys pt \\  --computed_column \u0026#39;_year=year(age)\u0026#39; \\  --mongodb_conf hosts=127.0.0.1:27017 \\  --mongodb_conf username=root \\  --mongodb_conf password=123456 \\  --mongodb_conf database=source_db \\  --mongodb_conf collection=source_table1 \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --table_conf changelog-producer=input \\  --table_conf sink.parallelism=4 Example 2: Synchronize collection into a Paimon table according to the specified field mapping.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  mongodb_sync_table \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --partition_keys pt \\  --mongodb_conf hosts=127.0.0.1:27017 \\  --mongodb_conf username=root \\  --mongodb_conf password=123456 \\  --mongodb_conf database=source_db \\  --mongodb_conf collection=source_table1 \\  --mongodb_conf schema.start.mode=specified \\  --mongodb_conf field.name=_id,name,description \\  --mongodb_conf parser.path=$._id,$.name,$.description \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --table_conf changelog-producer=input \\  --table_conf sink.parallelism=4 Synchronizing Databases #  By using MongoDBSyncDatabaseAction in a Flink DataStream job or directly through flink run, users can synchronize the whole MongoDB database into one Paimon database.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  mongodb_sync_database \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  [--table_prefix \u0026lt;paimon-table-prefix\u0026gt;] \\  [--table_suffix \u0026lt;paimon-table-suffix\u0026gt;] \\  [--including_tables \u0026lt;mongodb-table-name|name-regular-expr\u0026gt;] \\  [--excluding_tables \u0026lt;mongodb-table-name|name-regular-expr\u0026gt;] \\  [--partition_keys \u0026lt;partition_keys\u0026gt;] \\  [--primary_keys \u0026lt;primary-keys\u0026gt;] \\  [--mongodb_conf \u0026lt;mongodb-cdc-source-conf\u0026gt; [--mongodb_conf \u0026lt;mongodb-cdc-source-conf\u0026gt; ...]] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; ...]]    Configuration Description     --warehouse The path to Paimon warehouse.   --database The database name in Paimon catalog.   --table_prefix The prefix of all Paimon tables to be synchronized. For example, if you want all synchronized tables to have \"ods_\" as prefix, you can specify \"--table_prefix ods_\".   --table_suffix The suffix of all Paimon tables to be synchronized. The usage is same as \"--table_prefix\".   --including_tables It is used to specify which source tables are to be synchronized. You must use '|' to separate multiple tables.Because '|' is a special character, a comma is required, for example: 'a|b|c'.Regular expression is supported, for example, specifying \"--including_tables test|paimon.*\" means to synchronize table 'test' and all tables start with 'paimon'.   --excluding_tables It is used to specify which source tables are not to be synchronized. The usage is same as \"--including_tables\". \"--excluding_tables\" has higher priority than \"--including_tables\" if you specified both.   --partition_keys The partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example \"dt,hh,mm\". If the keys are not in source table, the sink table won't set partition keys.   --primary_keys The primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example \"buyer_id,seller_id\". If the keys are not in source table, but the source table has primary keys, the sink table will use source table's primary keys. Otherwise, the sink table won't set primary keys.   --mongodb_conf The configuration for Flink CDC MongoDB sources. Each configuration should be specified in the format \"key=value\". hosts, username, password, database are required configurations, others are optional. See its document for a complete list of configurations.   --catalog_conf The configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.   --table_conf The configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.    All collections to be synchronized need to set _id as the primary key. For each MongoDB collection to be synchronized, if the corresponding Paimon table does not exist, this action will automatically create the table. Its schema will be derived from all specified MongoDB collection. If the Paimon table already exists, its schema will be compared against the schema of all specified MongoDB collection. Any MongoDB tables created after the commencement of the task will automatically be included.\nExample 1: synchronize entire database\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  mongodb_sync_database \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --mongodb_conf hosts=127.0.0.1:27017 \\  --mongodb_conf username=root \\  --mongodb_conf password=123456 \\  --mongodb_conf database=source_db \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --table_conf changelog-producer=input \\  --table_conf sink.parallelism=4 Example 2: Synchronize the specified table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ --fromSavepoint savepointPath \\ /path/to/paimon-flink-action-1.1.1.jar \\ mongodb_sync_database \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --mongodb_conf hosts=127.0.0.1:27017 \\ --mongodb_conf username=root \\ --mongodb_conf password=123456 \\ --mongodb_conf database=source_db \\ --catalog_conf metastore=hive \\ --catalog_conf uri=thrift://hive-metastore:9083 \\ --table_conf bucket=4 \\ --including_tables \u0026#39;product|user|address|order|custom\u0026#39; "});index.add({'id':56,'href':'/docs/1.1/program-api/python-api/','title':"Python API",'section':"Program API",'content':"Java-based Implementation For Python API #  Python SDK  has defined Python API for Paimon. Currently, there is only a Java-based implementation.\nJava-based implementation will launch a JVM and use py4j to execute Java code to read and write Paimon table.\nEnvironment Settings #  SDK Installing #  SDK is published at pypaimon. You can install by\npip install pypaimon Java Runtime Environment #  This SDK needs JRE 1.8. After installing JRE, make sure that at least one of the following conditions is met:\n java command is available. You can verify it by java -version. JAVA_HOME and PATH variables are set correctly. For example, you can set:  export JAVA_HOME=/path/to/java-directory export PATH=$JAVA_HOME/bin:$PATH Set Environment Variables #  Because we need to launch a JVM to access Java code, JVM environment need to be set. Besides, the java code need Hadoop dependencies, so hadoop environment should be set.\nJava classpath #  The package has set necessary paimon core dependencies (Local/Hadoop FileIO, Avro/Orc/Parquet format support and FileSystem/Jdbc/Hive catalog), so If you just test codes in local or in hadoop environment, you don\u0026rsquo;t need to set classpath.\nIf you need other dependencies such as OSS/S3 filesystem jars, or special format and catalog ,please prepare jars and set classpath via one of the following ways:\n Set system environment variable: export _PYPAIMON_JAVA_CLASSPATH=/path/to/jars/* Set environment variable in Python code:  import os from pypaimon.py4j import constants os.environ[constants.PYPAIMON_JAVA_CLASSPATH] = \u0026#39;/path/to/jars/*\u0026#39; JVM args (optional) #  You can set JVM args via one of the following ways:\n Set system environment variable: export _PYPAIMON_JVM_ARGS='arg1 arg2 ...' Set environment variable in Python code:  import os from pypaimon.py4j import constants os.environ[constants.PYPAIMON_JVM_ARGS] = \u0026#39;arg1 arg2 ...\u0026#39; Hadoop classpath #  If the machine is in a hadoop environment, please ensure the value of the environment variable HADOOP_CLASSPATH include path to the common Hadoop libraries, then you don\u0026rsquo;t need to set hadoop.\nOtherwise, you should set hadoop classpath via one of the following ways:\n Set system environment variable: export _PYPAIMON_HADOOP_CLASSPATH=/path/to/jars/* Set environment variable in Python code:  import os from pypaimon.py4j import constants os.environ[constants.PYPAIMON_HADOOP_CLASSPATH] = \u0026#39;/path/to/jars/*\u0026#39; If you just want to test codes in local, we recommend to use Flink Pre-bundled hadoop jar.\nCreate Catalog #  Before coming into contact with the Table, you need to create a Catalog.\nfrom pypaimon.py4j import Catalog # Note that keys and values are all string catalog_options = { \u0026#39;metastore\u0026#39;: \u0026#39;filesystem\u0026#39;, \u0026#39;warehouse\u0026#39;: \u0026#39;file:///path/to/warehouse\u0026#39; } catalog = Catalog.create(catalog_options) Create Database \u0026amp; Table #  You can use the catalog to create table for writing data.\nCreate Database (optional) #  Table is located in a database. If you want to create table in a new database, you should create it.\ncatalog.create_database( name=\u0026#39;database_name\u0026#39;, ignore_if_exists=True, # If you want to raise error if the database exists, set False properties={\u0026#39;key\u0026#39;: \u0026#39;value\u0026#39;} # optional database properties ) Create Schema #  Table schema contains fields definition, partition keys, primary keys, table options and comment. The field definition is described by pyarrow.Schema. All arguments except fields definition are optional.\nGenerally, there are two ways to build pyarrow.Schema.\nFirst, you can use pyarrow.schema method directly, for example:\nimport pyarrow as pa from pypaimon import Schema pa_schema = pa.schema([ (\u0026#39;dt\u0026#39;, pa.string()), (\u0026#39;hh\u0026#39;, pa.string()), (\u0026#39;pk\u0026#39;, pa.int64()), (\u0026#39;value\u0026#39;, pa.string()) ]) schema = Schema( pa_schema=pa_schema, partition_keys=[\u0026#39;dt\u0026#39;, \u0026#39;hh\u0026#39;], primary_keys=[\u0026#39;dt\u0026#39;, \u0026#39;hh\u0026#39;, \u0026#39;pk\u0026#39;], options={\u0026#39;bucket\u0026#39;: \u0026#39;2\u0026#39;}, comment=\u0026#39;my test table\u0026#39; ) See Data Types for all supported pyarrow-to-paimon data types mapping.\nSecond, if you have some Pandas data, the pa_schema can be extracted from DataFrame:\nimport pandas as pd import pyarrow as pa from pypaimon import Schema # Example DataFrame data data = { \u0026#39;dt\u0026#39;: [\u0026#39;2024-01-01\u0026#39;, \u0026#39;2024-01-01\u0026#39;, \u0026#39;2024-01-02\u0026#39;], \u0026#39;hh\u0026#39;: [\u0026#39;12\u0026#39;, \u0026#39;15\u0026#39;, \u0026#39;20\u0026#39;], \u0026#39;pk\u0026#39;: [1, 2, 3], \u0026#39;value\u0026#39;: [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;], } dataframe = pd.DataFrame(data) # Get Paimon Schema record_batch = pa.RecordBatch.from_pandas(dataframe) schema = Schema( pa_schema=record_batch.schema, partition_keys=[\u0026#39;dt\u0026#39;, \u0026#39;hh\u0026#39;], primary_keys=[\u0026#39;dt\u0026#39;, \u0026#39;hh\u0026#39;, \u0026#39;pk\u0026#39;], options={\u0026#39;bucket\u0026#39;: \u0026#39;2\u0026#39;}, comment=\u0026#39;my test table\u0026#39; ) Create Table #  After building table schema, you can create corresponding table:\nschema = ... catalog.create_table( identifier=\u0026#39;database_name.table_name\u0026#39;, schema=schema, ignore_if_exists=True # If you want to raise error if the table exists, set False ) Get Table #  The Table interface provides tools to read and write table.\ntable = catalog.get_table(\u0026#39;database_name.table_name\u0026#39;) Batch Read #  Set Read Parallelism #  TableRead interface provides parallelly reading for multiple splits. You can set 'max-workers': 'N' in catalog_options to set thread numbers for reading splits. max-workers is 1 by default, that means TableRead will read splits sequentially if you doesn\u0026rsquo;t set max-workers.\nGet ReadBuilder and Perform pushdown #  A ReadBuilder is used to build reading utils and perform filter and projection pushdown.\ntable = catalog.get_table(\u0026#39;database_name.table_name\u0026#39;) read_builder = table.new_read_builder() You can use PredicateBuilder to build filters and pushdown them by ReadBuilder:\n# Example filter: (\u0026#39;f0\u0026#39; \u0026lt; 3 OR \u0026#39;f1\u0026#39; \u0026gt; 6) AND \u0026#39;f3\u0026#39; = \u0026#39;A\u0026#39; predicate_builder = read_builder.new_predicate_builder() predicate1 = predicate_builder.less_than(\u0026#39;f0\u0026#39;, 3) predicate2 = predicate_builder.greater_than(\u0026#39;f1\u0026#39;, 6) predicate3 = predicate_builder.or_predicates([predicate1, predicate2]) predicate4 = predicate_builder.equal(\u0026#39;f3\u0026#39;, \u0026#39;A\u0026#39;) predicate_5 = predicate_builder.and_predicates([predicate3, predicate4]) read_builder = read_builder.with_filter(predicate_5) See Predicate for all supported filters and building methods.\nYou can also pushdown projection by ReadBuilder:\n# select f3 and f2 columns read_builder = read_builder.with_projection([\u0026#39;f3\u0026#39;, \u0026#39;f2\u0026#39;]) Scan Plan #  Then you can step into Scan Plan stage to get splits:\ntable_scan = read_builder.new_scan() splits = table_scan.plan().splits() Read Splits #  Finally, you can read data from the splits to various data format.\nApache Arrow #  This requires pyarrow to be installed.\nYou can read all the data into a pyarrow.Table:\ntable_read = read_builder.new_read() pa_table = table_read.to_arrow(splits) print(pa_table) # pyarrow.Table # f0: int32 # f1: string # ---- # f0: [[1,2,3],[4,5,6],...] # f1: [[\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;c\u0026#34;],[\u0026#34;d\u0026#34;,\u0026#34;e\u0026#34;,\u0026#34;f\u0026#34;],...] You can also read data into a pyarrow.RecordBatchReader and iterate record batches:\ntable_read = read_builder.new_read() for batch in table_read.to_arrow_batch_reader(splits): print(batch) # pyarrow.RecordBatch # f0: int32 # f1: string # ---- # f0: [1,2,3] # f1: [\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;c\u0026#34;] Pandas #  This requires pandas to be installed.\nYou can read all the data into a pandas.DataFrame:\ntable_read = read_builder.new_read() df = table_read.to_pandas(splits) print(df) # f0 f1 # 0 1 a # 1 2 b # 2 3 c # 3 4 d # ... DuckDB #  This requires duckdb to be installed.\nYou can convert the splits into an in-memory DuckDB table and query it:\ntable_read = read_builder.new_read() duckdb_con = table_read.to_duckdb(splits, \u0026#39;duckdb_table\u0026#39;) print(duckdb_con.query(\u0026#34;SELECT * FROM duckdb_table\u0026#34;).fetchdf()) # f0 f1 # 0 1 a # 1 2 b # 2 3 c # 3 4 d # ... print(duckdb_con.query(\u0026#34;SELECT * FROM duckdb_table WHERE f0 = 1\u0026#34;).fetchdf()) # f0 f1 # 0 1 a Ray #  This requires ray to be installed.\nYou can convert the splits into a Ray dataset and handle it by Ray API:\ntable_read = read_builder.new_read() ray_dataset = table_read.to_ray(splits) print(ray_dataset) # MaterializedDataset(num_blocks=1, num_rows=9, schema={f0: int32, f1: string}) print(ray_dataset.take(3)) # [{\u0026#39;f0\u0026#39;: 1, \u0026#39;f1\u0026#39;: \u0026#39;a\u0026#39;}, {\u0026#39;f0\u0026#39;: 2, \u0026#39;f1\u0026#39;: \u0026#39;b\u0026#39;}, {\u0026#39;f0\u0026#39;: 3, \u0026#39;f1\u0026#39;: \u0026#39;c\u0026#39;}] print(ray_dataset.to_pandas()) # f0 f1 # 0 1 a # 1 2 b # 2 3 c # 3 4 d # ... Batch Write #  Paimon table write is Two-Phase Commit, you can write many times, but once committed, no more data can be written.\nCurrently, Python SDK doesn\u0026rsquo;t support writing primary key table with bucket=-1.  table = catalog.get_table(\u0026#39;database_name.table_name\u0026#39;) # 1. Create table write and commit write_builder = table.new_batch_write_builder() table_write = write_builder.new_write() table_commit = write_builder.new_commit() # 2. Write data. Support 3 methods: # 2.1 Write pandas.DataFrame dataframe = ... table_write.write_pandas(dataframe) # 2.2 Write pyarrow.Table pa_table = ... table_write.write_arrow(pa_table) # 2.3 Write pyarrow.RecordBatch record_batch = ... table_write.write_arrow_batch(record_batch) # 3. Commit data commit_messages = table_write.prepare_commit() table_commit.commit(commit_messages) # 4. Close resources table_write.close() table_commit.close() By default, the data will be appended to table. If you want to overwrite table, you should use TableWrite#overwrite API:\n# overwrite whole table write_builder.overwrite() # overwrite partition \u0026#39;dt=2024-01-01\u0026#39; write_builder.overwrite({\u0026#39;dt\u0026#39;: \u0026#39;2024-01-01\u0026#39;}) Data Types #     pyarrow Paimon     pyarrow.int8() TINYINT   pyarrow.int16() SMALLINT   pyarrow.int32() INT   pyarrow.int64() BIGINT   pyarrow.float16() pyarrow.float32() FLOAT   pyarrow.float64() DOUBLE   pyarrow.string() STRING   pyarrow.boolean() BOOLEAN    Predicate #     Predicate kind Predicate method     p1 and p2 PredicateBuilder.and_predicates([p1, p2])   p1 or p2 PredicateBuilder.or_predicates([p1, p2])   f = literal PredicateBuilder.equal(f, literal)   f != literal PredicateBuilder.not_equal(f, literal)   f \u0026lt; literal PredicateBuilder.less_than(f, literal)   f \u0026lt;= literal PredicateBuilder.less_or_equal(f, literal)   f \u0026gt; literal PredicateBuilder.greater_than(f, literal)   f \u0026gt;= literal PredicateBuilder.greater_or_equal(f, literal)   f is null PredicateBuilder.is_null(f)   f is not null PredicateBuilder.is_not_null(f)   f.startswith(literal) PredicateBuilder.startswith(f, literal)   f.endswith(literal) PredicateBuilder.endswith(f, literal)   f.contains(literal) PredicateBuilder.contains(f, literal)   f is in [l1, l2] PredicateBuilder.is_in(f, [l1, l2])   f is not in [l1, l2] PredicateBuilder.is_not_in(f, [l1, l2])   lower \u0026lt;= f \u0026lt;= upper PredicateBuilder.between(f, lower, upper)    "});index.add({'id':57,'href':'/docs/1.1/flink/sql-query/','title':"SQL Query",'section':"Engine Flink",'content':"SQL Query #  Just like all other tables, Paimon tables can be queried with SELECT statement.\nBatch Query #  Paimon\u0026rsquo;s batch read returns all the data in a snapshot of the table. By default, batch reads return the latest snapshot.\n-- Flink SQL SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;batch\u0026#39;; Batch Time Travel #  Paimon batch reads with time travel can specify a snapshot or a tag and read the corresponding data.\nFlink (dynamic option) -- read the snapshot with id 1L SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.snapshot-id\u0026#39; = \u0026#39;1\u0026#39;) */; -- read the snapshot from specified timestamp in unix milliseconds SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.timestamp-millis\u0026#39; = \u0026#39;1678883047356\u0026#39;) */; -- read the snapshot from specified timestamp string ,it will be automatically converted to timestamp in unix milliseconds -- Supported formats include：yyyy-MM-dd, yyyy-MM-dd HH:mm:ss, yyyy-MM-dd HH:mm:ss.SSS, use default local time zone SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.timestamp\u0026#39; = \u0026#39;2023-12-09 23:09:12\u0026#39;) */; -- read tag \u0026#39;my-tag\u0026#39; SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.tag-name\u0026#39; = \u0026#39;my-tag\u0026#39;) */; -- read the snapshot from watermark, will match the first snapshot after the watermark SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.watermark\u0026#39; = \u0026#39;1678883047356\u0026#39;) */; Flink 1.18\u0026#43; Flink SQL supports time travel syntax after 1.18.\n-- read the snapshot from specified timestamp SELECT * FROM t FOR SYSTEM_TIME AS OF TIMESTAMP \u0026#39;2023-01-01 00:00:00\u0026#39;; -- you can also use some simple expressions (see flink document to get supported functions) SELECT * FROM t FOR SYSTEM_TIME AS OF TIMESTAMP \u0026#39;2023-01-01 00:00:00\u0026#39; + INTERVAL \u0026#39;1\u0026#39; DAY  Batch Incremental #  Read incremental changes between start snapshot (exclusive) and end snapshot.\nFor example:\n \u0026lsquo;5,10\u0026rsquo; means changes between snapshot 5 and snapshot 10. \u0026lsquo;TAG1,TAG3\u0026rsquo; means changes between TAG1 and TAG3.  -- incremental between snapshot ids SELECT * FROM t /*+ OPTIONS(\u0026#39;incremental-between\u0026#39; = \u0026#39;12,20\u0026#39;) */; -- incremental between snapshot time mills SELECT * FROM t /*+ OPTIONS(\u0026#39;incremental-between-timestamp\u0026#39; = \u0026#39;1692169000000,1692169900000\u0026#39;) */; SELECT * FROM t /*+ OPTIONS(\u0026#39;incremental-between-timestamp\u0026#39; = \u0026#39;2025-03-12 00:00:00,2025-03-12 00:08:00\u0026#39;) */; By default, will scan changelog files for the table which produces changelog files. Otherwise, scan newly changed files. You can also force specifying 'incremental-between-scan-mode'.\nIn Batch SQL, the DELETE records are not allowed to be returned, so records of -D will be dropped. If you want see DELETE records, you can use audit_log table:\nSELECT * FROM t$audit_log /*+ OPTIONS(\u0026#39;incremental-between\u0026#39; = \u0026#39;12,20\u0026#39;) */; Batch Incremental between Auto-created Tags #  You can use incremental-between to query incremental changes between two tags. But for auto-created tag, the tag may not be created in-time because of data delay.\nFor example, assume that tags \u0026lsquo;2024-12-01\u0026rsquo;, \u0026lsquo;2024-12-02\u0026rsquo; and \u0026lsquo;2024-12-04\u0026rsquo; are auto created daily. Data for 12/03 are delayed and ingested with data for 12/04. Now if you want to query the incremental changes between tags, and you don\u0026rsquo;t know the tag of 12/03 is not created, you will use incremental-between with \u0026lsquo;2024-12-01,2024-12-02\u0026rsquo;, \u0026lsquo;2024-12-02,2024-12-03\u0026rsquo; and \u0026lsquo;2024-12-03,2024-12-04\u0026rsquo; respectively, then you will get an error that the tag \u0026lsquo;2024-12-03\u0026rsquo; doesn\u0026rsquo;t exist.\nWe introduced a new option incremental-to-auto-tag for this scenario. You can only specify the end tag, and Paimon will find an earlier tag and return changes between them. If the tag doesn\u0026rsquo;t exist or the earlier tag doesn\u0026rsquo;t exist, return empty.\nFor example, when you query \u0026lsquo;incremental-to-auto-tag=2024-12-01\u0026rsquo; or \u0026lsquo;incremental-to-auto-tag=2024-12-03\u0026rsquo;, the result is empty; Query \u0026lsquo;incremental-to-auto-tag=2024-12-02\u0026rsquo;, the result is change between 12/01 and 12/02; Query \u0026lsquo;incremental-to-auto-tag=2024-12-04\u0026rsquo;, the result is change between 12/02 and 12/04.\nStreaming Query #  By default, Streaming read produces the latest snapshot on the table upon first startup, and continue to read the latest changes.\nPaimon by default ensures that your startup is properly processed with all data included.\nPaimon Source in Streaming mode is unbounded, like a queue that never ends.  -- Flink SQL SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;streaming\u0026#39;; You can also do streaming read without the snapshot data, you can use latest scan mode:\n-- Continuously reads latest changes without producing a snapshot at the beginning. SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.mode\u0026#39; = \u0026#39;latest\u0026#39;) */; Streaming Time Travel #  If you only want to process data for today and beyond, you can do so with partitioned filters:\nSELECT * FROM t WHERE dt \u0026gt; \u0026#39;2023-06-26\u0026#39;; If it\u0026rsquo;s not a partitioned table, or you can\u0026rsquo;t filter by partition, you can use Time travel\u0026rsquo;s stream read.\nFlink (dynamic option) -- read changes from snapshot id 1L SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.snapshot-id\u0026#39; = \u0026#39;1\u0026#39;) */; -- read changes from snapshot specified timestamp SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.timestamp-millis\u0026#39; = \u0026#39;1678883047356\u0026#39;) */; -- read snapshot id 1L upon first startup, and continue to read the changes SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.mode\u0026#39;=\u0026#39;from-snapshot-full\u0026#39;,\u0026#39;scan.snapshot-id\u0026#39; = \u0026#39;1\u0026#39;) */; Flink 1.18\u0026#43; Flink SQL supports time travel syntax after 1.18.\n-- read the snapshot from specified timestamp SELECT * FROM t FOR SYSTEM_TIME AS OF TIMESTAMP \u0026#39;2023-01-01 00:00:00\u0026#39;; -- you can also use some simple expressions (see flink document to get supported functions) SELECT * FROM t FOR SYSTEM_TIME AS OF TIMESTAMP \u0026#39;2023-01-01 00:00:00\u0026#39; + INTERVAL \u0026#39;1\u0026#39; DAY  Time travel\u0026rsquo;s stream read rely on snapshots, but by default, snapshot only retains data within 1 hour, which can prevent you from reading older incremental data. So, Paimon also provides another mode for streaming reads, scan.file-creation-time-millis, which provides a rough filtering to retain files generated after timeMillis.\nSELECT * FROM t /*+ OPTIONS(\u0026#39;scan.file-creation-time-millis\u0026#39; = \u0026#39;1678883047356\u0026#39;) */; Read Overwrite #  Streaming reading will ignore the commits generated by INSERT OVERWRITE by default. If you want to read the commits of OVERWRITE, you can configure streaming-read-overwrite.\nRead Parallelism #  By default, the parallelism of batch reads is the same as the number of splits, while the parallelism of stream reads is the same as the number of buckets, but not greater than scan.infer-parallelism.max.\nDisable scan.infer-parallelism, global parallelism will be used for reads.\nYou can also manually specify the parallelism from scan.parallelism.\n  Key Default Type Description     scan.infer-parallelism true Boolean If it is false, parallelism of source are set by global parallelism. Otherwise, source parallelism is inferred from splits number (batch mode) or bucket number(streaming mode).   scan.infer-parallelism.max 1024 Integer If scan.infer-parallelism is true, limit the parallelism of source through this option.   scan.parallelism (none) Integer Define a custom parallelism for the scan source. By default, if this option is not defined, the planner will derive the parallelism for each statement individually by also considering the global configuration. If user enable the scan.infer-parallelism, the planner will derive the parallelism by inferred parallelism.    Query Optimization #   Batch Streaming It is highly recommended to specify partition and primary key filters along with the query, which will speed up the data skipping of the query.\nThe filter functions that can accelerate data skipping are:\n = \u0026lt; \u0026lt;= \u0026gt; \u0026gt;= IN (...) LIKE 'abc%' IS NULL  Paimon will sort the data by primary key, which speeds up the point queries and range queries. When using a composite primary key, it is best for the query filters to form a leftmost prefix of the primary key for good acceleration.\nSuppose that a table has the following specification:\nCREATE TABLE orders ( catalog_id BIGINT, order_id BIGINT, ....., PRIMARY KEY (catalog_id, order_id) NOT ENFORCED -- composite primary key ); The query obtains a good acceleration by specifying a range filter for the leftmost prefix of the primary key.\nSELECT * FROM orders WHERE catalog_id=1025; SELECT * FROM orders WHERE catalog_id=1025 AND order_id=29495; SELECT * FROM orders WHERE catalog_id=1025 AND order_id\u0026gt;2035 AND order_id\u0026lt;6000; However, the following filter cannot accelerate the query well.\nSELECT * FROM orders WHERE order_id=29495; SELECT * FROM orders WHERE catalog_id=1025 OR order_id=29495; "});index.add({'id':58,'href':'/docs/1.1/spark/sql-query/','title':"SQL Query",'section':"Engine Spark",'content':"SQL Query #  Just like all other tables, Paimon tables can be queried with SELECT statement.\nBatch Query #  Paimon\u0026rsquo;s batch read returns all the data in a snapshot of the table. By default, batch reads return the latest snapshot.\n-- read all columns SELECT * FROM t; Paimon also supports reading some hidden metadata columns, currently supporting the following columns:\n __paimon_file_path: the file path of the record. __paimon_partition: the partition of the record. __paimon_bucket: the bucket of the record. __paimon_row_index: the row index of the record.  -- read all columns and the corresponding file path, partition, bucket, rowIndex of the record SELECT *, __paimon_file_path, __paimon_partition, __paimon_bucket, __paimon_row_index FROM t; Batch Time Travel #  Paimon batch reads with time travel can specify a snapshot or a tag and read the corresponding data.\nRequires Spark 3.3+.\nyou can use VERSION AS OF and TIMESTAMP AS OF in query to do time travel:\n-- read the snapshot with id 1L (use snapshot id as version) SELECT * FROM t VERSION AS OF 1; -- read the snapshot from specified timestamp SELECT * FROM t TIMESTAMP AS OF \u0026#39;2023-06-01 00:00:00.123\u0026#39;; -- read the snapshot from specified timestamp in unix seconds SELECT * FROM t TIMESTAMP AS OF 1678883047; -- read tag \u0026#39;my-tag\u0026#39; SELECT * FROM t VERSION AS OF \u0026#39;my-tag\u0026#39;; -- read the snapshot from specified watermark. will match the first snapshot after the watermark SELECT * FROM t VERSION AS OF \u0026#39;watermark-1678883047356\u0026#39;; If tag\u0026rsquo;s name is a number and equals to a snapshot id, the VERSION AS OF syntax will consider tag first. For example, if you have a tag named \u0026lsquo;1\u0026rsquo; based on snapshot 2, the statement SELECT * FROM t VERSION AS OF '1' actually queries snapshot 2 instead of snapshot 1.  Batch Incremental #  Read incremental changes between start snapshot (exclusive) and end snapshot.\nFor example:\n \u0026lsquo;5,10\u0026rsquo; means changes between snapshot 5 and snapshot 10. \u0026lsquo;TAG1,TAG3\u0026rsquo; means changes between TAG1 and TAG3.  By default, will scan changelog files for the table which produces changelog files. Otherwise, scan newly changed files. You can also force specifying 'incremental-between-scan-mode'.\nPaimon supports that use Spark SQL to do the incremental query that implemented by Spark Table Valued Function.\n-- read the incremental data between snapshot id 12 and snapshot id 20. SELECT * FROM paimon_incremental_query(\u0026#39;tableName\u0026#39;, 12, 20); -- read the incremental data between ts 1692169900000 and ts 1692169900000. SELECT * FROM paimon_incremental_between_timestamp(\u0026#39;tableName\u0026#39;, \u0026#39;1692169000000\u0026#39;, \u0026#39;1692169900000\u0026#39;); SELECT * FROM paimon_incremental_between_timestamp(\u0026#39;tableName\u0026#39;, \u0026#39;2025-03-12 00:00:00\u0026#39;, \u0026#39;2025-03-12 00:08:00\u0026#39;); -- read the incremental data to tag \u0026#39;2024-12-04\u0026#39;. -- Paimon will find an earlier tag and return changes between them. -- If the tag doesn\u0026#39;t exist or the earlier tag doesn\u0026#39;t exist, return empty. SELECT * FROM paimon_incremental_to_auto_tag(\u0026#39;tableName\u0026#39;, \u0026#39;2024-12-04\u0026#39;); In Batch SQL, the DELETE records are not allowed to be returned, so records of -D will be dropped. If you want see DELETE records, you can query audit_log table.\nStreaming Query #  Paimon currently supports Spark 3.3+ for streaming read.  Paimon supports rich scan mode for streaming read. There is a list:\n  Scan Mode Description     latest For streaming sources, continuously reads latest changes without producing a snapshot at the beginning.    latest-full For streaming sources, produces the latest snapshot on the table upon first startup, and continue to read the latest changes.   from-timestamp For streaming sources, continuously reads changes starting from timestamp specified by \"scan.timestamp-millis\", without producing a snapshot at the beginning.    from-snapshot For streaming sources, continuously reads changes starting from snapshot specified by \"scan.snapshot-id\", without producing a snapshot at the beginning.    from-snapshot-full For streaming sources, produces from snapshot specified by \"scan.snapshot-id\" on the table upon first startup, and continuously reads changes.   default It is equivalent to from-snapshot if \"scan.snapshot-id\" is specified. It is equivalent to from-timestamp if \"timestamp-millis\" is specified. Or, It is equivalent to latest-full.    A simple example with default scan mode:\n// no any scan-related configs are provided, that will use latest-full scan mode. val query = spark.readStream .format(\u0026#34;paimon\u0026#34;) .load(\u0026#34;/path/to/paimon/source/table\u0026#34;) .writeStream .format(\u0026#34;console\u0026#34;) .start() Paimon Structured Streaming also supports a variety of streaming read modes, it can support many triggers and many read limits.\nThese read limits are supported:\n  Key Default Type Description     read.stream.maxFilesPerTrigger (none) Integer The maximum number of files returned in a single batch.   read.stream.maxBytesPerTrigger (none) Long The maximum number of bytes returned in a single batch.   read.stream.maxRowsPerTrigger (none) Long The maximum number of rows returned in a single batch.   read.stream.minRowsPerTrigger (none) Long The minimum number of rows returned in a single batch, which used to create MinRowsReadLimit with read.stream.maxTriggerDelayMs together.   read.stream.maxTriggerDelayMs (none) Long The maximum delay between two adjacent batches, which used to create MinRowsReadLimit with read.stream.minRowsPerTrigger together.    Example: One\nUse org.apache.spark.sql.streaming.Trigger.AvailableNow() and maxBytesPerTrigger defined by paimon.\n// Trigger.AvailableNow()) processes all available data at the start // of the query in one or multiple batches, then terminates the query. // That set read.stream.maxBytesPerTrigger to 128M means that each // batch processes a maximum of 128 MB of data. val query = spark.readStream .format(\u0026#34;paimon\u0026#34;) .option(\u0026#34;read.stream.maxBytesPerTrigger\u0026#34;, \u0026#34;134217728\u0026#34;) .load(\u0026#34;/path/to/paimon/source/table\u0026#34;) .writeStream .format(\u0026#34;console\u0026#34;) .trigger(Trigger.AvailableNow()) .start() Example: Two\nUse org.apache.spark.sql.connector.read.streaming.ReadMinRows.\n// It will not trigger a batch until there are more than 5,000 pieces of data, // unless the interval between the two batches is more than 300 seconds. val query = spark.readStream .format(\u0026#34;paimon\u0026#34;) .option(\u0026#34;read.stream.minRowsPerTrigger\u0026#34;, \u0026#34;5000\u0026#34;) .option(\u0026#34;read.stream.maxTriggerDelayMs\u0026#34;, \u0026#34;300000\u0026#34;) .load(\u0026#34;/path/to/paimon/source/table\u0026#34;) .writeStream .format(\u0026#34;console\u0026#34;) .start() Paimon Structured Streaming supports read row in the form of changelog (add rowkind column in row to represent its change type) in two ways:\n Direct streaming read with the system audit_log table Set read.changelog to true (default is false), then streaming read with table location  Example:\n// Option 1 val query1 = spark.readStream .format(\u0026#34;paimon\u0026#34;) .table(\u0026#34;`table_name$audit_log`\u0026#34;) .writeStream .format(\u0026#34;console\u0026#34;) .start() // Option 2 val query2 = spark.readStream .format(\u0026#34;paimon\u0026#34;) .option(\u0026#34;read.changelog\u0026#34;, \u0026#34;true\u0026#34;) .load(\u0026#34;/path/to/paimon/source/table\u0026#34;) .writeStream .format(\u0026#34;console\u0026#34;) .start() /* +I 1 Hi +I 2 Hello */ Query Optimization #  It is highly recommended to specify partition and primary key filters along with the query, which will speed up the data skipping of the query.\nThe filter functions that can accelerate data skipping are:\n = \u0026lt; \u0026lt;= \u0026gt; \u0026gt;= IN (...) LIKE 'abc%' IS NULL  Paimon will sort the data by primary key, which speeds up the point queries and range queries. When using a composite primary key, it is best for the query filters to form a leftmost prefix of the primary key for good acceleration.\nSuppose that a table has the following specification:\nCREATE TABLE orders ( catalog_id BIGINT, order_id BIGINT, ....., ) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;catalog_id,order_id\u0026#39; ); The query obtains a good acceleration by specifying a range filter for the leftmost prefix of the primary key.\nSELECT * FROM orders WHERE catalog_id=1025; SELECT * FROM orders WHERE catalog_id=1025 AND order_id=29495; SELECT * FROM orders WHERE catalog_id=1025 AND order_id\u0026gt;2035 AND order_id\u0026lt;6000; However, the following filter cannot accelerate the query well.\nSELECT * FROM orders WHERE order_id=29495; SELECT * FROM orders WHERE catalog_id=1025 OR order_id=29495; "});index.add({'id':59,'href':'/docs/1.1/append-table/update/','title':"Update",'section':"Table w/o PK",'content':"Update #  Now, only Spark SQL supports DELETE \u0026amp; UPDATE, you can take a look at Spark Write.\nExample:\nDELETE FROM my_table WHERE currency = \u0026#39;UNKNOWN\u0026#39;; Update append table has two modes:\n COW (Copy on Write): search for the hit files and then rewrite each file to remove the data that needs to be deleted from the files. This operation is costly. MOW (Merge on Write): By specifying 'deletion-vectors.enabled' = 'true', the Deletion Vectors mode can be enabled. Only marks certain records of the corresponding file for deletion and writes the deletion file, without rewriting the entire file.  "});index.add({'id':60,'href':'/docs/1.1/append-table/bucketed/','title':"Bucketed",'section':"Table w/o PK",'content':"Bucketed Append #  You can define the bucket and bucket-key to get a bucketed append table.\nExample to create bucketed append table:\nFlink CREATE TABLE my_table ( product_id BIGINT, price DOUBLE, sales BIGINT ) WITH ( \u0026#39;bucket\u0026#39; = \u0026#39;8\u0026#39;, \u0026#39;bucket-key\u0026#39; = \u0026#39;product_id\u0026#39; );  Streaming #  An ordinary Append table has no strict ordering guarantees for its streaming writes and reads, but there are some cases where you need to define a key similar to Kafka\u0026rsquo;s.\nEvery record in the same bucket is ordered strictly, streaming read will transfer the record to down-stream exactly in the order of writing. To use this mode, you do not need to config special configurations, all the data will go into one bucket as a queue.\nCompaction in Bucket #  By default, the sink node will automatically perform compaction to control the number of files. The following options control the strategy of compaction:\n  Key Default Type Description     write-only false Boolean If set to true, compactions and snapshot expiration will be skipped. This option is used along with dedicated compact jobs.   compaction.min.file-num 5 Integer For file set [f_0,...,f_N], the minimum file number to trigger a compaction for append table.   full-compaction.delta-commits (none) Integer Full compaction will be constantly triggered after delta commits.    Streaming Read Order #  For streaming reads, records are produced in the following order:\n For any two records from two different partitions  If scan.plan-sort-partition is set to true, the record with a smaller partition value will be produced first. Otherwise, the record with an earlier partition creation time will be produced first.   For any two records from the same partition and the same bucket, the first written record will be produced first. For any two records from the same partition but two different buckets, different buckets are processed by different tasks, there is no order guarantee between them.  Watermark Definition #  You can define watermark for reading Paimon tables:\nCREATE TABLE t ( `user` BIGINT, product STRING, order_time TIMESTAMP(3), WATERMARK FOR order_time AS order_time - INTERVAL \u0026#39;5\u0026#39; SECOND ) WITH (...); -- launch a bounded streaming job to read paimon_table SELECT window_start, window_end, COUNT(`user`) FROM TABLE( TUMBLE(TABLE t, DESCRIPTOR(order_time), INTERVAL \u0026#39;10\u0026#39; MINUTES)) GROUP BY window_start, window_end; You can also enable Flink Watermark alignment, which will make sure no sources/splits/shards/partitions increase their watermarks too far ahead of the rest:\n  Key Default Type Description     scan.watermark.alignment.group (none) String A group of sources to align watermarks.   scan.watermark.alignment.max-drift (none) Duration Maximal drift to align watermarks, before we pause consuming from the source/task/partition.    Bounded Stream #  Streaming Source can also be bounded, you can specify \u0026lsquo;scan.bounded.watermark\u0026rsquo; to define the end condition for bounded streaming mode, stream reading will end until a larger watermark snapshot is encountered.\nWatermark in snapshot is generated by writer, for example, you can specify a kafka source and declare the definition of watermark. When using this kafka source to write to Paimon table, the snapshots of Paimon table will generate the corresponding watermark, so that you can use the feature of bounded watermark when streaming reads of this Paimon table.\nCREATE TABLE kafka_table ( `user` BIGINT, product STRING, order_time TIMESTAMP(3), WATERMARK FOR order_time AS order_time - INTERVAL \u0026#39;5\u0026#39; SECOND ) WITH (\u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;...); -- launch a streaming insert job INSERT INTO paimon_table SELECT * FROM kakfa_table; -- launch a bounded streaming job to read paimon_table SELECT * FROM paimon_table /*+ OPTIONS(\u0026#39;scan.bounded.watermark\u0026#39;=\u0026#39;...\u0026#39;) */; Batch #  Bucketed table can be used to avoid shuffle if necessary in batch query, for example, you can use the following Spark SQL to read a Paimon table:\nSET spark.sql.sources.v2.bucketing.enabled = true; CREATE TABLE FACT_TABLE (order_id INT, f1 STRING) TBLPROPERTIES (\u0026#39;bucket\u0026#39;=\u0026#39;10\u0026#39;, \u0026#39;bucket-key\u0026#39; = \u0026#39;order_id\u0026#39;); CREATE TABLE DIM_TABLE (order_id INT, f2 STRING) TBLPROPERTIES (\u0026#39;bucket\u0026#39;=\u0026#39;10\u0026#39;, \u0026#39;primary-key\u0026#39; = \u0026#39;order_id\u0026#39;); SELECT * FROM FACT_TABLE JOIN DIM_TABLE on t1.order_id = t4.order_id; The spark.sql.sources.v2.bucketing.enabled config is used to enable bucketing for V2 data sources. When turned on, Spark will recognize the specific distribution reported by a V2 data source through SupportsReportPartitioning, and will try to avoid shuffle if necessary.\nThe costly join shuffle will be avoided if two tables have the same bucketing strategy and same number of buckets.\n"});index.add({'id':61,'href':'/docs/1.1/primary-key-table/changelog-producer/','title':"Changelog Producer",'section':"Table with PK",'content':"Changelog Producer #  Streaming write can continuously produce the latest changes for streaming read.\nBy specifying the changelog-producer table property when creating the table, users can choose the pattern of changes produced from table files.\nchangelog-producer may significantly reduce compaction performance, please do not enable it unless necessary.  None #  By default, no extra changelog producer will be applied to the writer of table. Paimon source can only see the merged changes across snapshots, like what keys are removed and what are the new values of some keys.\nHowever, these merged changes cannot form a complete changelog, because we can\u0026rsquo;t read the old values of the keys directly from them. Merged changes require the consumers to \u0026ldquo;remember\u0026rdquo; the values of each key and to rewrite the values without seeing the old ones. Some consumers, however, need the old values to ensure correctness or efficiency.\nConsider a consumer which calculates the sum on some grouping keys (might not be equal to the primary keys). If the consumer only sees a new value 5, it cannot determine what values should be added to the summing result. For example, if the old value is 4, it should add 1 to the result. But if the old value is 6, it should in turn subtract 1 from the result. Old values are important for these types of consumers.\nTo conclude, none changelog producers are best suited for consumers such as a database system. Flink also has a built-in \u0026ldquo;normalize\u0026rdquo; operator which persists the values of each key in states. As one can easily tell, this operator will be very costly and should be avoided. (You can force removing \u0026ldquo;normalize\u0026rdquo; operator via 'scan.remove-normalize'.)\nInput #  By specifying 'changelog-producer' = 'input', Paimon writers rely on their inputs as a source of complete changelog. All input records will be saved in separated changelog files and will be given to the consumers by Paimon sources.\ninput changelog producer can be used when Paimon writers' inputs are complete changelog, such as from a database CDC, or generated by Flink stateful computation.\nLookup #  If your input can’t produce a complete changelog but you still want to get rid of the costly normalized operator, you may consider using the 'lookup' changelog producer.\nBy specifying 'changelog-producer' = 'lookup', Paimon will generate changelog through 'lookup' before committing the data writing (You can also enable Async Compaction).\nLookup will cache data on the memory and local disk, you can use the following options to tune performance:\n  Option Default Type Description     lookup.cache-file-retention 1 h Duration The cached files retention time for lookup. After the file expires, if there is a need for access, it will be re-read from the DFS to build an index on the local disk.   lookup.cache-max-disk-size unlimited MemorySize Max disk size for lookup cache, you can use this option to limit the use of local disks.   lookup.cache-max-memory-size 256 mb MemorySize Max memory size for lookup cache.    Lookup changelog-producer supports changelog-producer.row-deduplicate to avoid generating -U, +U changelog for the same record.\n(Note: Please increase 'execution.checkpointing.max-concurrent-checkpoints' Flink configuration, this is very important for performance).\nFull Compaction #  You can also consider using \u0026lsquo;full-compaction\u0026rsquo; changelog producer to generate changelog, and is more suitable for scenarios with large latency (For example, 30 minutes).\n By specifying 'changelog-producer' = 'full-compaction', Paimon will compare the results between full compactions and produce the differences as changelog. The latency of changelog is affected by the frequency of full compactions. By specifying full-compaction.delta-commits table property, full compaction will be constantly triggered after delta commits (checkpoints). This is set to 1 by default, so each checkpoint will have a full compression and generate a changelog.  Generally speaking, the cost and consumption of full compaction are high, so we recommend using 'lookup' changelog producer.\nFull compaction changelog producer can produce complete changelog for any type of source. However it is not as efficient as the input changelog producer and the latency to produce changelog might be high.  Full-compaction changelog-producer supports changelog-producer.row-deduplicate to avoid generating -U, +U changelog for the same record.\nChangelog Merging #  For input, lookup, full-compaction \u0026lsquo;changelog-producer\u0026rsquo;.\nIf Flink\u0026rsquo;s checkpoint interval is short (for example, 30 seconds) and the number of buckets is large, each snapshot may produce lots of small changelog files. Too many files may put a burden on the distributed storage cluster.\nIn order to compact small changelog files into large ones, you can set the table option precommit-compact = true. Default value of this option is false, if true, it will add a compact coordinator and worker operator after the writer operator, which copies changelog files into large ones.\n"});index.add({'id':62,'href':'/docs/1.1/flink/consumer-id/','title':"Consumer ID",'section':"Engine Flink",'content':"Consumer ID #  Consumer id can help you accomplish the following two things:\n Safe consumption: When deciding whether a snapshot has expired, Paimon looks at all the consumers of the table in the file system, and if there are consumers that still depend on this snapshot, then this snapshot will not be deleted by expiration. Resume from breakpoint: When previous job is stopped, the newly started job can continue to consume from the previous progress without resuming from the state.  Usage #  You can specify the consumer-id when streaming read table.\nThe consumer will prevent expiration of the snapshot. In order to prevent too many snapshots caused by mistakes, you need to specify 'consumer.expiration-time' to manage the lifetime of consumers.\nALTER TABLE t SET (\u0026#39;consumer.expiration-time\u0026#39; = \u0026#39;1 d\u0026#39;); Then, restart streaming write job of this table, expiration of consumers will be triggered in writing job.\nSELECT * FROM t /*+ OPTIONS(\u0026#39;consumer-id\u0026#39; = \u0026#39;myid\u0026#39;, \u0026#39;consumer.mode\u0026#39; = \u0026#39;at-least-once\u0026#39;) */; Ignore Progress #  Sometimes, you only want the feature of \u0026lsquo;Safe Consumption\u0026rsquo;. You want to get a new snapshot progress when restarting the stream consumption job , you can enable the 'consumer.ignore-progress' option.\nSELECT * FROM t /*+ OPTIONS(\u0026#39;consumer-id\u0026#39; = \u0026#39;myid\u0026#39;, \u0026#39;consumer.ignore-progress\u0026#39; = \u0026#39;true\u0026#39;) */; The startup of this job will retrieve the snapshot that should be read again.\nConsumer Mode #  By default, the consumption of snapshots is strictly aligned within the checkpoint to make \u0026lsquo;Resume from breakpoint\u0026rsquo; feature exactly-once.\nBut in some scenarios where you don\u0026rsquo;t need \u0026lsquo;Resume from breakpoint\u0026rsquo;, or you don\u0026rsquo;t need strict \u0026lsquo;Resume from breakpoint\u0026rsquo;, you can consider enabling 'consumer.mode' = 'at-least-once' mode. This mode:\n Allow readers consume snapshots at different rates and record the slowest snapshot-id among all readers into the consumer. It doesn\u0026rsquo;t affect the checkpoint time and have good performance. This mode can provide more capabilities, such as watermark alignment.  About 'consumer.mode', since the implementation of exactly-once mode and at-least-once mode are completely different, the state of flink is incompatible and cannot be restored from the state when switching modes.  Reset Consumer #  You can reset or delete a consumer with a given consumer ID and next snapshot ID and delete a consumer with a given consumer ID. First, you need to stop the streaming task using this consumer ID, and then execute the reset consumer action job.\nRun the following command:\nFlink SQL CALL sys.reset_consumer( `table` =\u0026gt; \u0026#39;database_name.table_name\u0026#39;, consumer_id =\u0026gt; \u0026#39;consumer_id\u0026#39;, next_snapshot_id =\u0026gt; \u0026lt;snapshot_id\u0026gt; ); -- No next_snapshot_id if you want to delete the consumer Flink Action \u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  reset-consumer \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  --consumer_id \u0026lt;consumer-id\u0026gt; \\  [--next_snapshot \u0026lt;next-snapshot-id\u0026gt;] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] ## No next_snapshot if you want to delete the consumer  Clear Consumers #  You can clear consumers in bulk with a given including consumers and excluding consumers(accept regular expression).\nRun the following command:\nFlink SQL CALL sys.clear_consumers( `table` =\u0026gt; \u0026#39;database_name.table_name\u0026#39;, `including_consumers` =\u0026gt; \u0026#39;including_consumers\u0026#39;, `excluding_consumers` =\u0026gt; \u0026#39;excluding_consumers\u0026#39; ); -- No including_consumers if you want to clear all consumers except excluding_consumers in the table Flink Action \u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  clear_consumers \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  [--including_consumers \u0026lt;including-consumers\u0026gt;] \\  [--excluding_consumers \u0026lt;excluding-consumers\u0026gt;] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] ## No including_consumers if you want to clear all consumers except excluding_consumers in the table  "});index.add({'id':63,'href':'/docs/1.1/concepts/spec/datafile/','title':"DataFile",'section':"Specification",'content':"DataFile #  Partition #  Consider a Partition table via Flink SQL:\nCREATE TABLE part_t ( f0 INT, f1 STRING, dt STRING ) PARTITIONED BY (dt); INSERT INTO part_t VALUES (1, \u0026#39;11\u0026#39;, \u0026#39;20240514\u0026#39;); The file system will be:\npart_t ├── dt=20240514 │ └── bucket-0 │ └── data-ca1c3c38-dc8d-4533-949b-82e195b41bd4-0.orc ├── manifest │ ├── manifest-08995fe5-c2ac-4f54-9a5f-d3af1fcde41d-0 │ ├── manifest-list-51c16f7b-421c-4bc0-80a0-17677f343358-0 │ └── manifest-list-51c16f7b-421c-4bc0-80a0-17677f343358-1 ├── schema │ └── schema-0 └── snapshot ├── EARLIEST ├── LATEST └── snapshot-1 Paimon adopts the same partitioning concept as Apache Hive to separate data. The files of the partition will be placed in a separate partition directory.\nBucket #  The storage of all Paimon tables relies on buckets, and data files are stored in the bucket directory. The relationship between various table types and buckets in Paimon:\n Primary Key Table:  bucket = -1: Default mode, the dynamic bucket mode records which bucket the key corresponds to through the index files. The index records the correspondence between the hash value of the primary-key and the bucket. bucket = 10: The data is distributed to the corresponding buckets according to the hash value of bucket key ( default is primary key).   Append Table:  bucket = -1: Default mode, ignoring bucket concept, although all data is written to bucket-0, the parallelism of reads and writes is unrestricted. bucket = 10: You need to define bucket-key too, the data is distributed to the corresponding buckets according to the hash value of bucket key.    Data File #  The name of data file is data-${uuid}-${id}.${format}. For the append table, the file stores the data of the table without adding any new columns. But for the primary key table, each row of data stores additional system columns:\nTable with Primary key Data File #   Primary key columns, _KEY_ prefix to key columns, this is to avoid conflicts with columns of the table. It\u0026rsquo;s optional, Paimon version 1.0 and above will retrieve the primary key fields from value_columns. _VALUE_KIND: TINYINT, row is deleted or added. Similar to RocksDB, each row of data can be deleted or added, which will be used for updating the primary key table. _SEQUENCE_NUMBER: BIGINT, this number is used for comparison during updates, determining which data came first and which data came later. Value columns. All columns declared in the table.  For example, data file for table:\nCREATE TABLE T ( a INT PRIMARY KEY NOT ENFORCED, b INT, c INT ); Its file has 6 columns: _KEY_a, _VALUE_KIND, _SEQUENCE_NUMBER, a, b, c.\nWhen data-file.thin-mode enabled, its file has 5 columns: _VALUE_KIND, _SEQUENCE_NUMBER, a, b, c.\nTable w/o Primary key Data File #   Value columns. All columns declared in the table.  For example, data file for table:\nCREATE TABLE T ( a INT, b INT, c INT ); Its file has 3 columns: a, b, c.\nChangelog File #  Changelog file and Data file are exactly the same, it only takes effect on the primary key table. It is similar to the Binlog in a database, recording changes to the data in the table.\n"});index.add({'id':64,'href':'/docs/1.1/flink/','title':"Engine Flink",'section':"Apache Paimon",'content':""});index.add({'id':65,'href':'/docs/1.1/maintenance/manage-snapshots/','title':"Manage Snapshots",'section':"Maintenance",'content':"Manage Snapshots #  This section will describe the management and behavior related to snapshots.\nExpire Snapshots #  Paimon writers generate one or two snapshot per commit. Each snapshot may add some new data files or mark some old data files as deleted. However, the marked data files are not truly deleted because Paimon also supports time traveling to an earlier snapshot. They are only deleted when the snapshot expires.\nCurrently, expiration is automatically performed by Paimon writers when committing new changes. By expiring old snapshots, old data files and metadata files that are no longer used can be deleted to release disk space.\nSnapshot expiration is controlled by the following table properties.\n  Option Required Default Type Description     snapshot.time-retained No 1 h Duration The maximum time of completed snapshots to retain.   snapshot.num-retained.min No 10 Integer The minimum number of completed snapshots to retain. Should be greater than or equal to 1.   snapshot.num-retained.max No Integer.MAX_VALUE Integer The maximum number of completed snapshots to retain. Should be greater than or equal to the minimum number.   snapshot.expire.execution-mode No sync Enum Specifies the execution mode of expire.   snapshot.expire.limit No 10 Integer The maximum number of snapshots allowed to expire at a time.    When the number of snapshots is less than snapshot.num-retained.min, no snapshots will be expired(even the condition snapshot.time-retained meet), after which snapshot.num-retained.max and snapshot.time-retained will be used to control the snapshot expiration until the remaining snapshot meets the condition.\nThe following example show more details(snapshot.num-retained.min is 2, snapshot.time-retained is 1h, snapshot.num-retained.max is 5):\n snapshot item is described using tuple (snapshotId, corresponding time)\n    New Snapshots All snapshots after expiration check explanation      (snapshots-1, 2023-07-06 10:00)   (snapshots-1, 2023-07-06 10:00)   No snapshot expired     (snapshots-2, 2023-07-06 10:20)   (snapshots-1, 2023-07-06 10:00) (snapshots-2, 2023-07-06 10:20)   No snapshot expired     (snapshots-3, 2023-07-06 10:40)   (snapshots-1, 2023-07-06 10:00) (snapshots-2, 2023-07-06 10:20) (snapshots-3, 2023-07-06 10:40)   No snapshot expired     (snapshots-4, 2023-07-06 11:00)   (snapshots-1, 2023-07-06 10:00) (snapshots-2, 2023-07-06 10:20) (snapshots-3, 2023-07-06 10:40) (snapshots-4, 2023-07-06 11:00)   No snapshot expired     (snapshots-5, 2023-07-06 11:20)   (snapshots-2, 2023-07-06 10:20) (snapshots-3, 2023-07-06 10:40) (snapshots-4, 2023-07-06 11:00) (snapshots-5, 2023-07-06 11:20)   snapshot-1 was expired because the condition `snapshot.time-retained` is not met     (snapshots-6, 2023-07-06 11:30)   (snapshots-3, 2023-07-06 10:40) (snapshots-4, 2023-07-06 11:00) (snapshots-5, 2023-07-06 11:20) (snapshots-6, 2023-07-06 11:30)   snapshot-2 was expired because the condition `snapshot.time-retained` is not met     (snapshots-7, 2023-07-06 11:35)   (snapshots-3, 2023-07-06 10:40) (snapshots-4, 2023-07-06 11:00) (snapshots-5, 2023-07-06 11:20) (snapshots-6, 2023-07-06 11:30) (snapshots-7, 2023-07-06 11:35)   No snapshot expired     (snapshots-8, 2023-07-06 11:36)   (snapshots-4, 2023-07-06 11:00) (snapshots-5, 2023-07-06 11:20) (snapshots-6, 2023-07-06 11:30) (snapshots-7, 2023-07-06 11:35) (snapshots-8, 2023-07-06 11:36)   snapshot-3 was expired because the condition `snapshot.num-retained.max` is not met     Please note that too short retain time or too small retain number may result in:\n Batch queries cannot find the file. For example, the table is relatively large and the batch query takes 10 minutes to read, but the snapshot from 10 minutes ago expires, at which point the batch query will read a deleted snapshot. Streaming reading jobs on table files fail to restart. When the job restarts, the snapshot it recorded may have expired. (You can use Consumer Id to protect streaming reading in a small retain time of snapshot expiration).  By default, paimon will delete expired snapshots synchronously. When there are too many files that need to be deleted, they may not be deleted quickly and back-pressured to the upstream operator. To avoid this situation, users can use asynchronous expiration mode by setting snapshot.expire.execution-mode to async.\nRollback to Snapshot #  Rollback a table to a specific snapshot ID.\nFlink SQL Run the following command:\nCALL sys.rollback_to(`table` =\u0026gt; \u0026#39;database_name.table_name\u0026#39;, snapshot_id =\u0026gt; \u0026lt;snasphot-id\u0026gt;); Flink Action Run the following command:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  rollback_to \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  --version \u0026lt;snapshot-id\u0026gt; \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] Java API import org.apache.paimon.table.Table; public class RollbackTo { public static void main(String[] args) { // before rollback:  // snapshot-3  // snapshot-4  // snapshot-5  // snapshot-6  // snapshot-7  table.rollbackTo(5); // after rollback:  // snapshot-3  // snapshot-4  // snapshot-5  } } Spark Run the following sql:\nCALL sys.rollback(table =\u0026gt; \u0026#39;database_name.table_name\u0026#39;, snapshot =\u0026gt; snasphot_id);  Remove Orphan Files #  Paimon files are deleted physically only when expiring snapshots. However, it is possible that some unexpected errors occurred when deleting files, so that there may exist files that are not used by Paimon snapshots (so-called \u0026ldquo;orphan files\u0026rdquo;). You can submit a remove_orphan_files job to clean them:\nSpark SQL/Flink SQL CALL sys.remove_orphan_files(`table` =\u0026gt; \u0026#39;my_db.my_table\u0026#39;, [older_than =\u0026gt; \u0026#39;2023-10-31 12:00:00\u0026#39;]) CALL sys.remove_orphan_files(`table` =\u0026gt; \u0026#39;my_db.*\u0026#39;, [older_than =\u0026gt; \u0026#39;2023-10-31 12:00:00\u0026#39;]) Flink Action \u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  remove_orphan_files \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  [--older_than \u0026lt;timestamp\u0026gt;] \\  [--dry_run \u0026lt;false/true\u0026gt;] \\  [--parallelism \u0026lt;parallelism\u0026gt;] To avoid deleting files that are newly added by other writing jobs, this action only deletes orphan files older than 1 day by default. The interval can be modified by --older_than. For example:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  remove_orphan_files \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table T \\  --older_than \u0026#39;2023-10-31 12:00:00\u0026#39; The table can be * to clean all tables in the database.\n "});index.add({'id':66,'href':'/docs/1.1/cdc-ingestion/pulsar-cdc/','title':"Pulsar CDC",'section':"CDC Ingestion",'content':"Pulsar CDC #  Prepare Pulsar Bundled Jar #  flink-connector-pulsar-*.jar Supported Formats #  Flink provides several Pulsar CDC formats: Canal Json, Debezium Json, Debezium Avro, Ogg Json, Maxwell Json and Normal Json. If a message in a pulsar topic is a change event captured from another database using the Change Data Capture (CDC) tool, then you can use the Paimon Pulsar CDC. Write the INSERT, UPDATE, DELETE messages parsed into the paimon table.\n  Formats Supported     Canal CDC True   Debezium CDC True   Maxwell CDC True   OGG CDC True   JSON True    The JSON sources possibly missing some information. For example, Ogg and Maxwell format standards don\u0026rsquo;t contain field types; When you write JSON sources into Flink Pulsar sink, it will only reserve data and row type and drop other information. The synchronization job will try best to handle the problem as follows:\n If missing field types, Paimon will use \u0026lsquo;STRING\u0026rsquo; type as default. If missing database name or table name, you cannot do database synchronization, but you can still do table synchronization. If missing primary keys, the job might create non primary key table. You can set primary keys when submit job in table synchronization.   Synchronizing Tables #  By using PulsarSyncTableAction in a Flink DataStream job or directly through flink run, users can synchronize one or multiple tables from Pulsar\u0026rsquo;s one topic into one Paimon table.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  pulsar_sync_table \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  [--partition_keys \u0026lt;partition_keys\u0026gt;] \\  [--primary_keys \u0026lt;primary-keys\u0026gt;] \\  [--type_mapping to-string] \\  [--computed_column \u0026lt;\u0026#39;column-name=expr-name(args[, ...])\u0026#39;\u0026gt; [--computed_column ...]] \\  [--pulsar_conf \u0026lt;pulsar-source-conf\u0026gt; [--pulsar_conf \u0026lt;pulsar-source-conf\u0026gt; ...]] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; ...]]    Configuration Description     --warehouse The path to Paimon warehouse.   --database The database name in Paimon catalog.   --table The Paimon table name.   --partition_keys The partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example \"dt,hh,mm\".   --primary_keys The primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example \"buyer_id,seller_id\".   --type_mapping It is used to specify how to map MySQL data type to Paimon type.\nSupported options:  \"tinyint1-not-bool\": maps MySQL TINYINT(1) to TINYINT instead of BOOLEAN. \"to-nullable\": ignores all NOT NULL constraints (except for primary keys). This is used to solve the problem that Flink cannot accept the MySQL 'ALTER TABLE ADD COLUMN column type NOT NULL DEFAULT x' operation.  \"to-string\": maps all MySQL types to STRING. \"char-to-string\": maps MySQL CHAR(length)/VARCHAR(length) types to STRING. \"longtext-to-bytes\": maps MySQL LONGTEXT types to BYTES. \"bigint-unsigned-to-bigint\": maps MySQL BIGINT UNSIGNED, BIGINT UNSIGNED ZEROFILL, SERIAL to BIGINT. You should ensure overflow won't occur when using this option.     --computed_column The definitions of computed columns. The argument field is from Pulsar topic's table field name. See here for a complete list of configurations.    --pulsar_conf The configuration for Flink Pulsar sources. Each configuration should be specified in the format `key=value`. `topic/topic-pattern`, `value.format`, `pulsar.client.serviceUrl`, `pulsar.admin.adminUrl`, and `pulsar.consumer.subscriptionName` are required configurations, others are optional.See its document for a complete list of configurations.   --catalog_conf The configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.   --table_conf The configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.    If the Paimon table you specify does not exist, this action will automatically create the table. Its schema will be derived from all specified Pulsar topic\u0026rsquo;s tables,it gets the earliest non-DDL data parsing schema from topic. If the Paimon table already exists, its schema will be compared against the schema of all specified Pulsar topic\u0026rsquo;s tables.\nExample 1:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  pulsar_sync_table \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --partition_keys pt \\  --primary_keys pt,uid \\  --computed_column \u0026#39;_year=year(age)\u0026#39; \\  --pulsar_conf topic=order \\  --pulsar_conf value.format=canal-json \\  --pulsar_conf pulsar.client.serviceUrl=pulsar://127.0.0.1:6650 \\  --pulsar_conf pulsar.admin.adminUrl=http://127.0.0.1:8080 \\  --pulsar_conf pulsar.consumer.subscriptionName=paimon-tests \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --table_conf changelog-producer=input \\  --table_conf sink.parallelism=4 If the Pulsar topic doesn\u0026rsquo;t contain message when you start the synchronization job, you must manually create the table before submitting the job. You can define the partition keys and primary keys only, and the left columns will be added by the synchronization job.\nNOTE: In this case you shouldn\u0026rsquo;t use \u0026ndash;partition_keys or \u0026ndash;primary_keys, because those keys are defined when creating the table and can not be modified. Additionally, if you specified computed columns, you should also define all the argument columns used for computed columns.\nExample 2: If you want to synchronize a table which has primary key \u0026lsquo;id INT\u0026rsquo;, and you want to compute a partition key \u0026lsquo;part=date_format(create_time,yyyy-MM-dd)\u0026rsquo;, you can create a such table first (the other columns can be omitted):\nCREATE TABLE test_db.test_table ( id INT, -- primary key  create_time TIMESTAMP, -- the argument of computed column part  part STRING, -- partition key  PRIMARY KEY (id, part) NOT ENFORCED ) PARTITIONED BY (part); Then you can submit synchronization job:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  pulsar_sync_table \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --computed_column \u0026#39;part=date_format(create_time,yyyy-MM-dd)\u0026#39; \\  ... (other conf) Example 3: For some append data (such as log data), it can be treated as special CDC data with only INSERT operation type, so you can use \u0026lsquo;format=json\u0026rsquo; to synchronize such data to the Paimon table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  kafka_sync_table \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --partition_keys pt \\  --computed_column \u0026#39;pt=date_format(event_tm, yyyyMMdd)\u0026#39; \\  --kafka_conf properties.bootstrap.servers=127.0.0.1:9020 \\  --kafka_conf topic=test_log \\  --kafka_conf properties.group.id=123456 \\  --kafka_conf value.format=json \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf sink.parallelism=4 Synchronizing Databases #  By using PulsarSyncDatabaseAction in a Flink DataStream job or directly through flink run, users can synchronize the multi topic or one topic into one Paimon database.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  pulsar_sync_database \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  [--table_prefix \u0026lt;paimon-table-prefix\u0026gt;] \\  [--table_suffix \u0026lt;paimon-table-suffix\u0026gt;] \\  [--including_tables \u0026lt;table-name|name-regular-expr\u0026gt;] \\  [--excluding_tables \u0026lt;table-name|name-regular-expr\u0026gt;] \\  [--type_mapping to-string] \\  [--partition_keys \u0026lt;partition_keys\u0026gt;] \\  [--primary_keys \u0026lt;primary-keys\u0026gt;] \\  [--pulsar_conf \u0026lt;pulsar-source-conf\u0026gt; [--pulsar_conf \u0026lt;pulsar-source-conf\u0026gt; ...]] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; [--table_conf \u0026lt;paimon-table-sink-conf\u0026gt; ...]]    Configuration Description     --warehouse The path to Paimon warehouse.   --database The database name in Paimon catalog.   --ignore_incompatible It is default false, in this case, if MySQL table name exists in Paimon and their schema is incompatible,an exception will be thrown. You can specify it to true explicitly to ignore the incompatible tables and exception.   --table_prefix The prefix of all Paimon tables to be synchronized. For example, if you want all synchronized tables to have \"ods_\" as prefix, you can specify \"--table_prefix ods_\".   --table_suffix The suffix of all Paimon tables to be synchronized. The usage is same as \"--table_prefix\".   --including_tables It is used to specify which source tables are to be synchronized. You must use '|' to separate multiple tables.Because '|' is a special character, a comma is required, for example: 'a|b|c'.Regular expression is supported, for example, specifying \"--including_tables test|paimon.*\" means to synchronize table 'test' and all tables start with 'paimon'.   --excluding_tables It is used to specify which source tables are not to be synchronized. The usage is same as \"--including_tables\". \"--excluding_tables\" has higher priority than \"--including_tables\" if you specified both.   --type_mapping It is used to specify how to map MySQL data type to Paimon type.\nSupported options:  \"tinyint1-not-bool\": maps MySQL TINYINT(1) to TINYINT instead of BOOLEAN. \"to-nullable\": ignores all NOT NULL constraints (except for primary keys). This is used to solve the problem that Flink cannot accept the MySQL 'ALTER TABLE ADD COLUMN column type NOT NULL DEFAULT x' operation.  \"to-string\": maps all MySQL types to STRING. \"char-to-string\": maps MySQL CHAR(length)/VARCHAR(length) types to STRING. \"longtext-to-bytes\": maps MySQL LONGTEXT types to BYTES. \"bigint-unsigned-to-bigint\": maps MySQL BIGINT UNSIGNED, BIGINT UNSIGNED ZEROFILL, SERIAL to BIGINT. You should ensure overflow won't occur when using this option.     --partition_keys The partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example \"dt,hh,mm\". If the keys are not in source table, the sink table won't set partition keys.   --primary_keys The primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example \"buyer_id,seller_id\". If the keys are not in source table, but the source table has primary keys, the sink table will use source table's primary keys. Otherwise, the sink table won't set primary keys.   --pulsar_conf The configuration for Flink Pulsar sources. Each configuration should be specified in the format `key=value`. `topic/topic-pattern`, `value.format`, `pulsar.client.serviceUrl`, `pulsar.admin.adminUrl`, and `pulsar.consumer.subscriptionName` are required configurations, others are optional.See its document for a complete list of configurations.   --catalog_conf The configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.   --table_conf The configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.    Only tables with primary keys will be synchronized.\nThis action will build a single combined sink for all tables. For each Pulsar topic\u0026rsquo;s table to be synchronized, if the corresponding Paimon table does not exist, this action will automatically create the table, and its schema will be derived from all specified Pulsar topic\u0026rsquo;s tables. If the Paimon table already exists and its schema is different from that parsed from Pulsar record, this action will try to preform schema evolution.\nExample\nSynchronization from one Pulsar topic to Paimon database.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  pulsar_sync_database \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --pulsar_conf topic=order \\  --pulsar_conf value.format=canal-json \\  --pulsar_conf pulsar.client.serviceUrl=pulsar://127.0.0.1:6650 \\  --pulsar_conf pulsar.admin.adminUrl=http://127.0.0.1:8080 \\  --pulsar_conf pulsar.consumer.subscriptionName=paimon-tests \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --table_conf changelog-producer=input \\  --table_conf sink.parallelism=4 Synchronization from multiple Pulsar topics to Paimon database.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  pulsar_sync_database \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --pulsar_conf topic=order,logistic_order,user \\  --pulsar_conf value.format=canal-json \\  --pulsar_conf pulsar.client.serviceUrl=pulsar://127.0.0.1:6650 \\  --pulsar_conf pulsar.admin.adminUrl=http://127.0.0.1:8080 \\  --pulsar_conf pulsar.consumer.subscriptionName=paimon-tests \\  --catalog_conf metastore=hive \\  --catalog_conf uri=thrift://hive-metastore:9083 \\  --table_conf bucket=4 \\  --table_conf changelog-producer=input \\  --table_conf sink.parallelism=4 Additional pulsar_config #  There are some useful options to build Flink Pulsar Source, but they are not provided by flink-pulsar-connector document. They are:\n  Key Default Type Description     value.format (none) String Defines the format identifier for encoding value data.   topic (none) String Topic name(s) from which the data is read. It also supports topic list by separating topic by semicolon like 'topic-1;topic-2'. Note, only one of \"topic-pattern\" and \"topic\" can be specified.    topic-pattern (none) String The regular expression for a pattern of topic names to read from. All topics with names that match the specified regular expression will be subscribed by the consumer when the job starts running. Note, only one of \"topic-pattern\" and \"topic\" can be specified.    pulsar.startCursor.fromMessageId EARLIEST Sting Using a unique identifier of a single message to seek the start position. The common format is a triple '\u0026ltlong\u0026gtledgerId,\u0026ltlong\u0026gtentryId,\u0026ltint\u0026gtpartitionIndex'. Specially, you can set it to EARLIEST (-1, -1, -1) or LATEST (Long.MAX_VALUE, Long.MAX_VALUE, -1).    pulsar.startCursor.fromPublishTime (none) Long Using the message publish time to seek the start position.   pulsar.startCursor.fromMessageIdInclusive true Boolean Whether to include the given message id. This option only works when the message id is not EARLIEST or LATEST.   pulsar.stopCursor.atMessageId (none) String Stop consuming when the message id is equal or greater than the specified message id. Message that is equal to the specified message id will not be consumed. The common format is a triple '\u0026ltlong\u0026gtledgerId,\u0026ltlong\u0026gtentryId,\u0026ltint\u0026gtpartitionIndex'. Specially, you can set it to LATEST (Long.MAX_VALUE, Long.MAX_VALUE, -1).  pulsar.stopCursor.afterMessageId (none) String Stop consuming when the message id is greater than the specified message id. Message that is equal to the specified message id will be consumed. The common format is a triple '\u0026ltlong\u0026gtledgerId,\u0026ltlong\u0026gtentryId,\u0026ltint\u0026gtpartitionIndex'. Specially, you can set it to LATEST (Long.MAX_VALUE, Long.MAX_VALUE, -1).    pulsar.stopCursor.atEventTime (none) Long Stop consuming when message event time is greater than or equals the specified timestamp. Message that even time is equal to the specified timestamp will not be consumed.    pulsar.stopCursor.afterEventTime (none) Long Stop consuming when message event time is greater than the specified timestamp. Message that even time is equal to the specified timestamp will be consumed.    pulsar.source.unbounded true Boolean To specify the boundedness of a stream.   schema.registry.url (none) String When configuring \"value.format=debezium-avro\" which requires using the Confluence schema registry model for Apache Avro serialization, you need to provide the schema registry URL.    "});index.add({'id':67,'href':'/docs/1.1/concepts/rest/','title':"RESTCatalog",'section':"Concepts",'content':""});index.add({'id':68,'href':'/docs/1.1/ecosystem/trino/','title':"Trino",'section':"Ecosystem",'content':"Trino #  This documentation is a guide for using Paimon in Trino.\nVersion #  Paimon currently supports Trino 440.\nFilesystem #  From version 0.8, Paimon share Trino filesystem for all actions, which means, you should config Trino filesystem before using trino-paimon. You can find information about how to config filesystems for Trino on Trino official website.\nPreparing Paimon Jar File #  Download\nYou can also manually build a bundled jar from the source code. However, there are a few preliminary steps that need to be taken before compiling:\n To build from the source code, clone the git repository. Install JDK21 locally, and configure JDK21 as a global environment variable;  Then,you can build bundled jar with the following command:\nmvn clean install -DskipTests You can find Trino connector jar in ./paimon-trino-\u0026lt;trino-version\u0026gt;/target/paimon-trino-\u0026lt;trino-version\u0026gt;-1.1.1-plugin.tar.gz.\nWe use hadoop-apache as a dependency for Hadoop, and the default Hadoop dependency typically supports both Hadoop 2 and Hadoop 3. If you encounter an unsupported scenario, you can specify the corresponding Apache Hadoop version.\nFor example, if you want to use Hadoop 3.3.5-1, you can use the following command to build the jar:\nmvn clean install -DskipTests -Dhadoop.apache.version=3.3.5-1 Configure Paimon Catalog #  Install Paimon Connector #  tar -zxf paimon-trino-\u0026lt;trino-version\u0026gt;-1.1.1-plugin.tar.gz -C ${TRINO_HOME}/plugin  NOTE: For JDK 21, when Deploying Trino, should add jvm options: --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED\n Configure #  Catalogs are registered by creating a catalog properties file in the etc/catalog directory. For example, create etc/catalog/paimon.properties with the following contents to mount the paimon connector as the paimon catalog:\nconnector.name=paimon warehouse=file:/tmp/warehouse If you are using HDFS, choose one of the following ways to configure your HDFS:\n set environment variable HADOOP_HOME. set environment variable HADOOP_CONF_DIR. configure hadoop-conf-dir in the properties.  If you are using a Hadoop filesystem, you can still use trino-hdfs and trino-hive to config it. For example, if you use oss as a storage, you can write in paimon.properties according to Trino Reference:\nhive.config.resources=/path/to/core-site.xml Then, config core-site.xml according to Jindo Reference\nKerberos #  You can configure kerberos keytab file when using KERBEROS authentication in the properties.\nsecurity.kerberos.login.principal=hadoop-user security.kerberos.login.keytab=/etc/trino/hdfs.keytab Keytab files must be distributed to every node in the cluster that runs Trino.\nCreate Schema #  CREATE SCHEMA paimon.test_db; Create Table #  CREATE TABLE paimon.test_db.orders ( order_key bigint, orders_tatus varchar, total_price decimal(18,4), order_date date ) WITH ( file_format = \u0026#39;ORC\u0026#39;, primary_key = ARRAY[\u0026#39;order_key\u0026#39;,\u0026#39;order_date\u0026#39;], partitioned_by = ARRAY[\u0026#39;order_date\u0026#39;], bucket = \u0026#39;2\u0026#39;, bucket_key = \u0026#39;order_key\u0026#39;, changelog_producer = \u0026#39;input\u0026#39; ); Add Column #  CREATE TABLE paimon.test_db.orders ( order_key bigint, orders_tatus varchar, total_price decimal(18,4), order_date date ) WITH ( file_format = \u0026#39;ORC\u0026#39;, primary_key = ARRAY[\u0026#39;order_key\u0026#39;,\u0026#39;order_date\u0026#39;], partitioned_by = ARRAY[\u0026#39;order_date\u0026#39;], bucket = \u0026#39;2\u0026#39;, bucket_key = \u0026#39;order_key\u0026#39;, changelog_producer = \u0026#39;input\u0026#39; ); ALTER TABLE paimon.test_db.orders ADD COLUMN shipping_address varchar; Query #  SELECT * FROM paimon.test_db.orders; Query with Time Traveling #  -- read the snapshot from specified timestamp SELECT * FROM t FOR TIMESTAMP AS OF TIMESTAMP \u0026#39;2023-01-01 00:00:00 Asia/Shanghai\u0026#39;; -- read the snapshot with id 1L (use snapshot id as version) SELECT * FROM t FOR VERSION AS OF 1; -- read tag \u0026#39;my-tag\u0026#39; SELECT * FROM t FOR VERSION AS OF \u0026#39;my-tag\u0026#39;; If tag\u0026rsquo;s name is a number and equals to a snapshot id, the VERSION AS OF syntax will consider tag first. For example, if you have a tag named \u0026lsquo;1\u0026rsquo; based on snapshot 2, the statement SELECT * FROM paimon.test_db.orders FOR VERSION AS OF '1' actually queries snapshot 2 instead of snapshot 1.  Insert #  INSERT INTO paimon.test_db.orders VALUES (.....); Supports:\n primary key table with fixed bucket. non-primary-key table with bucket -1.  Trino to Paimon type mapping #  This section lists all supported type conversion between Trino and Paimon. All Trino\u0026rsquo;s data types are available in package io.trino.spi.type.\n  Trino Data Type Paimon Data Type Atomic Type     RowType RowType false   MapType MapType false   ArrayType ArrayType false   BooleanType BooleanType true   TinyintType TinyIntType true   SmallintType SmallIntType true   IntegerType IntType true   BigintType BigIntType true   RealType FloatType true   DoubleType DoubleType true   CharType(length) CharType(length) true   VarCharType(VarCharType.MAX_LENGTH) VarCharType(VarCharType.MAX_LENGTH) true   VarCharType(length) VarCharType(length), length is less than VarCharType.MAX_LENGTH true   DateType DateType true   TimestampType TimestampType true   DecimalType(precision, scale) DecimalType(precision, scale) true   VarBinaryType(length) VarBinaryType(length) true   TimestampWithTimeZoneType LocalZonedTimestampType true    Tmp Dir #  Paimon will unzip some jars to the tmp directory for codegen. By default, Trino will use '/tmp' as the temporary directory, but '/tmp' may be periodically deleted.\nYou can configure this environment variable when Trino starts:\n-Djava.io.tmpdir=/path/to/other/tmpdir Let Paimon use a secure temporary directory.\n"});index.add({'id':69,'href':'/docs/1.1/ecosystem/amoro/','title':"Amoro",'section':"Ecosystem",'content':"Apache Amoro With Paimon #  Apache Amoro(incubating) is a Lakehouse management system built on open data lake formats. Working with compute engines including Flink, Spark, and Trino, Amoro brings pluggable and Table Maintenance features for a Lakehouse to provide out-of-the-box data warehouse experience, and helps data platforms or products easily build infra-decoupled, stream-and-batch-fused and lake-native architecture. AMS(Amoro Management Service) provides Lakehouse management features, like self-optimizing, data expiration, etc. It also provides a unified catalog service for all compute engines, which can also be combined with existing metadata services like HMS(Hive Metastore).\nTable Format #  Apache Amoro supports all catalog types supported by paimon, including common catalog: Hadoop, Hive, Glue, JDBC, Nessie and other third-party catalog. Amoro supports all storage types supported by Paimon, including common store: Hadoop, S3, GCS, ECS, OSS, and so on.\nIn the future, Paimon automatic optimization strategy will be supported, and users can achieve the best balance experience by cooperating with Amoro automatic optimization\n"});index.add({'id':70,'href':'/docs/1.1/cdc-ingestion/debezium-bson/','title':"Debezium BSON",'section':"CDC Ingestion",'content':"Debezium BSON Format #  The debezium-bson format is one of the formats supported by Kafka CDC. It is the format obtained by collecting mongodb through debezium, which is similar to debezium-json format. However, MongoDB does not have a fixed schema, and the field types of each document may be different, so the before/after fields in JSON are all string types, while the debezium-json format requires a JSON object type.\nPrepare MongoDB BSON Jar #  Can be downloaded from the Maven repository\nbson-*.jar Introduction #  The debezium bson format requires insert/update/delete event messages include the full document, and include a field that represents the state of the document before the change. This requires setting debezium\u0026rsquo;s capture.mode to change_streams_update_full_with_pre_image and capture.mode.full.update.type to post_image. Before version 6.0 of MongoDB, it was not possible to obtain \u0026lsquo;Update Before\u0026rsquo; information. Therefore, using the id field in the Kafka Key as \u0026lsquo;Update before\u0026rsquo; information  Here is a simple example for an update operation captured from a Mongodb customers collection in JSON format:\n{ \u0026#34;schema\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;struct\u0026#34;, \u0026#34;fields\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;optional\u0026#34;: true, \u0026#34;name\u0026#34;: \u0026#34;io.debezium.data.Json\u0026#34;, \u0026#34;version\u0026#34;: 1, \u0026#34;field\u0026#34;: \u0026#34;before\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;optional\u0026#34;: true, \u0026#34;name\u0026#34;: \u0026#34;io.debezium.data.Json\u0026#34;, \u0026#34;version\u0026#34;: 1, \u0026#34;field\u0026#34;: \u0026#34;after\u0026#34; }, ... ] }, \u0026#34;payload\u0026#34;: { \u0026#34;before\u0026#34;: \u0026#34;{\\\u0026#34;_id\\\u0026#34;: {\\\u0026#34;$oid\\\u0026#34; : \\\u0026#34;596e275826f08b2730779e1f\\\u0026#34;}, \\\u0026#34;name\\\u0026#34; : \\\u0026#34;Anne\\\u0026#34;, \\\u0026#34;create_time\\\u0026#34; : {\\\u0026#34;$numberLong\\\u0026#34; : \\\u0026#34;1558965506000\\\u0026#34;}, \\\u0026#34;tags\\\u0026#34;:[\\\u0026#34;success\\\u0026#34;]}\u0026#34;, \u0026#34;after\u0026#34;: \u0026#34;{\\\u0026#34;_id\\\u0026#34;: {\\\u0026#34;$oid\\\u0026#34; : \\\u0026#34;596e275826f08b2730779e1f\\\u0026#34;}, \\\u0026#34;name\\\u0026#34; : \\\u0026#34;Anne\\\u0026#34;, \\\u0026#34;create_time\\\u0026#34; : {\\\u0026#34;$numberLong\\\u0026#34; : \\\u0026#34;1558965506000\\\u0026#34;}, \\\u0026#34;tags\\\u0026#34;:[\\\u0026#34;passion\\\u0026#34;,\\\u0026#34;success\\\u0026#34;]}\u0026#34;, \u0026#34;source\u0026#34;: { \u0026#34;db\u0026#34;: \u0026#34;inventory\u0026#34;, \u0026#34;rs\u0026#34;: \u0026#34;rs0\u0026#34;, \u0026#34;collection\u0026#34;: \u0026#34;customers\u0026#34;, ... }, \u0026#34;op\u0026#34;: \u0026#34;u\u0026#34;, \u0026#34;ts_ms\u0026#34;: 1558965515240, \u0026#34;ts_us\u0026#34;: 1558965515240142, \u0026#34;ts_ns\u0026#34;: 1558965515240142879 } } This document from the MongoDB collection customers has 4 columns, the _id is a BSON ObjectID, name is a string, create_time is a long, tags is an array of string. The following is the processing result in debezium-bson format:\nDocument Schema:\n   Field Name Field Type Key     _id STRING Primary Key   name STRING    create_time STRING    tags STRING     Records:\n   RowKind _id name create_time tags     -U 596e275826f08b2730779e1f Anne 1558965506000 [\u0026ldquo;success\u0026rdquo;]   +U 596e275826f08b2730779e1f Anne 1558965506000 [\u0026ldquo;passion\u0026rdquo;,\u0026ldquo;success\u0026rdquo;]    How it works #  Because the schema field of the event message does not have the field information of the document, the debezium-bson format does not require event messages to have schema information. The specific operations are as follows:\n Parse the before/after fields of the event message into BSONDocument. Recursive traversal all fields of BSONDocument and convert BsonValue to Java Object. All top-level fields of before/after are converted to string type, and _id is fixed to primary key If the top-level fields of before/after is a basic type(such as Integer/Long, etc.), it is directly converted to a string, if not, it is converted to a JSON string  Below is a list of top-level field BsonValue conversion examples:\n  BsonValue Type Json Value Conversion Result String     BsonString \"hello\" \"hello\"   BsonInt32 123 \"123\"   BsonInt64   1735934393769 {\"$numberLong\": \"1735934393769\"}   \"1735934393769\"   BsonDouble   {\"$numberDouble\": \"3.14\"} {\"$numberDouble\": \"NaN\"} {\"$numberDouble\": \"Infinity\"} {\"$numberDouble\": \"-Infinity\"}     \"3.14\" \"NaN\" \"Infinity\" \"-Infinity\"     BsonBoolean   true false     \"true\" \"false\"     BsonArray [1,2,{\"$numberLong\": \"1735934393769\"}] \"[1,2,1735934393769]\"   BsonObjectId {\"$oid\": \"596e275826f08b2730779e1f\"} \"596e275826f08b2730779e1f\"   BsonDateTime {\"$date\": 1735934393769 } \"1735934393769\"   BsonNull null null   BsonUndefined {\"$undefined\": true} null   BsonBinary {\"$binary\": \"uE2/4v5MSVOiJZkOo3APKQ==\", \"$type\": \"0\"} \"uE2/4v5MSVOiJZkOo3APKQ==\"   BsonBinary(type=UUID) {\"$binary\": \"uE2/4v5MSVOiJZkOo3APKQ==\", \"$type\": \"4\"} \"b84dbfe2-fe4c-4953-a225-990ea3700f29\"   BsonDecimal128   {\"$numberDecimal\": \"3.14\"} {\"$numberDecimal\": \"NaN\"}     \"3.14\" \"NaN\"     BsonRegularExpression {\"$regularExpression\": {\"pattern\": \"^pass$\", \"options\": \"i\"}} \"/^pass$/i\"   BsonSymbol {\"$symbol\": \"symbol\"} \"symbol\"   BsonTimestamp {\"$timestamp\": {\"t\": 1736997330, \"i\": 2}} \"1736997330\"   BsonMinKey {\"$minKey\": 1} \"BsonMinKey\"   BsonMaxKey {\"$maxKey\": 1} \"BsonMaxKey\"   BsonJavaScript {\"$code\": \"function(){}\"} \"function(){}\"   BsonJavaScriptWithScope {\"$code\": \"function(){}\", \"$scope\": {\"name\": \"Anne\"}} '{\"$code\": \"function(){}\", \"$scope\": {\"name\": \"Anne\"}}'   BsonDocument   { \"decimalPi\": {\"$numberDecimal\": \"3.14\"}, \"doublePi\": {\"$numberDouble\": \"3.14\"}, \"doubleNaN\": {\"$numberDouble\": \"NaN\"}, \"decimalNaN\": {\"$numberDecimal\": \"NaN\"}, \"long\": {\"$numberLong\": \"100\"}, \"bool\": true, \"array\": [ {\"$numberInt\": \"1\"}, {\"$numberLong\": \"2\"} ] }     '{ \"decimalPi\":3.14, \"doublePi\":3.14, \"doubleNaN\":\"NaN\", \"decimalNaN\":\"NaN\", \"long\":100, \"bool\":true, \"array\":[1,2] }'      How to use #  Use debezium-bson by adding the kafka_conf parameter value.format=debezium-bson. Let’s take table synchronization as an example:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  kafka_sync_table \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --table ods_mongodb_customers \\  --primary_keys _id \\  --kafka_conf properties.bootstrap.servers=127.0.0.1:9020 \\  --kafka_conf topic=customers \\  --kafka_conf properties.group.id=123456 \\  --kafka_conf value.format=debezium-bson \\  --catalog_conf metastore=filesystem \\  --table_conf bucket=4 \\  --table_conf changelog-producer=input \\  --table_conf sink.parallelism=4 "});index.add({'id':71,'href':'/docs/1.1/spark/','title':"Engine Spark",'section':"Apache Paimon",'content':""});index.add({'id':72,'href':'/docs/1.1/primary-key-table/sequence-rowkind/','title':"Sequence \u0026 Rowkind",'section':"Table with PK",'content':"Sequence and Rowkind #  When creating a table, you can specify the 'sequence.field' by specifying fields to determine the order of updates, or you can specify the 'rowkind.field' to determine the changelog kind of record.\nSequence Field #  By default, the primary key table determines the merge order according to the input order (the last input record will be the last to merge). However, in distributed computing, there will be some cases that lead to data disorder. At this time, you can use a time field as sequence.field, for example:\nFlink CREATE TABLE my_table ( pk BIGINT PRIMARY KEY NOT ENFORCED, v1 DOUBLE, v2 BIGINT, update_time TIMESTAMP ) WITH ( \u0026#39;sequence.field\u0026#39; = \u0026#39;update_time\u0026#39; );  The record with the largest sequence.field value will be the last to merge, if the values are the same, the input order will be used to determine which one is the last one. sequence.field supports fields of all data types.\nYou can define multiple fields for sequence.field, for example 'update_time,flag', multiple fields will be compared in order.\nUser defined sequence fields conflict with features such as first_row and first_value, which may result in unexpected results.  Row Kind Field #  By default, the primary key table determines the row kind according to the input row. You can also define the 'rowkind.field' to use a field to extract row kind.\nThe valid row kind string should be '+I', '-U', '+U' or '-D'.\n"});index.add({'id':73,'href':'/docs/1.1/spark/sql-alter/','title':"SQL Alter",'section':"Engine Spark",'content':"Altering Tables #  Changing/Adding Table Properties #  The following SQL sets write-buffer-size table property to 256 MB.\nALTER TABLE my_table SET TBLPROPERTIES ( \u0026#39;write-buffer-size\u0026#39; = \u0026#39;256 MB\u0026#39; ); Removing Table Properties #  The following SQL removes write-buffer-size table property.\nALTER TABLE my_table UNSET TBLPROPERTIES (\u0026#39;write-buffer-size\u0026#39;); Changing/Adding Table Comment #  The following SQL changes comment of table my_table to table comment.\nALTER TABLE my_table SET TBLPROPERTIES ( \u0026#39;comment\u0026#39; = \u0026#39;table comment\u0026#39; ); Removing Table Comment #  The following SQL removes table comment.\nALTER TABLE my_table UNSET TBLPROPERTIES (\u0026#39;comment\u0026#39;); Rename Table Name #  The following SQL rename the table name to new name.\nThe simplest sql to call is:\nALTER TABLE my_table RENAME TO my_table_new; Note that: we can rename paimon table in spark this way:\nALTER TABLE [catalog.[database.]]test1 RENAME to [database.]test2; But we can\u0026rsquo;t put catalog name before the renamed-to table, it will throw an error if we write sql like this:\nALTER TABLE catalog.database.test1 RENAME to catalog.database.test2; If you use object storage, such as S3 or OSS, please use this syntax carefully, because the renaming of object storage is not atomic, and only partial files may be moved in case of failure.  Adding New Columns #  The following SQL adds two columns c1 and c2 to table my_table.\nALTER TABLE my_table ADD COLUMNS ( c1 INT, c2 STRING ); The following SQL adds a nested column f3 to a struct type.\n-- column v previously has type STRUCT\u0026lt;f1: STRING, f2: INT\u0026gt; ALTER TABLE my_table ADD COLUMN v.f3 STRING; The following SQL adds a nested column f3 to a struct type, which is the element type of an array type.\n-- column v previously has type ARRAY\u0026lt;STRUCT\u0026lt;f1: STRING, f2: INT\u0026gt;\u0026gt; ALTER TABLE my_table ADD COLUMN v.element.f3 STRING; The following SQL adds a nested column f3 to a struct type, which is the value type of a map type.\n-- column v previously has type MAP\u0026lt;INT, STRUCT\u0026lt;f1: STRING, f2: INT\u0026gt;\u0026gt; ALTER TABLE my_table ADD COLUMN v.value.f3 STRING; Renaming Column Name #  The following SQL renames column c0 in table my_table to c1.\nALTER TABLE my_table RENAME COLUMN c0 TO c1; The following SQL renames a nested column f1 to f100 in a struct type.\n-- column v previously has type STRUCT\u0026lt;f1: STRING, f2: INT\u0026gt; ALTER TABLE my_table RENAME COLUMN v.f1 to f100; The following SQL renames a nested column f1 to f100 in a struct type, which is the element type of an array type.\n-- column v previously has type ARRAY\u0026lt;STRUCT\u0026lt;f1: STRING, f2: INT\u0026gt;\u0026gt; ALTER TABLE my_table RENAME COLUMN v.element.f1 to f100; The following SQL renames a nested column f1 to f100 in a struct type, which is the value type of a map type.\n-- column v previously has type MAP\u0026lt;INT, STRUCT\u0026lt;f1: STRING, f2: INT\u0026gt;\u0026gt; ALTER TABLE my_table RENAME COLUMN v.value.f1 to f100; Dropping Columns #  The following SQL drops two columns c1 and c2 from table my_table.\nALTER TABLE my_table DROP COLUMNS (c1, c2); The following SQL drops a nested column f2 from a struct type.\n-- column v previously has type STRUCT\u0026lt;f1: STRING, f2: INT\u0026gt; ALTER TABLE my_table DROP COLUMN v.f2; The following SQL drops a nested column f2 from a struct type, which is the element type of an array type.\n-- column v previously has type ARRAY\u0026lt;STRUCT\u0026lt;f1: STRING, f2: INT\u0026gt;\u0026gt; ALTER TABLE my_table DROP COLUMN v.element.f2; The following SQL drops a nested column f2 from a struct type, which is the value type of a map type.\n-- column v previously has type MAP\u0026lt;INT, STRUCT\u0026lt;f1: STRING, f2: INT\u0026gt;\u0026gt; ALTER TABLE my_table DROP COLUMN v.value.f2; In hive catalog, you need to ensure:\n disable hive.metastore.disallow.incompatible.col.type.changes in your hive server or spark-sql --conf spark.hadoop.hive.metastore.disallow.incompatible.col.type.changes=false in your spark.  Otherwise this operation may fail, throws an exception like The following columns have types incompatible with the existing columns in their respective positions.\nDropping Partitions #  The following SQL drops the partitions of the paimon table. For spark sql, you need to specify all the partition columns.\nALTER TABLE my_table DROP PARTITION (`id` = 1, `name` = \u0026#39;paimon\u0026#39;); Adding Partitions #  The following SQL adds the partitions of the paimon table. For spark sql, you need to specify all the partition columns, only with metastore configured metastore.partitioned-table=true.\nALTER TABLE my_table ADD PARTITION (`id` = 1, `name` = \u0026#39;paimon\u0026#39;); Changing Column Comment #  The following SQL changes comment of column buy_count to buy count.\nALTER TABLE my_table ALTER COLUMN buy_count COMMENT \u0026#39;buy count\u0026#39;; Adding Column Position #  ALTER TABLE my_table ADD COLUMN c INT FIRST; ALTER TABLE my_table ADD COLUMN c INT AFTER b; Changing Column Position #  ALTER TABLE my_table ALTER COLUMN col_a FIRST; ALTER TABLE my_table ALTER COLUMN col_a AFTER col_b; Changing Column Type #  ALTER TABLE my_table ALTER COLUMN col_a TYPE DOUBLE; The following SQL changes the type of a nested column f2 to BIGINT in a struct type.\n-- column v previously has type STRUCT\u0026lt;f1: STRING, f2: INT\u0026gt; ALTER TABLE my_table ALTER COLUMN v.f2 TYPE BIGINT; The following SQL changes the type of a nested column f2 to BIGINT in a struct type, which is the element type of an array type.\n-- column v previously has type ARRAY\u0026lt;STRUCT\u0026lt;f1: STRING, f2: INT\u0026gt;\u0026gt; ALTER TABLE my_table ALTER COLUMN v.element.f2 TYPE BIGINT; The following SQL changes the type of a nested column f2 to BIGINT in a struct type, which is the value type of a map type.\n-- column v previously has type MAP\u0026lt;INT, STRUCT\u0026lt;f1: STRING, f2: INT\u0026gt;\u0026gt; ALTER TABLE my_table ALTER COLUMN v.value.f2 TYPE BIGINT; ALTER DATABASE #  The following SQL sets one or more properties in the specified database. If a particular property is already set in the database, override the old value with the new one.\nALTER { DATABASE | SCHEMA | NAMESPACE } my_database SET { DBPROPERTIES | PROPERTIES } ( property_name = property_value [ , ... ] ) Altering Database Location #  The following SQL sets the location of the specified database to file:/temp/my_database.db.\nALTER DATABASE my_database SET LOCATION \u0026#39;file:/temp/my_database.db\u0026#39; "});index.add({'id':74,'href':'/docs/1.1/flink/sql-lookup/','title':"SQL Lookup",'section':"Engine Flink",'content':"Lookup Joins #  Lookup Joins are a type of join in streaming queries. It is used to enrich a table with data that is queried from Paimon. The join requires one table to have a processing time attribute and the other table to be backed by a lookup source connector.\nPaimon supports lookup joins on tables with primary keys and append tables in Flink. The following example illustrates this feature.\nPrepare #  First, let\u0026rsquo;s create a Paimon table and update it in real-time.\n-- Create a paimon catalog CREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39;=\u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39;=\u0026#39;hdfs://nn:8020/warehouse/path\u0026#39; -- or \u0026#39;file://tmp/foo/bar\u0026#39; ); USE CATALOG my_catalog; -- Create a table in paimon catalog CREATE TABLE customers ( id INT PRIMARY KEY NOT ENFORCED, name STRING, country STRING, zip STRING ); -- Launch a streaming job to update customers table INSERT INTO customers ... -- Create a temporary left table, like from kafka CREATE TEMPORARY TABLE orders ( order_id INT, total INT, customer_id INT, proc_time AS PROCTIME() ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ... ); Normal Lookup #  You can now use customers in a lookup join query.\n-- enrich each order with customer information SELECT o.order_id, o.total, c.country, c.zip FROM orders AS o JOIN customers FOR SYSTEM_TIME AS OF o.proc_time AS c ON o.customer_id = c.id; Retry Lookup #  If the records of orders (main table) join missing because the corresponding data of customers (lookup table) is not ready. You can consider using Flink\u0026rsquo;s Delayed Retry Strategy For Lookup. Only for Flink 1.16+.\n-- enrich each order with customer information SELECT /*+ LOOKUP(\u0026#39;table\u0026#39;=\u0026#39;c\u0026#39;, \u0026#39;retry-predicate\u0026#39;=\u0026#39;lookup_miss\u0026#39;, \u0026#39;retry-strategy\u0026#39;=\u0026#39;fixed_delay\u0026#39;, \u0026#39;fixed-delay\u0026#39;=\u0026#39;1s\u0026#39;, \u0026#39;max-attempts\u0026#39;=\u0026#39;600\u0026#39;) */ o.order_id, o.total, c.country, c.zip FROM orders AS o JOIN customers FOR SYSTEM_TIME AS OF o.proc_time AS c ON o.customer_id = c.id; Async Retry Lookup #  The problem with synchronous retry is that one record will block subsequent records, causing the entire job to be blocked. You can consider using async + allow_unordered to avoid blocking, the records that join missing will no longer block other records.\n-- enrich each order with customer information SELECT /*+ LOOKUP(\u0026#39;table\u0026#39;=\u0026#39;c\u0026#39;, \u0026#39;retry-predicate\u0026#39;=\u0026#39;lookup_miss\u0026#39;, \u0026#39;output-mode\u0026#39;=\u0026#39;allow_unordered\u0026#39;, \u0026#39;retry-strategy\u0026#39;=\u0026#39;fixed_delay\u0026#39;, \u0026#39;fixed-delay\u0026#39;=\u0026#39;1s\u0026#39;, \u0026#39;max-attempts\u0026#39;=\u0026#39;600\u0026#39;) */ o.order_id, o.total, c.country, c.zip FROM orders AS o JOIN customers /*+ OPTIONS(\u0026#39;lookup.async\u0026#39;=\u0026#39;true\u0026#39;, \u0026#39;lookup.async-thread-number\u0026#39;=\u0026#39;16\u0026#39;) */ FOR SYSTEM_TIME AS OF o.proc_time AS c ON o.customer_id = c.id; If the main table (orders) is CDC stream, allow_unordered will be ignored by Flink SQL (only supports append stream), your streaming job may be blocked. You can try to use audit_log system table feature of Paimon to walk around (convert CDC stream to append stream).  Dynamic Partition #  In traditional data warehouses, each partition often maintains the latest full data, so this partition table only needs to join the latest partition. Paimon has specifically developed the max_pt feature for this scenario.\nCreate Paimon Partitioned Table\nCREATE TABLE customers ( id INT, name STRING, country STRING, zip STRING, dt STRING, PRIMARY KEY (id, dt) NOT ENFORCED ) PARTITIONED BY (dt); Lookup Join\nSELECT o.order_id, o.total, c.country, c.zip FROM orders AS o JOIN customers /*+ OPTIONS(\u0026#39;scan.partitions\u0026#39;=\u0026#39;max_pt()\u0026#39;, \u0026#39;lookup.dynamic-partition.refresh-interval\u0026#39;=\u0026#39;1 h\u0026#39;) */ FOR SYSTEM_TIME AS OF o.proc_time AS c ON o.customer_id = c.id; The Lookup node will automatically refresh the latest partition and query the data of the latest partition.\nThe option scan.partitions can also specify fixed partitions in the form of key1=value1,key2=value2. Multiple partitions should be separated by semicolon (;). When specifying fixed partitions, this option can also be used in batch joins.\nQuery Service #  You can run a Flink Streaming Job to start query service for the table. When QueryService exists, Flink Lookup Join will prioritize obtaining data from it, which will effectively improve query performance.\nFlink SQL CALL sys.query_service(\u0026#39;database_name.table_name\u0026#39;, parallelism); Flink Action \u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  query_service \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  [--parallelism \u0026lt;parallelism\u0026gt;] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]]  "});index.add({'id':75,'href':'/docs/1.1/concepts/spec/tableindex/','title':"Table Index",'section':"Specification",'content':"Table index #  Table Index files is in the index directory.\nDynamic Bucket Index #  Dynamic bucket index is used to store the correspondence between the hash value of the primary-key and the bucket.\nIts structure is very simple, only storing hash values in the file:\nHASH_VALUE | HASH_VALUE | HASH_VALUE | HASH_VALUE | \u0026hellip;\nHASH_VALUE is the hash value of the primary-key. 4 bytes, BIG_ENDIAN.\nDeletion Vectors #  Deletion file is used to store the deleted records position for each data file. Each bucket has one deletion file for primary key table.\nThe deletion file is a binary file, and the format is as follows:\n First, record version by a byte. Current version is 1. Then, record \u0026lt;size of serialized bin, serialized bin, checksum of serialized bin\u0026gt; in sequence. Size and checksum are BIG_ENDIAN Integer.  For each serialized bin:\n First, record a const magic number by an int (BIG_ENDIAN). Current the magic number is 1581511376. Then, record serialized bitmap. Which is a RoaringBitmap (org.roaringbitmap.RoaringBitmap).  "});index.add({'id':76,'href':'/docs/1.1/concepts/table-types/','title':"Table Types",'section':"Concepts",'content':"Table Types #  Paimon supports table types:\n table with pk: Paimon Data Table with Primary key table w/o pk: Paimon Data Table without Primary key view: metastore required, views in SQL are a kind of virtual table format-table: file format table refers to a directory that contains multiple files of the same format, where operations on this table allow for reading or writing to these files, compatible with Hive tables object table: provides metadata indexes for unstructured data objects in the specified Object Storage directory. materialized-table: aimed at simplifying both batch and stream data pipelines, providing a consistent development experience, see Flink Materialized Table  Table with PK #  See Paimon with Primary key.\nPrimary keys consist of a set of columns that contain unique values for each record. Paimon enforces data ordering by sorting the primary key within each bucket, allowing streaming update and streaming changelog read.\nThe definition of primary key is similar to that of standard SQL, as it ensures that there is only one data entry for the same primary key during batch queries.\nFlink SQL CREATE TABLE my_table ( a INT PRIMARY KEY NOT ENFORCED, b STRING ) WITH ( \u0026#39;bucket\u0026#39;=\u0026#39;8\u0026#39; ) Spark SQL CREATE TABLE my_table ( a INT, b STRING ) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;a\u0026#39;, \u0026#39;bucket\u0026#39; = \u0026#39;8\u0026#39; )  Table w/o PK #  See Paimon w/o Primary key.\nIf a table does not have a primary key defined, it is an append table. Compared to the primary key table, it does not have the ability to directly receive changelogs. It cannot be directly updated with data through streaming upsert. It can only receive incoming data from append data.\nHowever, it also supports batch sql: DELETE, UPDATE, and MERGE-INTO.\nCREATE TABLE my_table ( a INT, b STRING ) View #  View is supported when the metastore can support view, for example, hive metastore. If you don\u0026rsquo;t have metastore, you can only use temporary View, which only exists in the current session. This chapter mainly describes persistent views.\nView will currently save the original SQL. If you need to use View across engines, you can write a cross engine SQL statement. For example:\nFlink SQL CREATE VIEW [IF NOT EXISTS] [catalog_name.][db_name.]view_name [( columnName [, columnName ]* )] [COMMENT view_comment] AS query_expression; DROP VIEW [IF EXISTS] [catalog_name.][db_name.]view_name; SHOW VIEWS; SHOW CREATE VIEW my_view; Spark SQL CREATE [OR REPLACE] VIEW [IF NOT EXISTS] [catalog_name.][db_name.]view_name [( columnName [, columnName ]* )] [COMMENT view_comment] AS query_expression; DROP VIEW [IF EXISTS] [catalog_name.][db_name.]view_name; SHOW VIEWS;  Format Table #  Format table is supported when the metastore can support format table, for example, hive metastore. The Hive tables inside the metastore will be mapped to Paimon\u0026rsquo;s Format Table for computing engines (Spark, Hive, Flink) to read and write.\nFormat table refers to a directory that contains multiple files of the same format, where operations on this table allow for reading or writing to these files, facilitating the retrieval of existing data and the addition of new files.\nPartitioned file format table just like the standard hive format. Partitions are discovered and inferred based on directory structure.\nFormat Table is enabled by default, you can disable it by configuring Catalog option: 'format-table.enabled'.\nCurrently only support CSV, Parquet, ORC, JSON formats.\nFlink-CSV CREATE TABLE my_csv_table ( a INT, b STRING ) WITH ( \u0026#39;type\u0026#39;=\u0026#39;format-table\u0026#39;, \u0026#39;file.format\u0026#39;=\u0026#39;csv\u0026#39;, \u0026#39;field-delimiter\u0026#39;=\u0026#39;,\u0026#39; ) Spark-CSV CREATE TABLE my_csv_table ( a INT, b STRING ) USING csv OPTIONS (\u0026#39;field-delimiter\u0026#39; \u0026#39;,\u0026#39;) Flink-Parquet CREATE TABLE my_parquet_table ( a INT, b STRING ) WITH ( \u0026#39;type\u0026#39;=\u0026#39;format-table\u0026#39;, \u0026#39;file.format\u0026#39;=\u0026#39;parquet\u0026#39; ) Spark-Parquet CREATE TABLE my_parquet_table ( a INT, b STRING ) USING parquet Flink-JSON CREATE TABLE my_json_table ( a INT, b STRING ) WITH ( \u0026#39;type\u0026#39;=\u0026#39;format-table\u0026#39;, \u0026#39;file.format\u0026#39;=\u0026#39;json\u0026#39; ) Spark-JSON CREATE TABLE my_json_table ( a INT, b STRING ) USING json  Object Table #  Object Table provides metadata indexes for unstructured data objects in the specified Object Storage storage directory. Object tables allow users to analyze unstructured data in Object Storage:\n Use Python API to manipulate these unstructured data, such as converting images to PDF format. Model functions can also be used to perform inference, and then the results of these operations can be concatenated with other structured data in the Catalog.  The object table is managed by Catalog and can also have access permissions and the ability to manage blood relations.\nFlink-SQL -- Create Object Table  CREATE TABLE `my_object_table` WITH ( \u0026#39;type\u0026#39; = \u0026#39;object-table\u0026#39;, \u0026#39;object-location\u0026#39; = \u0026#39;oss://my_bucket/my_location\u0026#39; ); -- Refresh Object Table  CALL sys.refresh_object_table(\u0026#39;mydb.my_object_table\u0026#39;); -- Query Object Table  SELECT * FROM `my_object_table`; -- Query Object Table with Time Travel  SELECT * FROM `my_object_table` /*+ OPTIONS(\u0026#39;scan.snapshot-id\u0026#39; = \u0026#39;1\u0026#39;) */; Spark-SQL -- Create Object Table  CREATE TABLE `my_object_table` TBLPROPERTIES ( \u0026#39;type\u0026#39; = \u0026#39;object-table\u0026#39;, \u0026#39;object-location\u0026#39; = \u0026#39;oss://my_bucket/my_location\u0026#39; ); -- Refresh Object Table  CALL sys.refresh_object_table(\u0026#39;mydb.my_object_table\u0026#39;); -- Query Object Table  SELECT * FROM `my_object_table`; -- Query Object Table with Time Travel  SELECT * FROM `my_object_table` VERSION AS OF 1;  Materialized Table #  Materialized Table aimed at simplifying both batch and stream data pipelines, providing a consistent development experience, see Flink Materialized Table.\nNow only Flink SQL integrate to Materialized Table, we plan to support it in Spark SQL too.\nCREATE MATERIALIZED TABLE continuous_users_shops PARTITIONED BY (ds) FRESHNESS = INTERVAL \u0026#39;30\u0026#39; SECOND AS SELECT user_id, ds, SUM (payment_amount_cents) AS payed_buy_fee_sum, SUM (1) AS PV FROM ( SELECT user_id, order_created_at AS ds, payment_amount_cents FROM json_source ) AS tmp GROUP BY user_id, ds; "});index.add({'id':77,'href':'/docs/1.1/spark/auxiliary/','title':"Auxiliary",'section':"Engine Spark",'content':"Auxiliary Statements #  Set / Reset #  The SET command sets a property, returns the value of an existing property or returns all SQLConf properties with value and meaning. The RESET command resets runtime configurations specific to the current session which were set via the SET command to their default values.\nTo set dynamic options globally, you need add the spark.paimon. prefix. You can also set dynamic table options at this format: spark.paimon.${catalogName}.${dbName}.${tableName}.${config_key}. The catalogName/dbName/tableName can be *, which means matching all the specific parts. Dynamic table options will override global options if there are conflicts.\n-- set spark conf SET spark.sql.sources.partitionOverwriteMode=dynamic; -- set paimon conf SET spark.paimon.file.block-size=512M; -- reset conf RESET spark.paimon.file.block-size; -- set scan.snapshot-id=1 for the table default.T in any catalogs SET spark.paimon.*.default.T.scan.snapshot-id=1; SELECT * FROM default.T; -- set scan.snapshot-id=1 for the table T in any databases and catalogs SET spark.paimon.*.*.T.scan.snapshot-id=1; SELECT * FROM default.T; -- set scan.snapshot-id=2 for the table default.T1 in any catalogs and scan.snapshot-id=1 on other tables SET spark.paimon.scan.snapshot-id=1; SET spark.paimon.*.default.T1.scan.snapshot-id=2; SELECT * FROM default.T1 JOIN default.T2 ON xxxx; Describe table #  DESCRIBE TABLE statement returns the basic metadata information of a table or view. The metadata information includes column name, column type and column comment.\n-- describe table or view DESCRIBE TABLE my_table; -- describe table or view with additional metadata DESCRIBE TABLE EXTENDED my_table; Show create table #  SHOW CREATE TABLE returns the CREATE TABLE statement or CREATE VIEW statement that was used to create a given table or view.\nSHOW CREATE TABLE my_table; Show columns #  Returns the list of columns in a table. If the table does not exist, an exception is thrown.\nSHOW COLUMNS FROM my_table; Show partitions #  The SHOW PARTITIONS statement is used to list partitions of a table. An optional partition spec may be specified to return the partitions matching the supplied partition spec.\n-- Lists all partitions for my_table SHOW PARTITIONS my_table; -- Lists partitions matching the supplied partition spec for my_table SHOW PARTITIONS my_table PARTITION (dt=\u0026#39;20230817\u0026#39;); Show Table Extended #  The SHOW TABLE EXTENDED statement is used to list table or partition information.\n-- Lists tables that satisfy regular expressions SHOW TABLE EXTENDED IN db_name LIKE \u0026#39;test*\u0026#39;; -- Lists the specified partition information for the table SHOW TABLE EXTENDED IN db_name LIKE \u0026#39;table_name\u0026#39; PARTITION(pt = \u0026#39;2024\u0026#39;); Show views #  The SHOW VIEWS statement returns all the views for an optionally specified database.\n-- Lists all views SHOW VIEWS; -- Lists all views that satisfy regular expressions SHOW VIEWS LIKE \u0026#39;test*\u0026#39;; Analyze table #  The ANALYZE TABLE statement collects statistics about the table, that are to be used by the query optimizer to find a better query execution plan. Paimon supports collecting table-level statistics and column statistics through analyze.\n-- collect table-level statistics ANALYZE TABLE my_table COMPUTE STATISTICS; -- collect table-level statistics and column statistics for col1 ANALYZE TABLE my_table COMPUTE STATISTICS FOR COLUMNS col1; -- collect table-level statistics and column statistics for all columns ANALYZE TABLE my_table COMPUTE STATISTICS FOR ALL COLUMNS; "});index.add({'id':78,'href':'/docs/1.1/primary-key-table/compaction/','title':"Compaction",'section':"Table with PK",'content':"Compaction #  When more and more records are written into the LSM tree, the number of sorted runs will increase. Because querying an LSM tree requires all sorted runs to be combined, too many sorted runs will result in a poor query performance, or even out of memory.\nTo limit the number of sorted runs, we have to merge several sorted runs into one big sorted run once in a while. This procedure is called compaction.\nHowever, compaction is a resource intensive procedure which consumes a certain amount of CPU time and disk IO, so too frequent compaction may in turn result in slower writes. It is a trade-off between query and write performance. Paimon currently adopts a compaction strategy similar to Rocksdb\u0026rsquo;s universal compaction.\nCompaction solves:\n Reduce Level 0 files to avoid poor query performance. Produce changelog via changelog-producer. Produce deletion vectors for MOW mode. Snapshot Expiration, Tag Expiration, Partitions Expiration.  Limitation:\n There can only be one job working on the same partition\u0026rsquo;s compaction, otherwise it will cause conflicts and one side will throw an exception failure.  Writing performance is almost always affected by compaction, so its tuning is crucial.\nAsynchronous Compaction #  Compaction is inherently asynchronous, but if you want it to be completely asynchronous without blocking writes, expecting a mode for maximum writing throughput, the compaction can be done slowly and not in a hurry. You can use the following strategies for your table:\nnum-sorted-run.stop-trigger = 2147483647 sort-spill-threshold = 10 lookup-wait = false This configuration will generate more files during peak write periods and gradually merge them for optimal read performance during low write periods.\nDedicated compaction job #  In general, if you expect multiple jobs to be written to the same table, you need to separate the compaction. You can use dedicated compaction job.\nRecord-Level expire #  In compaction, you can configure record-Level expire time to expire records, you should configure:\n 'record-level.expire-time': time retain for records. 'record-level.time-field': time field for record level expire.  Expiration happens in compaction, and there is no strong guarantee to expire records in time. You can trigger a full compaction manually to expire records which were not expired in time.\nFull Compaction #  Paimon Compaction uses Universal-Compaction. By default, when there is too much incremental data, Full Compaction will be automatically performed. You don\u0026rsquo;t usually have to worry about it.\nPaimon also provides a configuration that allows for regular execution of Full Compaction.\n \u0026lsquo;compaction.optimization-interval\u0026rsquo;: Implying how often to perform an optimization full compaction, this configuration is used to ensure the query timeliness of the read-optimized system table. \u0026lsquo;full-compaction.delta-commits\u0026rsquo;: Full compaction will be constantly triggered after delta commits. Its disadvantage is that it can only perform compaction synchronously, which will affect writing efficiency.  Lookup Compaction #  When primary key table is configured with lookup changelog producer or first-row merge-engine or has enabled deletion vectors for MOW mode, Paimon will use a radical compaction strategy to force compacting level 0 files to higher levels for every compaction trigger.\nPaimon also provides configurations to optimize the frequency of this compaction.\n \u0026lsquo;lookup-compact\u0026rsquo;: compact mode used for lookup compaction. Possible values: radical, will use ForceUpLevel0Compaction strategy to radically compact new files; gentle, will use UniversalCompaction strategy to gently compact new files; \u0026lsquo;lookup-compact.max-interval\u0026rsquo;: The max interval for a forced L0 lookup compaction to be triggered in gentle mode. This option is only valid when lookup-compact mode is gentle.  By configuring \u0026lsquo;lookup-compact\u0026rsquo; as gentle, new files in L0 will not be compacted immediately, this may greatly reduce the overall resource usage at the expense of worse data freshness in certain cases.\nCompaction Options #  Number of Sorted Runs to Pause Writing #  When the number of sorted runs is small, Paimon writers will perform compaction asynchronously in separated threads, so records can be continuously written into the table. However, to avoid unbounded growth of sorted runs, writers will pause writing when the number of sorted runs hits the threshold. The following table property determines the threshold.\n  Option Required Default Type Description     num-sorted-run.stop-trigger No (none) Integer The number of sorted runs that trigger the stopping of writes, the default value is 'num-sorted-run.compaction-trigger' + 3.    Write stalls will become less frequent when num-sorted-run.stop-trigger becomes larger, thus improving writing performance. However, if this value becomes too large, more memory and CPU time will be needed when querying the table. If you are concerned about the OOM problem, please configure the following option. Its value depends on your memory size.\n  Option Required Default Type Description     sort-spill-threshold No (none) Integer If the maximum number of sort readers exceeds this value, a spill will be attempted. This prevents too many readers from consuming too much memory and causing OOM.    Number of Sorted Runs to Trigger Compaction #  Paimon uses LSM tree which supports a large number of updates. LSM organizes files in several sorted runs. When querying records from an LSM tree, all sorted runs must be combined to produce a complete view of all records.\nOne can easily see that too many sorted runs will result in poor query performance. To keep the number of sorted runs in a reasonable range, Paimon writers will automatically perform compactions. The following table property determines the minimum number of sorted runs to trigger a compaction.\n  Option Required Default Type Description     num-sorted-run.compaction-trigger No 5 Integer The sorted run number to trigger compaction. Includes level0 files (one file one sorted run) and high-level runs (one level one sorted run).    Compaction will become less frequent when num-sorted-run.compaction-trigger becomes larger, thus improving writing performance. However, if this value becomes too large, more memory and CPU time will be needed when querying the table. This is a trade-off between writing and query performance.\n"});index.add({'id':79,'href':'/docs/1.1/concepts/spec/fileindex/','title':"File Index",'section':"Specification",'content':"File index #  Define file-index.${index_type}.columns, Paimon will create its corresponding index file for each file. If the index file is too small, it will be stored directly in the manifest, or in the directory of the data file. Each data file corresponds to an index file, which has a separate file definition and can contain different types of indexes with multiple columns.\nIndex File #  File index file format. Put all column and offset in the header.\n ______________________________________ _____________________ | magic ｜version｜head length | |--------------------------------------| | column number | |--------------------------------------| | column 1 ｜ index number | |--------------------------------------| | index name 1 ｜start pos ｜length | |--------------------------------------| | index name 2 ｜start pos ｜length | |--------------------------------------| | index name 3 ｜start pos ｜length | |--------------------------------------| HEAD | column 2 ｜ index number | |--------------------------------------| | index name 1 ｜start pos ｜length | |--------------------------------------| | index name 2 ｜start pos ｜length | |--------------------------------------| | index name 3 ｜start pos ｜length | |--------------------------------------| | ... | |--------------------------------------| | ... | |--------------------------------------| | redundant length ｜redundant bytes | |--------------------------------------| --------------------- | BODY | | BODY | | BODY | BODY | BODY | |______________________________________| _____________________ * magic: 8 bytes long, value is 1493475289347502L, BIG_ENDIAN version: 4 bytes int, BIG_ENDIAN head length: 4 bytes int, BIG_ENDIAN column number: 4 bytes int, BIG_ENDIAN column x name: 2 bytes short BIG_ENDIAN and Java modified-utf-8 index number: 4 bytes int (how many column items below), BIG_ENDIAN index name x: 2 bytes short BIG_ENDIAN and Java modified-utf-8 start pos: 4 bytes int, BIG_ENDIAN length: 4 bytes int, BIG_ENDIAN redundant length: 4 bytes int (for compatibility with later versions, in this version, content is zero) redundant bytes: var bytes (for compatibility with later version, in this version, is empty) BODY: column index bytes + column index bytes + column index bytes + .......  Index: BloomFilter #  Define 'file-index.bloom-filter.columns'.\nContent of bloom filter index is simple:\n numHashFunctions 4 bytes int, BIG_ENDIAN bloom filter bytes  This class use (64-bits) long hash. Store the num hash function (one integer) and bit set bytes only. Hash bytes type (like varchar, binary, etc.) using xx hash, hash numeric type by specified number hash.\nIndex: Bitmap #   file-index.bitmap.columns: specify the columns that need bitmap index. file-index.bitmap.\u0026lt;column_name\u0026gt;.index-block-size: to config secondary index block size, default value is 16kb.  V2 Bitmap file index format (V2):\n Bitmap file index format (V2) +-------------------------------------------------+----------------- ｜ version (1 byte) = 2 ｜ +-------------------------------------------------+ ｜ row count (4 bytes int) ｜ +-------------------------------------------------+ ｜ non-null value bitmap number (4 bytes int) ｜ +-------------------------------------------------+ ｜ has null value (1 byte) ｜ +-------------------------------------------------+ ｜ null value offset (4 bytes if has null value) ｜ HEAD +-------------------------------------------------+ ｜ null bitmap length (4 bytes if has null value) ｜ +-------------------------------------------------+ ｜ bitmap index block number (4 bytes int) ｜ +-------------------------------------------------+ ｜ value 1 | offset 1 ｜ +-------------------------------------------------+ ｜ value 2 | offset 2 ｜ +-------------------------------------------------+ ｜ ... ｜ +-------------------------------------------------+ ｜ bitmap body offset (4 bytes int) ｜ +-------------------------------------------------+----------------- ｜ bitmap index block 1 ｜ +-------------------------------------------------+ ｜ bitmap index block 2 ｜ INDEX BLOCKS +-------------------------------------------------+ ｜ ... ｜ +-------------------------------------------------+----------------- ｜ serialized bitmap 1 ｜ +-------------------------------------------------+ ｜ serialized bitmap 2 ｜ +-------------------------------------------------+ BITMAP BLOCKS ｜ serialized bitmap 3 ｜ +-------------------------------------------------+ ｜ ... ｜ +-------------------------------------------------+----------------- index block format: +-------------------------------------------------+ ｜ entry number (4 bytes int) ｜ +-------------------------------------------------+ ｜ value 1 | offset 1 | length 1 ｜ +-------------------------------------------------+ ｜ value 2 | offset 2 | length 2 ｜ +-------------------------------------------------+ ｜ ... ｜ +-------------------------------------------------+ value x: var bytes for any data type (as bitmap identifier) offset: 4 bytes int (when it is negative, it represents that there is only one value and its position is the inverse of the negative value) length: 4 bytes int  V1 (Legacy) (Legacy) Bitmap file index format (V1):\nYou can configure file-index.bitmap.version to use legacy bitmap version 1.\n Bitmap file index format (V1) +-------------------------------------------------+----------------- | version (1 byte) | +-------------------------------------------------+ | row count (4 bytes int) | +-------------------------------------------------+ | non-null value bitmap number (4 bytes int) | +-------------------------------------------------+ | has null value (1 byte) | +-------------------------------------------------+ | null value offset (4 bytes if has null value) | HEAD +-------------------------------------------------+ | value 1 | offset 1 | +-------------------------------------------------+ | value 2 | offset 2 | +-------------------------------------------------+ | value 3 | offset 3 | +-------------------------------------------------+ | ... | +-------------------------------------------------+----------------- | serialized bitmap 1 | +-------------------------------------------------+ | serialized bitmap 2 | +-------------------------------------------------+ BODY | serialized bitmap 3 | +-------------------------------------------------+ | ... | +-------------------------------------------------+----------------- * value x: var bytes for any data type (as bitmap identifier) offset: 4 bytes int (when it is negative, it represents that there is only one value and its position is the inverse of the negative value)   Integers are all BIG_ENDIAN.\nBitmap only support the following data type: TinyIntType, SmallIntType, IntType, BigIntType, DateType, TimeType, LocalZonedTimestampType, TimestampType, CharType, VarCharType, StringType, BooleanType.\nIndex: Bit-Slice Index Bitmap #  BSI file index is a numeric range index, used to accelerate range query, it can be used with bitmap index.\nDefine 'file-index.bsi.columns'.\nBSI file index format (V1):\n BSI file index format (V1) +-------------------------------------------------+ | version (1 byte) | +-------------------------------------------------+ | row count (4 bytes int) | +-------------------------------------------------+ | has positive value (1 byte) | +-------------------------------------------------+ | positive BSI serialized (if has positive value) | +-------------------------------------------------+ | has negative value (1 byte) | +-------------------------------------------------+ | negative BSI serialized (if has negative value) | +-------------------------------------------------+  BSI serialized format (V1):\n BSI serialized format (V1) +-------------------------------------------------+ | version (1 byte) | +-------------------------------------------------+ | min value (8 bytes long) | +-------------------------------------------------+ | max value (8 bytes long) | +-------------------------------------------------+ | serialized existence bitmap | +-------------------------------------------------+ | bit slice bitmap count (4 bytes int) | +-------------------------------------------------+ | serialized bit 0 bitmap | +-------------------------------------------------+ | serialized bit 1 bitmap | +-------------------------------------------------+ | serialized bit 2 bitmap | +-------------------------------------------------+ | ... | +-------------------------------------------------+  BSI only support the following data type: TinyIntType, SmallIntType, IntType, BigIntType, DateType, LocalZonedTimestamp, TimestampType, DecimalType.\n"});index.add({'id':80,'href':'/docs/1.1/maintenance/rescale-bucket/','title':"Rescale Bucket",'section':"Maintenance",'content':"Rescale Bucket #  Since the number of total buckets dramatically influences the performance, Paimon allows users to tune bucket numbers by ALTER TABLE command and reorganize data layout by INSERT OVERWRITE without recreating the table/partition. When executing overwrite jobs, the framework will automatically scan the data with the old bucket number and hash the record according to the current bucket number.\nRescale Overwrite #  -- rescale number of total buckets ALTER TABLE table_identifier SET (\u0026#39;bucket\u0026#39; = \u0026#39;...\u0026#39;); -- reorganize data layout of table/partition INSERT OVERWRITE table_identifier [PARTITION (part_spec)] SELECT ... FROM table_identifier [WHERE part_spec]; Please note that\n ALTER TABLE only modifies the table\u0026rsquo;s metadata and will NOT reorganize or reformat existing data. Reorganize existing data must be achieved by INSERT OVERWRITE. Rescale bucket number does not influence the read and running write jobs. Once the bucket number is changed, any newly scheduled INSERT INTO jobs which write to without-reorganized existing table/partition will throw a TableException with message like Try to write table/partition ... with a new bucket num ..., but the previous bucket num is ... Please switch to batch mode, and perform INSERT OVERWRITE to rescale current data layout first.  For partitioned table, it is possible to have different bucket number for different partitions. E.g. ALTER TABLE my_table SET (\u0026#39;bucket\u0026#39; = \u0026#39;4\u0026#39;); INSERT OVERWRITE my_table PARTITION (dt = \u0026#39;2022-01-01\u0026#39;) SELECT * FROM ...; ALTER TABLE my_table SET (\u0026#39;bucket\u0026#39; = \u0026#39;8\u0026#39;); INSERT OVERWRITE my_table PARTITION (dt = \u0026#39;2022-01-02\u0026#39;) SELECT * FROM ...;  During overwrite period, make sure there are no other jobs writing the same table/partition.  Note: For the table which enables log system(e.g. Kafka), please rescale the topic\u0026rsquo;s partition as well to keep consistency.  Use Case #  Rescale bucket helps to handle sudden spikes in throughput. Suppose there is a daily streaming ETL task to sync transaction data. The table\u0026rsquo;s DDL and pipeline are listed as follows.\n-- table DDL CREATE TABLE verified_orders ( trade_order_id BIGINT, item_id BIGINT, item_price DOUBLE, dt STRING, PRIMARY KEY (dt, trade_order_id, item_id) NOT ENFORCED ) PARTITIONED BY (dt) WITH ( \u0026#39;bucket\u0026#39; = \u0026#39;16\u0026#39; ); -- like from a kafka table CREATE temporary TABLE raw_orders( trade_order_id BIGINT, item_id BIGINT, item_price BIGINT, gmt_create STRING, order_status STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ... ); -- streaming insert as bucket num = 16 INSERT INTO verified_orders SELECT trade_order_id, item_id, item_price, DATE_FORMAT(gmt_create, \u0026#39;yyyy-MM-dd\u0026#39;) AS dt FROM raw_orders WHERE order_status = \u0026#39;verified\u0026#39;; The pipeline has been running well for the past few weeks. However, the data volume has grown fast recently, and the job\u0026rsquo;s latency keeps increasing. To improve the data freshness, users can\n Suspend the streaming job with a savepoint ( see Suspended State and Stopping a Job Gracefully Creating a Final Savepoint ) $ ./bin/flink stop \\  --savepointPath /tmp/flink-savepoints \\  $JOB_ID  Increase the bucket number -- scaling out ALTER TABLE verified_orders SET (\u0026#39;bucket\u0026#39; = \u0026#39;32\u0026#39;);  Switch to the batch mode and overwrite the current partition(s) to which the streaming job is writing SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;batch\u0026#39;; -- suppose today is 2022-06-22 -- case 1: there is no late event which updates the historical partitions, thus overwrite today\u0026#39;s partition is enough INSERT OVERWRITE verified_orders PARTITION (dt = \u0026#39;2022-06-22\u0026#39;) SELECT trade_order_id, item_id, item_price FROM verified_orders WHERE dt = \u0026#39;2022-06-22\u0026#39;; -- case 2: there are late events updating the historical partitions, but the range does not exceed 3 days INSERT OVERWRITE verified_orders SELECT trade_order_id, item_id, item_price, dt FROM verified_orders WHERE dt IN (\u0026#39;2022-06-20\u0026#39;, \u0026#39;2022-06-21\u0026#39;, \u0026#39;2022-06-22\u0026#39;);  After overwrite job has finished, switch back to streaming mode. And now, the parallelism can be increased alongside with bucket number to restore the streaming job from the savepoint ( see Start a SQL Job from a savepoint ) SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;streaming\u0026#39;; SET \u0026#39;execution.savepoint.path\u0026#39; = \u0026lt;savepointPath\u0026gt;; INSERT INTO verified_orders SELECT trade_order_id, item_id, item_price, DATE_FORMAT(gmt_create, \u0026#39;yyyy-MM-dd\u0026#39;) AS dt FROM raw_orders WHERE order_status = \u0026#39;verified\u0026#39;;   "});index.add({'id':81,'href':'/docs/1.1/flink/sql-alter/','title':"SQL Alter",'section':"Engine Flink",'content':"Altering Tables #  Changing/Adding Table Properties #  The following SQL sets write-buffer-size table property to 256 MB.\nALTER TABLE my_table SET ( \u0026#39;write-buffer-size\u0026#39; = \u0026#39;256 MB\u0026#39; ); Removing Table Properties #  The following SQL removes write-buffer-size table property.\nALTER TABLE my_table RESET (\u0026#39;write-buffer-size\u0026#39;); Changing/Adding Table Comment #  The following SQL changes comment of table my_table to table comment.\nALTER TABLE my_table SET ( \u0026#39;comment\u0026#39; = \u0026#39;table comment\u0026#39; ); Removing Table Comment #  The following SQL removes table comment.\nALTER TABLE my_table RESET (\u0026#39;comment\u0026#39;); Rename Table Name #  The following SQL rename the table name to new name.\nALTER TABLE my_table RENAME TO my_table_new; If you use object storage, such as S3 or OSS, please use this syntax carefully, because the renaming of object storage is not atomic, and only partial files may be moved in case of failure.  Adding New Columns #  The following SQL adds two columns c1 and c2 to table my_table.\nTo add a column in a row type, see Changing Column Type.  ALTER TABLE my_table ADD (c1 INT, c2 STRING); Renaming Column Name #  The following SQL renames column c0 in table my_table to c1.\nALTER TABLE my_table RENAME c0 TO c1; Dropping Columns #  The following SQL drops two columns c1 and c2 from table my_table.\nALTER TABLE my_table DROP (c1, c2); To drop a column in a row type, see Changing Column Type.  In hive catalog, you need to ensure:\n disable hive.metastore.disallow.incompatible.col.type.changes in your hive server or set hadoop.hive.metastore.disallow.incompatible.col.type.changes=false in your paimon catalog.  Otherwise this operation may fail, throws an exception like The following columns have types incompatible with the existing columns in their respective positions.\nDropping Partitions #  The following SQL drops the partitions of the paimon table.\nFor flink sql, you can specify the partial columns of partition columns, and you can also specify multiple partition values at the same time.\nALTER TABLE my_table DROP PARTITION (`id` = 1); ALTER TABLE my_table DROP PARTITION (`id` = 1, `name` = \u0026#39;paimon\u0026#39;); ALTER TABLE my_table DROP PARTITION (`id` = 1), PARTITION (`id` = 2); Adding Partitions #  The following SQL adds the partitions of the paimon table.\nFor flink sql, you can specify the partial columns of partition columns, and you can also specify multiple partition values at the same time, only with metastore configured metastore.partitioned-table=true.\nALTER TABLE my_table ADD PARTITION (`id` = 1); ALTER TABLE my_table ADD PARTITION (`id` = 1, `name` = \u0026#39;paimon\u0026#39;); ALTER TABLE my_table ADD PARTITION (`id` = 1), PARTITION (`id` = 2); Changing Column Nullability #  The following SQL changes nullability of column coupon_info.\nCREATE TABLE my_table (id INT PRIMARY KEY NOT ENFORCED, coupon_info FLOAT NOT NULL); -- Change column `coupon_info` from NOT NULL to nullable ALTER TABLE my_table MODIFY coupon_info FLOAT; -- Change column `coupon_info` from nullable to NOT NULL -- If there are NULL values already, set table option as below to drop those records silently before altering table. SET \u0026#39;table.exec.sink.not-null-enforcer\u0026#39; = \u0026#39;DROP\u0026#39;; ALTER TABLE my_table MODIFY coupon_info FLOAT NOT NULL; Changing nullable column to NOT NULL is only supported by Flink currently.  Changing Column Comment #  The following SQL changes comment of column buy_count to buy count.\nALTER TABLE my_table MODIFY buy_count BIGINT COMMENT \u0026#39;buy count\u0026#39;; Adding Column Position #  To add a new column with specified position, use FIRST or AFTER col_name.\nALTER TABLE my_table ADD c INT FIRST; ALTER TABLE my_table ADD c INT AFTER b; Changing Column Position #  To modify an existent column to a new position, use FIRST or AFTER col_name.\nALTER TABLE my_table MODIFY col_a DOUBLE FIRST; ALTER TABLE my_table MODIFY col_a DOUBLE AFTER col_b; Changing Column Type #  The following SQL changes type of column col_a to DOUBLE.\nALTER TABLE my_table MODIFY col_a DOUBLE; Paimon also supports changing columns of row type, array type, and map type.\n-- col_a previously has type ARRAY\u0026lt;MAP\u0026lt;INT, ROW(f1 INT, f2 STRING)\u0026gt;\u0026gt; -- the following SQL changes f1 to BIGINT, drops f2, and adds f3 ALTER TABLE my_table MODIFY col_a ARRAY\u0026lt;MAP\u0026lt;INT, ROW(f1 BIGINT, f3 DOUBLE)\u0026gt;\u0026gt;; Adding watermark #  The following SQL adds a computed column ts from existing column log_ts, and a watermark with strategy ts - INTERVAL '1' HOUR on column ts which is marked as event time attribute of table my_table.\nALTER TABLE my_table ADD ( ts AS TO_TIMESTAMP(log_ts) AFTER log_ts, WATERMARK FOR ts AS ts - INTERVAL \u0026#39;1\u0026#39; HOUR ); Dropping watermark #  The following SQL drops the watermark of table my_table.\nALTER TABLE my_table DROP WATERMARK; Changing watermark #  The following SQL modifies the watermark strategy to ts - INTERVAL '2' HOUR.\nALTER TABLE my_table MODIFY WATERMARK FOR ts AS ts - INTERVAL \u0026#39;2\u0026#39; HOUR; ALTER DATABASE #  The following SQL sets one or more properties in the specified database. If a particular property is already set in the database, override the old value with the new one.\nALTER DATABASE [catalog_name.]db_name SET (key1=val1, key2=val2, ...); Altering Database Location #  The following SQL changes location of database my_database to file:/temp/my_database.\nALTER DATABASE my_database SET (\u0026#39;location\u0026#39; = \u0026#39;file:/temp/my_database\u0026#39;); "});index.add({'id':82,'href':'/docs/1.1/concepts/system-tables/','title':"System Tables",'section':"Concepts",'content':"System Tables #  Paimon provides a very rich set of system tables to help users better analyze and query the status of Paimon tables:\n Query the status of the data table: Data System Table. Query the global status of the entire Catalog: Global System Table.  Data System Table #  Data System tables contain metadata and information about each Paimon data table, such as the snapshots created and the options in use. Users can access system tables with batch queries.\nCurrently, Flink, Spark, Trino and StarRocks support querying system tables.\nIn some cases, the table name needs to be enclosed with back quotes to avoid syntax parsing conflicts, for example triple access mode:\nSELECT * FROM my_catalog.my_db.`my_table$snapshots`; Snapshots Table #  You can query the snapshot history information of the table through snapshots table, including the record count occurred in the snapshot.\nSELECT * FROM my_table$snapshots; /* +--------------+------------+-----------------+-------------------+--------------+-------------------------+--------------------------------+------------------------------- +--------------------------------+---------------------+---------------------+-------------------------+----------------+ | snapshot_id | schema_id | commit_user | commit_identifier | commit_kind | commit_time | base_manifest_list | delta_manifest_list | changelog_manifest_list | total_record_count | delta_record_count | changelog_record_count | watermark | +--------------+------------+-----------------+-------------------+--------------+-------------------------+--------------------------------+------------------------------- +--------------------------------+---------------------+---------------------+-------------------------+----------------+ | 2 | 0 | 7ca4cd28-98e... | 2 | APPEND | 2022-10-26 11:44:15.600 | manifest-list-31323d5f-76e6... | manifest-list-31323d5f-76e6... | manifest-list-31323d5f-76e6... | 2 | 2 | 0 | 1666755855600 | | 1 | 0 | 870062aa-3e9... | 1 | APPEND | 2022-10-26 11:44:15.148 | manifest-list-31593d5f-76e6... | manifest-list-31593d5f-76e6... | manifest-list-31593d5f-76e6... | 1 | 1 | 0 | 1666755855148 | +--------------+------------+-----------------+-------------------+--------------+-------------------------+--------------------------------+------------------------------- +--------------------------------+---------------------+---------------------+-------------------------+----------------+ 2 rows in set */ By querying the snapshots table, you can know the commit and expiration information about that table and time travel through the data.\nSchemas Table #  You can query the historical schemas of the table through schemas table.\nSELECT * FROM my_table$schemas; /* +-----------+--------------------------------+----------------+--------------+---------+---------+-------------------------+ | schema_id | fields | partition_keys | primary_keys | options | comment | update_time | +-----------+--------------------------------+----------------+--------------+---------+---------+-------------------------+ | 0 | [{\u0026#34;id\u0026#34;:0,\u0026#34;name\u0026#34;:\u0026#34;word\u0026#34;,\u0026#34;typ... | [] | [\u0026#34;word\u0026#34;] | {} | | 2022-10-28 11:44:20.600 | | 1 | [{\u0026#34;id\u0026#34;:0,\u0026#34;name\u0026#34;:\u0026#34;word\u0026#34;,\u0026#34;typ... | [] | [\u0026#34;word\u0026#34;] | {} | | 2022-10-27 11:44:15.600 | | 2 | [{\u0026#34;id\u0026#34;:0,\u0026#34;name\u0026#34;:\u0026#34;word\u0026#34;,\u0026#34;typ... | [] | [\u0026#34;word\u0026#34;] | {} | | 2022-10-26 11:44:10.600 | +-----------+--------------------------------+----------------+--------------+---------+---------+-------------------------+ 3 rows in set */ You can join the snapshots table and schemas table to get the fields of given snapshots.\nSELECT s.snapshot_id, t.schema_id, t.fields FROM my_table$snapshots s JOIN my_table$schemas t ON s.schema_id=t.schema_id where s.snapshot_id=100; Options Table #  You can query the table\u0026rsquo;s option information which is specified from the DDL through options table. The options not shown will be the default value. You can take reference to Configuration.\nSELECT * FROM my_table$options; /* +------------------------+--------------------+ | key | value | +------------------------+--------------------+ | snapshot.time-retained | 5 h | +------------------------+--------------------+ 1 rows in set */ Audit log Table #  If you need to audit the changelog of the table, you can use the audit_log system table. Through audit_log table, you can get the rowkind column when you get the incremental data of the table. You can use this column for filtering and other operations to complete the audit.\nThere are four values for rowkind:\n +I: Insertion operation. -U: Update operation with the previous content of the updated row. +U: Update operation with new content of the updated row. -D: Deletion operation.  SELECT * FROM my_table$audit_log; /* +------------------+-----------------+-----------------+ | rowkind | column_0 | column_1 | +------------------+-----------------+-----------------+ | +I | ... | ... | +------------------+-----------------+-----------------+ | -U | ... | ... | +------------------+-----------------+-----------------+ | +U | ... | ... | +------------------+-----------------+-----------------+ 3 rows in set */ Binlog Table #  You can query the binlog through binlog table. In the binlog system table, the update before and update after will be packed in one row.\nCurrently, the binlog table is unable to display Flink\u0026rsquo;s computed columns.\nSELECT * FROM T$binlog; /* +------------------+----------------------+-----------------------+ | rowkind | column_0 | column_1 | +------------------+----------------------+-----------------------+ | +I | [col_0] | [col_1] | +------------------+----------------------+-----------------------+ | +U | [col_0_ub, col_0_ua] | [col_1_ub, col_1_ua] | +------------------+----------------------+-----------------------+ | -D | [col_0] | [col_1] | +------------------+----------------------+-----------------------+ */ Read-optimized Table #  If you require extreme reading performance and can accept reading slightly old data, you can use the ro (read-optimized) system table. Read-optimized system table improves reading performance by only scanning files which does not need merging.\nFor primary-key tables, ro system table only scans files on the topmost level. That is to say, ro system table only produces the result of the latest full compaction.\nIt is possible that different buckets carry out full compaction at difference times, so it is possible that the values of different keys come from different snapshots.  For append tables, as all files can be read without merging, ro system table acts like the normal append table.\nSELECT * FROM my_table$ro; Files Table #  You can query the files of the table with specific snapshot.\n-- Query the files of latest snapshot SELECT * FROM my_table$files; /* +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+---------------------+---------------------+-----------------------+ | partition | bucket | file_path | file_format | schema_id | level | record_count | file_size_in_bytes | min_key | max_key | null_value_counts | min_value_stats | max_value_stats | min_sequence_number | max_sequence_number | creation_time | +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+---------------------+---------------------+-----------------------+ | {3} | 0 | data-8f64af95-29cc-4342-adc... | orc | 0 | 0 | 1 | 593 | [c] | [c] | {cnt=0, val=0, word=0} | {cnt=3, val=33, word=c} | {cnt=3, val=33, word=c} | 1691551246234 | 1691551246637 |2023-02-24T16:06:21.166| | {2} | 0 | data-8b369068-0d37-4011-aa5... | orc | 0 | 0 | 1 | 593 | [b] | [b] | {cnt=0, val=0, word=0} | {cnt=2, val=22, word=b} | {cnt=2, val=22, word=b} | 1691551246233 | 1691551246732 |2023-02-24T16:06:21.166| | {2} | 0 | data-83aa7973-060b-40b6-8c8... | orc | 0 | 0 | 1 | 605 | [d] | [d] | {cnt=0, val=0, word=0} | {cnt=2, val=32, word=d} | {cnt=2, val=32, word=d} | 1691551246267 | 1691551246798 |2023-02-24T16:06:21.166| | {5} | 0 | data-3d304f4a-bcea-44dc-a13... | orc | 0 | 0 | 1 | 593 | [c] | [c] | {cnt=0, val=0, word=0} | {cnt=5, val=51, word=c} | {cnt=5, val=51, word=c} | 1691551246788 | 1691551246152 |2023-02-24T16:06:21.166| | {1} | 0 | data-10abb5bc-0170-43ae-b6a... | orc | 0 | 0 | 1 | 595 | [a] | [a] | {cnt=0, val=0, word=0} | {cnt=1, val=11, word=a} | {cnt=1, val=11, word=a} | 1691551246722 | 1691551246273 |2023-02-24T16:06:21.166| | {4} | 0 | data-2c9b7095-65b7-4013-a7a... | orc | 0 | 0 | 1 | 593 | [a] | [a] | {cnt=0, val=0, word=0} | {cnt=4, val=12, word=a} | {cnt=4, val=12, word=a} | 1691551246321 | 1691551246109 |2023-02-24T16:06:21.166| +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+---------------------+---------------------+-----------------------+ 6 rows in set */ -- You can also query the files with specific snapshot SELECT * FROM my_table$files /*+ OPTIONS(\u0026#39;scan.snapshot-id\u0026#39;=\u0026#39;1\u0026#39;) */; /* +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+---------------------+---------------------+-----------------------+ | partition | bucket | file_path | file_format | schema_id | level | record_count | file_size_in_bytes | min_key | max_key | null_value_counts | min_value_stats | max_value_stats | min_sequence_number | max_sequence_number | creation_time | +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+---------------------+---------------------+-----------------------+ | {3} | 0 | data-8f64af95-29cc-4342-adc... | orc | 0 | 0 | 1 | 593 | [c] | [c] | {cnt=0, val=0, word=0} | {cnt=3, val=33, word=c} | {cnt=3, val=33, word=c} | 1691551246234 | 1691551246637 |2023-02-24T16:06:21.166| | {2} | 0 | data-8b369068-0d37-4011-aa5... | orc | 0 | 0 | 1 | 593 | [b] | [b] | {cnt=0, val=0, word=0} | {cnt=2, val=22, word=b} | {cnt=2, val=22, word=b} | 1691551246233 | 1691551246732 |2023-02-24T16:06:21.166| | {1} | 0 | data-10abb5bc-0170-43ae-b6a... | orc | 0 | 0 | 1 | 595 | [a] | [a] | {cnt=0, val=0, word=0} | {cnt=1, val=11, word=a} | {cnt=1, val=11, word=a} | 1691551246267 | 1691551246798 |2023-02-24T16:06:21.166| +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+---------------------+---------------------+-----------------------+ 3 rows in set */ Tags Table #  You can query the tag history information of the table through tags table, including which snapshots are the tags based on and some historical information of the snapshots. You can also get all tag names and time travel to a specific tag data by name.\nSELECT * FROM my_table$tags; /* +----------+-------------+-----------+-------------------------+--------------+--------------+ | tag_name | snapshot_id | schema_id | commit_time | record_count | branches | +----------+-------------+-----------+-------------------------+--------------+--------------+ | tag1 | 1 | 0 | 2023-06-28 14:55:29.344 | 3 | [] | | tag3 | 3 | 0 | 2023-06-28 14:58:24.691 | 7 | [branch-1] | +----------+-------------+-----------+-------------------------+--------------+--------------+ 2 rows in set */ Branches Table #  You can query the branches of the table.\nSELECT * FROM my_table$branches; /* +----------------------+-------------------------+ | branch_name | create_time | +----------------------+-------------------------+ | branch1 | 2024-07-18 20:31:39.084 | | branch2 | 2024-07-18 21:11:14.373 | +----------------------+-------------------------+ 2 rows in set */ Consumers Table #  You can query all consumers which contains next snapshot.\nSELECT * FROM my_table$consumers; /* +-------------+------------------+ | consumer_id | next_snapshot_id | +-------------+------------------+ | id1 | 1 | | id2 | 3 | +-------------+------------------+ 2 rows in set */ Manifests Table #  You can query all manifest files contained in the latest snapshot or the specified snapshot of the current table.\n-- Query the manifest of latest snapshot SELECT * FROM my_table$manifests; /* +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ | file_name | file_size | num_added_files | num_deleted_files | schema_id | min_partition_stats | max_partition_stats | +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ | manifest-f4dcab43-ef6b-4713... | 12365| 40 | 0 | 0 | {20230315, 00} | {20230315, 20} | | manifest-f4dcab43-ef6b-4713... | 1648 | 1 | 0 | 0 | {20230115, 00} | {20230316, 23} | +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ 2 rows in set */ -- You can also query the manifest with specified snapshot SELECT * FROM my_table$manifests /*+ OPTIONS(\u0026#39;scan.snapshot-id\u0026#39;=\u0026#39;1\u0026#39;) */; /* +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ | file_name | file_size | num_added_files | num_deleted_files | schema_id | min_partition_stats | max_partition_stats | +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ | manifest-f4dcab43-ef6b-4713... | 12365| 40 | 0 | 0 | {20230315, 00} | {20230315, 20} | +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ 1 rows in set */ - You can also query the manifest with specified tagName SELECT * FROM my_table$manifests /*+ OPTIONS(\u0026#39;scan.tag-name\u0026#39;=\u0026#39;tag1\u0026#39;) */; /* +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ | file_name | file_size | num_added_files | num_deleted_files | schema_id | min_partition_stats | max_partition_stats | +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ | manifest-f4dcab43-ef6b-4713... | 12365| 40 | 0 | 0 | {20230315, 00} | {20230315, 20} | +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ 1 rows in set */ - You can also query the manifest with specified timestamp in unix milliseconds SELECT * FROM my_table$manifests /*+ OPTIONS(\u0026#39;scan.timestamp-millis\u0026#39;=\u0026#39;1678883047356\u0026#39;) */; /* +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ | file_name | file_size | num_added_files | num_deleted_files | schema_id | min_partition_stats | max_partition_stats | +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ | manifest-f4dcab43-ef6b-4713... | 12365| 40 | 0 | 0 | {20230315, 00} | {20230315, 20} | +--------------------------------+-------------+------------------+-------------------+---------------+---------------------+---------------------+ 1 rows in set */ Aggregation fields Table #  You can query the historical aggregation of the table through aggregation fields table.\nSELECT * FROM my_table$aggregation_fields; /* +------------+-----------------+--------------+--------------------------------+---------+ | field_name | field_type | function | function_options | comment | +------------+-----------------+--------------+--------------------------------+---------+ | product_id | BIGINT NOT NULL | [] | [] | \u0026lt;NULL\u0026gt; | | price | INT | [true,count] | [fields.price.ignore-retrac... | \u0026lt;NULL\u0026gt; | | sales | BIGINT | [sum] | [fields.sales.aggregate-fun... | \u0026lt;NULL\u0026gt; | +------------+-----------------+--------------+--------------------------------+---------+ 3 rows in set */ Partitions Table #  You can query the partition files of the table.\nSELECT * FROM my_table$partitions; /* +---------------+----------------+--------------------+--------------------+------------------------+ | partition | record_count | file_size_in_bytes| file_count| last_update_time| +---------------+----------------+--------------------+--------------------+------------------------+ | {1} | 1 | 645 | 1 | 2024-06-24 10:25:57.400| +---------------+----------------+--------------------+--------------------+------------------------+ */ Buckets Table #  You can query the bucket files of the table.\nSELECT * FROM my_table$buckets; /* +---------------+--------+----------------+--------------------+--------------------+------------------------+ | partition | bucket | record_count | file_size_in_bytes| file_count| last_update_time| +---------------+--------+----------------+--------------------+--------------------+------------------------+ | [1] | 0 | 1 | 645 | 1 | 2024-06-24 10:25:57.400| +---------------+--------+----------------+--------------------+--------------------+------------------------+ */ Statistic Table #  You can query the statistic information through statistic table.\nSELECT * FROM T$statistics; /* +--------------+------------+-----------------------+------------------+----------+ | snapshot_id | schema_id | mergedRecordCount | mergedRecordSize | colstat | +--------------+------------+-----------------------+------------------+----------+ | 2 | 0 | 2 | 2 | {} | +--------------+------------+-----------------------+------------------+----------+ 1 rows in set */ Table Indexes Table #  You can query the table\u0026rsquo;s index files generated for dynamic bucket table (index_type = HASH) and deletion vectors (index_type = DELETION_VECTORS) through indexes table.\nSELECT * FROM my_table$table_indexes; /* +--------------------------------+-------------+--------------------------------+--------------------------------+----------------------+----------------------+--------------------------------+ | partition | bucket | index_type | file_name | file_size | row_count | dv_ranges | +--------------------------------+-------------+--------------------------------+--------------------------------+----------------------+----------------------+--------------------------------+ | {2024-10-01} | 0 | HASH | index-70abfebf-149e-4796-9f... | 12 | 3 | \u0026lt;NULL\u0026gt; | | {2024-10-01} | 0 | DELETION_VECTORS | index-633857e7-cdce-47d2-87... | 33 | 1 | [(data-346cb9c8-4032-4d66-a... | +--------------------------------+-------------+--------------------------------+--------------------------------+----------------------+----------------------+--------------------------------+ 2 rows in set */ Global System Table #  Global system tables contain the statistical information of all the tables exists in paimon. For convenient of searching, we create a reference system database called sys. We can display all the global system tables by sql in flink:\nUSE sys; SHOW TABLES; ALL Options Table #  This table is similar to Options Table, but it shows all the table options is all database.\nSELECT * FROM sys.all_table_options; /* +---------------+--------------------------------+--------------------------------+------------------+ | database_name | table_name | key | value | +---------------+--------------------------------+--------------------------------+------------------+ | my_db | Orders_orc | bucket | -1 | | my_db | Orders2 | bucket | -1 | | my_db | Orders2 | sink.parallelism | 7 | | my_db2| OrdersSum | bucket | 1 | +---------------+--------------------------------+--------------------------------+------------------+ 7 rows in set */ Catalog Options Table #  You can query the catalog\u0026rsquo;s option information through catalog options table. The options not shown will be the default value. You can take reference to Configuration.\nSELECT * FROM sys.catalog_options; /* +-----------+---------------------------+ | key | value | +-----------+---------------------------+ | warehouse | hdfs:///path/to/warehouse | +-----------+---------------------------+ 1 rows in set */ "});index.add({'id':83,'href':'/docs/1.1/concepts/data-types/','title':"Data Types",'section':"Concepts",'content':"Data Types #  A data type describes the logical type of a value in the table ecosystem. It can be used to declare input and/or output types of operations.\nAll data types supported by Paimon are as follows:\n  DataType Description     BOOLEAN Data type of a boolean with a (possibly) three-valued logic of TRUE, FALSE, and UNKNOWN.   CHAR\nCHAR(n)  Data type of a fixed-length character string.\nThe type can be declared using CHAR(n) where n is the number of code points. n must have a value between 1 and 2,147,483,647 (both inclusive). If no length is specified, n is equal to 1.     VARCHAR\nVARCHAR(n)\nSTRING  Data type of a variable-length character string.\nThe type can be declared using VARCHAR(n) where n is the maximum number of code points. n must have a value between 1 and 2,147,483,647 (both inclusive). If no length is specified, n is equal to 1. \nSTRING is a synonym for VARCHAR(2147483647).    BINARY\nBINARY(n)\n Data type of a fixed-length binary string (=a sequence of bytes).\nThe type can be declared using BINARY(n) where n is the number of bytes. n must have a value between 1 and 2,147,483,647 (both inclusive). If no length is specified, n is equal to 1.    VARBINARY\nVARBINARY(n)\nBYTES  Data type of a variable-length binary string (=a sequence of bytes).\nThe type can be declared using VARBINARY(n) where n is the maximum number of bytes. n must have a value between 1 and 2,147,483,647 (both inclusive). If no length is specified, n is equal to 1.\nBYTES is a synonym for VARBINARY(2147483647).    DECIMAL\nDECIMAL(p)\nDECIMAL(p, s)  Data type of a decimal number with fixed precision and scale.\nThe type can be declared using DECIMAL(p, s) where p is the number of digits in a number (precision) and s is the number of digits to the right of the decimal point in a number (scale). p must have a value between 1 and 38 (both inclusive). s must have a value between 0 and p (both inclusive). The default value for p is 10. The default value for s is 0.    TINYINT Data type of a 1-byte signed integer with values from -128 to 127.   SMALLINT Data type of a 2-byte signed integer with values from -32,768 to 32,767.   INT Data type of a 4-byte signed integer with values from -2,147,483,648 to 2,147,483,647.   BIGINT Data type of an 8-byte signed integer with values from -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807.   FLOAT Data type of a 4-byte single precision floating point number.\nCompared to the SQL standard, the type does not take parameters.    DOUBLE Data type of an 8-byte double precision floating point number.   DATE Data type of a date consisting of year-month-day with values ranging from 0000-01-01 to 9999-12-31.\nCompared to the SQL standard, the range starts at year 0000.    TIME\nTIME(p)  Data type of a time without time zone consisting of hour:minute:second[.fractional] with up to nanosecond precision and values ranging from 00:00:00.000000000 to 23:59:59.999999999.\nThe type can be declared using TIME(p) where p is the number of digits of fractional seconds (precision). p must have a value between 0 and 9 (both inclusive). If no precision is specified, p is equal to 0.    TIMESTAMP\nTIMESTAMP(p)  Data type of a timestamp without time zone consisting of year-month-day hour:minute:second[.fractional] with up to nanosecond precision and values ranging from 0000-01-01 00:00:00.000000000 to 9999-12-31 23:59:59.999999999.\nThe type can be declared using TIMESTAMP(p) where p is the number of digits of fractional seconds (precision). p must have a value between 0 and 9 (both inclusive). If no precision is specified, p is equal to 6.    TIMESTAMP WITH LOCAL TIME ZONE\nTIMESTAMP(p) WITH LOCAL TIME ZONE  Data type of a timestamp with local time zone consisting of year-month-day hour:minute:second[.fractional] zone with up to nanosecond precision and values ranging from 0000-01-01 00:00:00.000000000 +14:59 to 9999-12-31 23:59:59.999999999 -14:59.\nThis type fills the gap between time zone free and time zone mandatory timestamp types by allowing the interpretation of UTC timestamps according to the configured session time zone. A conversion from and to int describes the number of seconds since epoch. A conversion from and to long describes the number of milliseconds since epoch.    ARRAY\u0026lt;t\u0026gt; Data type of an array of elements with same subtype.\nCompared to the SQL standard, the maximum cardinality of an array cannot be specified but is fixed at 2,147,483,647. Also, any valid type is supported as a subtype.\nThe type can be declared using ARRAY\u0026lt;t\u0026gt; where t is the data type of the contained elements.    MAP\u0026lt;kt, vt\u0026gt; Data type of an associative array that maps keys (including NULL) to values (including NULL). A map cannot contain duplicate keys; each key can map to at most one value.\nThere is no restriction of element types; it is the responsibility of the user to ensure uniqueness.\nThe type can be declared using MAP\u0026lt;kt, vt\u0026gt; where kt is the data type of the key elements and vt is the data type of the value elements.    MULTISET\u0026lt;t\u0026gt; Data type of a multiset (=bag). Unlike a set, it allows for multiple instances for each of its elements with a common subtype. Each unique value (including NULL) is mapped to some multiplicity.\nThere is no restriction of element types; it is the responsibility of the user to ensure uniqueness.\nThe type can be declared using MULTISET\u0026lt;t\u0026gt; where t is the data type of the contained elements.    ROW\u0026lt;n0 t0, n1 t1, ...\u0026gt;\nROW\u0026lt;n0 t0 'd0', n1 t1 'd1', ...\u0026gt;  Data type of a sequence of fields.\nA field consists of a field name, field type, and an optional description. The most specific type of a row of a table is a row type. In this case, each column of the row corresponds to the field of the row type that has the same ordinal position as the column.\nCompared to the SQL standard, an optional field description simplifies the handling with complex structures.\nA row type is similar to the STRUCT type known from other non-standard-compliant frameworks.\nThe type can be declared using ROW\u0026lt;n0 t0 'd0', n1 t1 'd1', ...\u0026gt; where n is the unique name of a field, t is the logical type of a field, d is the description of a field.     "});index.add({'id':84,'href':'/docs/1.1/maintenance/manage-tags/','title':"Manage Tags",'section':"Maintenance",'content':"Manage Tags #  Paimon\u0026rsquo;s snapshots can provide an easy way to query historical data. But in most scenarios, a job will generate too many snapshots and table will expire old snapshots according to table configuration. Snapshot expiration will also delete old data files, and the historical data of expired snapshots cannot be queried anymore.\nTo solve this problem, you can create a tag based on a snapshot. The tag will maintain the manifests and data files of the snapshot. A typical usage is creating tags daily, then you can maintain the historical data of each day for batch reading.\nAutomatic Creation #  Paimon supports automatic creation of tags in writing job.\nStep 1: Choose Creation Mode\nYou can set creation mode by table option 'tag.automatic-creation'. Supported values are:\n process-time: Create TAG based on the time of the machine. watermark: Create TAG based on the watermark of the Sink input. batch: In a batch processing scenario, a tag is generated after the current task is completed.  If you choose Watermark, you may need to specify the time zone of watermark, if watermark is not in the UTC time zone, please configure 'sink.watermark-time-zone'.  Step 2: Choose Creation Period\nWhat frequency is used to generate tags. You can choose 'daily', 'hourly' and 'two-hours' for 'tag.creation-period'.\nIf you need to wait for late data, you can configure a delay time: 'tag.creation-delay'.\nStep 3: Automatic deletion of tags\nYou can configure 'tag.num-retained-max' or tag.default-time-retained to delete tags automatically.\nExample, configure table to create a tag at 0:10 every day, with a maximum retention time of 3 months:\n-- Flink SQL CREATE TABLE t ( k INT PRIMARY KEY NOT ENFORCED, f0 INT, ... ) WITH ( \u0026#39;tag.automatic-creation\u0026#39; = \u0026#39;process-time\u0026#39;, \u0026#39;tag.creation-period\u0026#39; = \u0026#39;daily\u0026#39;, \u0026#39;tag.creation-delay\u0026#39; = \u0026#39;10 m\u0026#39;, \u0026#39;tag.num-retained-max\u0026#39; = \u0026#39;90\u0026#39; ); INSERT INTO t SELECT ...; -- Spark SQL  -- Read latest snapshot SELECT * FROM t; -- Read Tag snapshot SELECT * FROM t VERSION AS OF \u0026#39;2023-07-26\u0026#39;; -- Read Incremental between Tags SELECT * FROM paimon_incremental_query(\u0026#39;t\u0026#39;, \u0026#39;2023-07-25\u0026#39;, \u0026#39;2023-07-26\u0026#39;); See Query Tables to see more query for Spark.\nCreate Tags #  You can create a tag with given name and snapshot ID.\nFlink SQL Run the following command:\nCALL sys.create_tag(`table` =\u0026gt; \u0026#39;database_name.table_name\u0026#39;, tag =\u0026gt; \u0026#39;tag_name\u0026#39;, [snapshot_id =\u0026gt; \u0026lt;snapshot-id\u0026gt;]); If snapshot_id unset, snapshot_id defaults to the latest.\nFlink Action Run the following command:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  create_tag \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  --tag_name \u0026lt;tag-name\u0026gt; \\  [--snapshot \u0026lt;snapshot_id\u0026gt;] \\  [--time_retained \u0026lt;time-retained\u0026gt;] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] If snapshot unset, snapshot_id defaults to the latest.\nJava API import org.apache.paimon.table.Table; public class CreateTag { public static void main(String[] args) { Table table = ...; table.createTag(\u0026#34;my-tag\u0026#34;, 1); table.createTag(\u0026#34;my-tag-retained-12-hours\u0026#34;, 1, Duration.ofHours(12)); } } Spark Run the following sql:\nCALL sys.create_tag(table =\u0026gt; \u0026#39;test.t\u0026#39;, tag =\u0026gt; \u0026#39;test_tag\u0026#39;, snapshot =\u0026gt; 2); To create a tag with retained 1 day, run the following sql:\nCALL sys.create_tag(table =\u0026gt; \u0026#39;test.t\u0026#39;, tag =\u0026gt; \u0026#39;test_tag\u0026#39;, snapshot =\u0026gt; 2, time_retained =\u0026gt; \u0026#39;1 d\u0026#39;); To create a tag based on the latest snapshot id, run the following sql:\nCALL sys.create_tag(table =\u0026gt; \u0026#39;test.t\u0026#39;, tag =\u0026gt; \u0026#39;test_tag\u0026#39;);  Delete Tags #  You can delete a tag by its name.\nFlink SQL Run the following command:\nCALL sys.delete_tag(`table` =\u0026gt; \u0026#39;database_name.table_name\u0026#39;, tag =\u0026gt; \u0026#39;tag_name\u0026#39;); Flink Action Run the following command:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  delete_tag \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  --tag_name \u0026lt;tag-name\u0026gt; \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] Java API import org.apache.paimon.table.Table; public class DeleteTag { public static void main(String[] args) { Table table = ...; table.deleteTag(\u0026#34;my-tag\u0026#34;); } } Spark Run the following sql:\nCALL sys.delete_tag(table =\u0026gt; \u0026#39;test.t\u0026#39;, tag =\u0026gt; \u0026#39;test_tag\u0026#39;);  Rollback to Tag #  Rollback table to a specific tag. All snapshots and tags whose snapshot id is larger than the tag will be deleted (and the data will be deleted too).\nFlink SQL Run the following command:\nCALL sys.rollback_to(`table` =\u0026gt; \u0026#39;database_name.table_name\u0026#39;, tag =\u0026gt; \u0026#39;tag_name\u0026#39;); Flink Action Run the following command:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  rollback_to \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  --version \u0026lt;tag-name\u0026gt; \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] Java API import org.apache.paimon.table.Table; public class RollbackTo { public static void main(String[] args) { // before rollback:  // snapshot-3 [expired] -\u0026gt; tag3  // snapshot-4 [expired]  // snapshot-5 -\u0026gt; tag5  // snapshot-6  // snapshot-7  table.rollbackTo(\u0026#34;tag3\u0026#34;); // after rollback:  // snapshot-3 -\u0026gt; tag3  } } Spark Run the following sql:\nCALL sys.rollback(table =\u0026gt; \u0026#39;test.t\u0026#39;, version =\u0026gt; \u0026#39;2\u0026#39;);  "});index.add({'id':85,'href':'/docs/1.1/primary-key-table/query-performance/','title':"Query Performance",'section':"Table with PK",'content':"Query Performance #  Table Mode #  The table schema has the greatest impact on query performance. See Table Mode.\nFor Merge On Read table, the most important thing you should pay attention to is the number of buckets, which will limit the concurrency of reading data.\nFor MOW (Deletion Vectors) or COW table or Read Optimized table, there is no limit to the concurrency of reading data, and they can also utilize some filtering conditions for non-primary-key columns.\nData Skipping By Primary Key Filter #  For a regular bucketed table (For example, bucket = 5), the filtering conditions of the primary key will greatly accelerate queries and reduce the reading of a large number of files.\nData Skipping By File Index #  You can use file index to table with Deletion Vectors enabled, it filters files by index on the read side.\nCREATE TABLE \u0026lt;PAIMON_TABLE\u0026gt; WITH ( \u0026#39;deletion-vectors.enabled\u0026#39; = \u0026#39;true\u0026#39;, \u0026#39;file-index.bloom-filter.columns\u0026#39; = \u0026#39;c1,c2\u0026#39;, \u0026#39;file-index.bloom-filter.c1.items\u0026#39; = \u0026#39;200\u0026#39; ); Supported filter types:\nBloom Filter:\n file-index.bloom-filter.columns: specify the columns that need bloom filter index. file-index.bloom-filter.\u0026lt;column_name\u0026gt;.fpp to config false positive probability. file-index.bloom-filter.\u0026lt;column_name\u0026gt;.items to config the expected distinct items in one data file.  Bitmap:\n file-index.bitmap.columns: specify the columns that need bitmap index. See Index Bitmap.  Bit-Slice Index Bitmap\n file-index.bsi.columns: specify the columns that need bsi index.  More filter types will be supported\u0026hellip;\nIf you want to add file index to existing table, without any rewrite, you can use rewrite_file_index procedure. Before we use the procedure, you should config appropriate configurations in target table. You can use ALTER clause to config file-index.\u0026lt;filter-type\u0026gt;.columns to the table.\nHow to invoke: see flink procedures\n"});index.add({'id':86,'href':'/docs/1.1/maintenance/metrics/','title':"Metrics",'section':"Maintenance",'content':"Paimon Metrics #  Paimon has built a metrics system to measure the behaviours of reading and writing, like how many manifest files it scanned in the last planning, how long it took in the last commit operation, how many files it deleted in the last compact operation.\nIn Paimon\u0026rsquo;s metrics system, metrics are updated and reported at table granularity.\nThere are three types of metrics provided in the Paimon metric system, Gauge, Counter, Histogram.\n Gauge: Provides a value of any type at a point in time. Counter: Used to count values by incrementing and decrementing. Histogram: Measure the statistical distribution of a set of values including the min, max, mean, standard deviation and percentile.  Paimon has supported built-in metrics to measure operations of commits, scans, writes and compactions, which can be bridged to any computing engine that supports, like Flink, Spark etc.\nMetrics List #  Below is lists of Paimon built-in metrics. They are summarized into types of scan metrics, commit metrics, write metrics, write buffer metrics and compaction metrics.\nScan Metrics #    Metrics Name Type Description     lastScanDuration Gauge The time it took to complete the last scan.   scanDuration Histogram Distributions of the time taken by the last few scans.   lastScannedManifests Gauge Number of scanned manifest files in the last scan.   lastScanSkippedTableFiles Gauge Total skipped table files in the last scan.   lastScanResultedTableFiles Gauge Resulted table files in the last scan.    Commit Metrics #    Metrics Name Type Description     lastCommitDuration Gauge The time it took to complete the last commit.   commitDuration Histogram Distributions of the time taken by the last few commits.   lastCommitAttempts Gauge The number of attempts the last commit made.   lastTableFilesAdded Gauge Number of added table files in the last commit, including newly created data files and compacted after.   lastTableFilesDeleted Gauge Number of deleted table files in the last commit, which comes from compacted before.   lastTableFilesAppended Gauge Number of appended table files in the last commit, which means the newly created data files.   lastTableFilesCommitCompacted Gauge Number of compacted table files in the last commit, including compacted before and after.   lastChangelogFilesAppended Gauge Number of appended changelog files in last commit.   lastChangelogFileCommitCompacted Gauge Number of compacted changelog files in last commit.   lastGeneratedSnapshots Gauge Number of snapshot files generated in the last commit, maybe 1 snapshot or 2 snapshots.   lastDeltaRecordsAppended Gauge Delta records count in last commit with APPEND commit kind.   lastChangelogRecordsAppended Gauge Changelog records count in last commit with APPEND commit kind.   lastDeltaRecordsCommitCompacted Gauge Delta records count in last commit with COMPACT commit kind.   lastChangelogRecordsCommitCompacted Gauge Changelog records count in last commit with COMPACT commit kind.   lastPartitionsWritten Gauge Number of partitions written in the last commit.   lastBucketsWritten Gauge Number of buckets written in the last commit.   lastCompactionInputFileSize Gauge Total size of the input files for the last compaction.   lastCompactionOutputFileSize Gauge Total size of the output files for the last compaction.    Write Buffer Metrics #    Metrics Name Type Description     numWriters Gauge Number of writers in this parallelism.   bufferPreemptCount Gauge The total number of memory preempted.   usedWriteBufferSizeByte Gauge Current used write buffer size in byte.   totalWriteBufferSizeByte Gauge The total write buffer size configured in byte.    Compaction Metrics #    Metrics Name Type Description     maxLevel0FileCount Gauge The maximum number of level 0 files currently handled by this task. This value will become larger if asynchronous compaction cannot be done in time.   avgLevel0FileCount Gauge The average number of level 0 files currently handled by this task. This value will become larger if asynchronous compaction cannot be done in time.   compactionThreadBusy Gauge The maximum business of compaction threads in this task. Currently, there is only one compaction thread in each parallelism, so value of business ranges from 0 (idle) to 100 (compaction running all the time).   avgCompactionTime Gauge The average runtime of compaction threads, calculated based on recorded compaction time data in milliseconds. The value represents the average duration of compaction operations. Higher values indicate longer average compaction times, which may suggest the need for performance optimization.   compactionCompletedCount Counter The total number of compactions that have completed.   compactionQueuedCount Counter The total number of compactions that are queued/running.   maxCompactionInputSize Gauge The maximum input file size for this task's compaction.   avgCompactionInputSize/td Gauge The average input file size for this task's compaction.   maxCompactionOutputSize Gauge The maximum output file size for this task's compaction.   avgCompactionOutputSize Gauge The average output file size for this task's compaction.   maxTotalFileSize Gauge The maximum total file size of an active (currently being written) bucket.   avgTotalFileSize Gauge The average total file size of all active (currently being written) buckets.    Bridging To Flink #  Paimon has implemented bridging metrics to Flink\u0026rsquo;s metrics system, which can be reported by Flink, and the lifecycle of metric groups are managed by Flink.\nPlease join the \u0026lt;scope\u0026gt;.\u0026lt;infix\u0026gt;.\u0026lt;metric_name\u0026gt; to get the complete metric identifier when using Flink to access Paimon, metric_name can be got from Metric List.\nFor example, the identifier of metric lastPartitionsWritten for table word_count in Flink job named insert_word_count is:\nlocalhost.taskmanager.localhost:60340-775a20.insert_word_count.Global Committer : word_count.0.paimon.table.word_count.commit.lastPartitionsWritten.\nFrom Flink Web-UI, go to the committer operator\u0026rsquo;s metrics, it\u0026rsquo;s shown as:\n0.Global_Committer___word_count.paimon.table.word_count.commit.lastPartitionsWritten.\n Please refer to System Scope to understand Flink scope Scan metrics are only supported by Flink versions \u0026gt;= 1.18      Scope Infix     Scan Metrics \u0026lt;host\u0026gt;.jobmanager.\u0026lt;job_name\u0026gt; \u0026lt;source_operator_name\u0026gt;.coordinator. enumerator.paimon.table.\u0026lt;table_name\u0026gt;.scan   Commit Metrics \u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt;.\u0026lt;job_name\u0026gt;.\u0026lt;committer_operator_name\u0026gt;.\u0026lt;subtask_index\u0026gt; paimon.table.\u0026lt;table_name\u0026gt;.commit   Write Metrics \u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt;.\u0026lt;job_name\u0026gt;.\u0026lt;writer_operator_name\u0026gt;.\u0026lt;subtask_index\u0026gt; paimon.table.\u0026lt;table_name\u0026gt;.partition.\u0026lt;partition_string\u0026gt;.bucket.\u0026lt;bucket_index\u0026gt;.writer   Write Buffer Metrics \u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt;.\u0026lt;job_name\u0026gt;.\u0026lt;writer_operator_name\u0026gt;.\u0026lt;subtask_index\u0026gt; paimon.table.\u0026lt;table_name\u0026gt;.writeBuffer   Compaction Metrics \u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt;.\u0026lt;job_name\u0026gt;.\u0026lt;writer_operator_name\u0026gt;.\u0026lt;subtask_index\u0026gt; paimon.table.\u0026lt;table_name\u0026gt;.partition.\u0026lt;partition_string\u0026gt;.bucket.\u0026lt;bucket_index\u0026gt;.compaction   Flink Source Metrics \u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt;.\u0026lt;job_name\u0026gt;.\u0026lt;source_operator_name\u0026gt;.\u0026lt;subtask_index\u0026gt; -   Flink Sink Metrics \u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt;.\u0026lt;job_name\u0026gt;.\u0026lt;committer_operator_name\u0026gt;.\u0026lt;subtask_index\u0026gt; -    Flink Connector Standard Metrics #  When using Flink to read and write, Paimon has implemented some key standard Flink connector metrics to measure the source latency and output of sink, see FLIP-33: Standardize Connector Metrics. Flink source / sink metrics implemented are listed here.\nSource Metrics (Flink) #    Metrics Name Level Type Description     currentEmitEventTimeLag Flink Source Operator Gauge Time difference between sending the record out of source and file creation.   currentFetchEventTimeLag Flink Source Operator Gauge Time difference between reading the data file and file creation.    Please note that if you specified consumer-id in your streaming query, the level of source metrics should turn into the reader operator, which is behind the Monitor operator.  Sink Metrics (Flink) #    Metrics Name Level Type Description     numBytesOut Table Counter The total number of output bytes.   numBytesOutPerSecond Table Meter The output bytes per second.   numRecordsOut Table Counter The total number of output records.   numRecordsOutPerSecond Table Meter The output records per second.    "});index.add({'id':87,'href':'/docs/1.1/concepts/spec/','title':"Specification",'section':"Concepts",'content':""});index.add({'id':88,'href':'/docs/1.1/maintenance/manage-privileges/','title':"Manage Privileges",'section':"Maintenance",'content':"Manage Privileges #  Paimon provides a privilege system on catalogs. Privileges determine which users can perform which operations on which objects, so that you can manage table access in a fine-grained manner.\nCurrently, Paimon adopts the identity-based access control (IBAC) privilege model. That is, privileges are directly assigned to users.\nThis privilege system only prevents unwanted users from accessing tables through catalogs. It does not block access through temporary table (by specifying table path on filesystem), nor does it prevent user from directly modifying data files on filesystem. If you need more serious protection, use a filesystem with access management instead.  Basic Concepts #  We now introduce the basic concepts of the privilege system.\nObject #  An object is an entity to which access can be granted. Unless allowed by a grant, access is denied.\nCurrently, the privilege system in Paimon has three types of objects: CATALOG, DATABASE and TABLE. Objects have a logical hierarchy, which is related to the concept they represent. For example:\n If a user is granted a privilege on the catalog, he will also have this privilege on all databases and all tables in the catalog. If a user is granted a privilege on the database, he will also have this privilege on all tables in that database. If a user is revoked a privilege from the catalog, he will also lose this privilege on all databases and all tables in the catalog. If a user is revoked a privilege from the database, he will also lose this privilege on all tables in that database.  Privilege #  A privilege is a defined level of access to an object. Multiple privileges can be used to control the granularity of access granted on an object. Privileges are object-specific. Different objects may have different privileges.\nCurrently, we support the following privileges.\n   Privilege Description Can be Granted on     SELECT Queries data in a table. TABLE, DATABASE, CATALOG   INSERT Inserts, updates or drops data in a table. Creates or drops tags and branches in a table. TABLE, DATABASE, CATALOG   ALTER_TABLE Alters metadata of a table, including table name, column names, table options, etc. TABLE, DATABASE, CATALOG   DROP_TABLE Drops a table. TABLE, DATABASE, CATALOG   CREATE_TABLE Creates a table in a database. DATABASE, CATALOG   DROP_DATABASE Drops a database. DATABASE, CATALOG   CREATE_DATABASE Creates a database in the catalog. CATALOG   ADMIN Creates or drops privileged users, grants or revokes privileges from users in a catalog. CATALOG    User #  The entity to which privileges can be granted. Users are authenticated by their password.\nWhen the privilege system is enabled, two special users will be created automatically.\n The root user, which is identified by the provided root password when enabling the privilege system. This user always has all privileges in the catalog. The anonymous user. This is the default user if no username and password is provided when creating the catalog.  Enable Privileges #  Paimon currently only supports file-based privilege system. Only catalogs with 'metastore' = 'filesystem' (the default value) or 'metastore' = 'hive' support such privilege system.\nTo enable the privilege system on a filesystem / Hive catalog, do the following steps.\nFlink 1.18\u0026#43; Run the following Flink SQL.\n-- use the catalog where you want to enable the privilege system USE CATALOG `my-catalog`; -- initialize privilege system by providing a root password -- change \u0026#39;root-password\u0026#39; to the password you want CALL sys.init_file_based_privilege(\u0026#39;root-password\u0026#39;);  After the privilege system is enabled, please re-create the catalog and authenticate as root to create other users and grant them privileges.\nPrivilege system does not affect existing catalogs. That is, these catalogs can still access and modify the tables freely. Please drop and re-create all catalogs with the desired warehouse path if you want to use the privilege system in these catalogs.  Accessing Privileged Catalogs #  To access a privileged catalog and to be authenticated as a user, you need to define user and password catalog options when creating the catalog. For example, the following SQL creates a catalog while trying to be authenticated as root, whose password is mypassword.\nFlink CREATE CATALOG `my-catalog` WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, -- ...  \u0026#39;user\u0026#39; = \u0026#39;root\u0026#39;, \u0026#39;password\u0026#39; = \u0026#39;mypassword\u0026#39; );  Creating Users #  You must be authenticated as a user with ADMIN privilege (for example, root) to perform this operation.\nDo the following steps to create a user in the privilege system.\nFlink 1.18\u0026#43; Run the following Flink SQL.\n-- use the catalog where you want to create a user -- you must be authenticated as a user with ADMIN privilege in this catalog USE CATALOG `my-catalog`; -- create a user authenticated by the specified password -- change \u0026#39;user\u0026#39; and \u0026#39;password\u0026#39; to the username and password you want CALL sys.create_privileged_user(\u0026#39;user\u0026#39;, \u0026#39;password\u0026#39;);  Dropping Users #  You must be authenticated as a user with ADMIN privilege (for example, root) to perform this operation.\nDo the following steps to drop a user in the privilege system.\nFlink 1.18\u0026#43; Run the following Flink SQL.\n-- use the catalog where you want to drop a user -- you must be authenticated as a user with ADMIN privilege in this catalog USE CATALOG `my-catalog`; -- change \u0026#39;user\u0026#39; to the username you want to drop CALL sys.drop_privileged_user(\u0026#39;user\u0026#39;);  Granting Privileges to Users #  You must be authenticated as a user with ADMIN privilege (for example, root) to perform this operation.\nDo the following steps to grant a user with privilege in the privilege system.\nFlink 1.18\u0026#43; Run the following Flink SQL.\n-- use the catalog where you want to drop a user -- you must be authenticated as a user with ADMIN privilege in this catalog USE CATALOG `my-catalog`; -- you can change \u0026#39;user\u0026#39; to the username you want, and \u0026#39;SELECT\u0026#39; to other privilege you want -- grant \u0026#39;user\u0026#39; with privilege \u0026#39;SELECT\u0026#39; on the whole catalog CALL sys.grant_privilege_to_user(\u0026#39;user\u0026#39;, \u0026#39;SELECT\u0026#39;); -- grant \u0026#39;user\u0026#39; with privilege \u0026#39;SELECT\u0026#39; on database my_db CALL sys.grant_privilege_to_user(\u0026#39;user\u0026#39;, \u0026#39;SELECT\u0026#39;, \u0026#39;my_db\u0026#39;); -- grant \u0026#39;user\u0026#39; with privilege \u0026#39;SELECT\u0026#39; on table my_db.my_tbl CALL sys.grant_privilege_to_user(\u0026#39;user\u0026#39;, \u0026#39;SELECT\u0026#39;, \u0026#39;my_db\u0026#39;, \u0026#39;my_tbl\u0026#39;);  Revoking Privileges to Users #  You must be authenticated as a user with ADMIN privilege (for example, root) to perform this operation.\nDo the following steps to revoke a privilege from user in the privilege system.\nFlink 1.18\u0026#43; Run the following Flink SQL.\n-- use the catalog where you want to drop a user -- you must be authenticated as a user with ADMIN privilege in this catalog USE CATALOG `my-catalog`; -- you can change \u0026#39;user\u0026#39; to the username you want, and \u0026#39;SELECT\u0026#39; to other privilege you want -- revoke \u0026#39;user\u0026#39; with privilege \u0026#39;SELECT\u0026#39; on the whole catalog CALL sys.revoke_privilege_from_user(\u0026#39;user\u0026#39;, \u0026#39;SELECT\u0026#39;); -- revoke \u0026#39;user\u0026#39; with privilege \u0026#39;SELECT\u0026#39; on database my_db CALL sys.revoke_privilege_from_user(\u0026#39;user\u0026#39;, \u0026#39;SELECT\u0026#39;, \u0026#39;my_db\u0026#39;); -- revoke \u0026#39;user\u0026#39; with privilege \u0026#39;SELECT\u0026#39; on table my_db.my_tbl CALL sys.revoke_privilege_from_user(\u0026#39;user\u0026#39;, \u0026#39;SELECT\u0026#39;, \u0026#39;my_db\u0026#39;, \u0026#39;my_tbl\u0026#39;);  "});index.add({'id':89,'href':'/docs/1.1/maintenance/manage-branches/','title':"Manage Branches",'section':"Maintenance",'content':"Manage Branches #  In streaming data processing, it\u0026rsquo;s difficult to correct data for it may affect the existing data, and users will see the streaming provisional results, which is not expected.\nWe suppose the branch that the existing workflow is processing on is \u0026lsquo;main\u0026rsquo; branch, by creating custom data branch, it can help to do experimental tests and data validating for the new job on the existing table, which doesn\u0026rsquo;t need to stop the existing reading / writing workflows and no need to copy data from the main branch.\nBy merge or replace branch operations, users can complete the correcting of data.\nCreate Branches #  Paimon supports creating branch from a specific tag, or just creating an empty branch which means the initial state of the created branch is like an empty table.\nFlink SQL Run the following sql:\n-- create branch named \u0026#39;branch1\u0026#39; from tag \u0026#39;tag1\u0026#39; CALL sys.create_branch(\u0026#39;default.T\u0026#39;, \u0026#39;branch1\u0026#39;, \u0026#39;tag1\u0026#39;); -- create empty branch named \u0026#39;branch1\u0026#39; CALL sys.create_branch(\u0026#39;default.T\u0026#39;, \u0026#39;branch1\u0026#39;); Flink Action Jar Run the following command:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  create_branch \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  --branch_name \u0026lt;branch-name\u0026gt; \\  [--tag_name \u0026lt;tag-name\u0026gt;] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] Spark SQL Run the following sql:\n-- create branch named \u0026#39;branch1\u0026#39; from tag \u0026#39;tag1\u0026#39; CALL sys.create_branch(\u0026#39;default.T\u0026#39;, \u0026#39;branch1\u0026#39;, \u0026#39;tag1\u0026#39;); -- create empty branch named \u0026#39;branch1\u0026#39; CALL sys.create_branch(\u0026#39;default.T\u0026#39;, \u0026#39;empty_branch\u0026#39;);  Delete Branches #  You can delete branch by its name.\nNote: The Delete Branches operation only deletes the metadata file. If you want to clear the data written during the branch, use remove_orphan_files  Flink SQL Run the following sql:\nCALL sys.delete_branch(\u0026#39;default.T\u0026#39;, \u0026#39;branch1\u0026#39;); Flink Action Jar Run the following command:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  delete_branch \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  --branch_name \u0026lt;branch-name\u0026gt; \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] Spark SQL Run the following sql:\nCALL sys.delete_branch(\u0026#39;default.T\u0026#39;, \u0026#39;branch1\u0026#39;);  Read / Write With Branch #  You can read or write with branch as below.\nFlink -- read from branch \u0026#39;branch1\u0026#39; SELECT * FROM `t$branch_branch1`; SELECT * FROM `t$branch_branch1` /*+ OPTIONS(\u0026#39;consumer-id\u0026#39; = \u0026#39;myid\u0026#39;) */; -- write to branch \u0026#39;branch1\u0026#39; INSERT INTO `t$branch_branch1` SELECT ... Spark SQL -- read from branch \u0026#39;branch1\u0026#39; SELECT * FROM `t$branch_branch1`; -- write to branch \u0026#39;branch1\u0026#39; INSERT INTO `t$branch_branch1` SELECT ... Spark DataFrame -- read from branch \u0026#39;branch1\u0026#39; spark.read.format(\u0026#34;paimon\u0026#34;).option(\u0026#34;branch\u0026#34;, \u0026#34;branch1\u0026#34;).table(\u0026#34;t\u0026#34;)  Fast Forward #  Fast-Forward the custom branch to main will delete all the snapshots, tags and schemas in the main branch that are created after the branch\u0026rsquo;s initial tag. And copy snapshots, tags and schemas from the branch to the main branch.\nIf your branch modifies the schema, after Fast Forward, if it is Spark SQL, you can execute REFRESH TABLE my_table to clean up the cache to avoid inconsistencies caused by caching.\nFlink SQL CALL sys.fast_forward(\u0026#39;default.T\u0026#39;, \u0026#39;branch1\u0026#39;); Flink Action Jar Run the following command:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  fast_forward \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  --branch_name \u0026lt;branch-name\u0026gt; \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]]  Batch Reading from Fallback Branch #  You can set the table option scan.fallback-branch so that when a batch job reads from the current branch, if a partition does not exist, the reader will try to read this partition from the fallback branch. For streaming read jobs, this feature is currently not supported, and will only produce results from the current branch.\nWhat\u0026rsquo;s the use case of this feature? Say you have created a Paimon table partitioned by date. You have a long-running streaming job which inserts records into Paimon, so that today\u0026rsquo;s data can be queried in time. You also have a batch job which runs at every night to insert corrected records of yesterday into Paimon, so that the preciseness of the data can be promised.\nWhen you query from this Paimon table, you would like to first read from the results of batch job. But if a partition (for example, today\u0026rsquo;s partition) does not exist in its result, then you would like to read from the results of streaming job. In this case, you can create a branch for streaming job, and set scan.fallback-branch to this streaming branch.\nLet\u0026rsquo;s look at an example.\nFlink -- create Paimon table CREATE TABLE T ( dt STRING NOT NULL, name STRING NOT NULL, amount BIGINT ) PARTITIONED BY (dt); -- create a branch for streaming job CALL sys.create_branch(\u0026#39;default.T\u0026#39;, \u0026#39;test\u0026#39;); -- set primary key and bucket number for the branch ALTER TABLE `T$branch_test` SET ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,name\u0026#39;, \u0026#39;bucket\u0026#39; = \u0026#39;2\u0026#39;, \u0026#39;changelog-producer\u0026#39; = \u0026#39;lookup\u0026#39; ); -- set fallback branch ALTER TABLE T SET ( \u0026#39;scan.fallback-branch\u0026#39; = \u0026#39;test\u0026#39; ); -- write records into the streaming branch INSERT INTO `T$branch_test` VALUES (\u0026#39;20240725\u0026#39;, \u0026#39;apple\u0026#39;, 4), (\u0026#39;20240725\u0026#39;, \u0026#39;peach\u0026#39;, 10), (\u0026#39;20240726\u0026#39;, \u0026#39;cherry\u0026#39;, 3), (\u0026#39;20240726\u0026#39;, \u0026#39;pear\u0026#39;, 6); -- write records into the default branch INSERT INTO T VALUES (\u0026#39;20240725\u0026#39;, \u0026#39;apple\u0026#39;, 5), (\u0026#39;20240725\u0026#39;, \u0026#39;banana\u0026#39;, 7); SELECT * FROM T; /* +------------------+------------------+--------+ | dt | name | amount | +------------------+------------------+--------+ | 20240725 | apple | 5 | | 20240725 | banana | 7 | | 20240726 | cherry | 3 | | 20240726 | pear | 6 | +------------------+------------------+--------+ */ -- reset fallback branch ALTER TABLE T RESET ( \u0026#39;scan.fallback-branch\u0026#39; ); -- now it only reads from default branch SELECT * FROM T; /* +------------------+------------------+--------+ | dt | name | amount | +------------------+------------------+--------+ | 20240725 | apple | 5 | | 20240725 | banana | 7 | +------------------+------------------+--------+ */  "});index.add({'id':90,'href':'/docs/1.1/maintenance/manage-partitions/','title':"Manage Partitions",'section':"Maintenance",'content':"Manage Partitions #  Paimon provides multiple ways to manage partitions, including expire historical partitions by different strategies or mark a partition done to notify the downstream application that the partition has finished writing.\nExpiring Partitions #  You can set partition.expiration-time when creating a partitioned table. Paimon streaming sink will periodically check the status of partitions and delete expired partitions according to time.\nHow to determine whether a partition has expired: you can set partition.expiration-strategy when creating a partitioned table, this strategy determines how to extract the partition time and compare it with the current time to see if survival time has exceeded the partition.expiration-time. Expiration strategy supported values are:\n values-time : The strategy compares the time extracted from the partition value with the current time, this strategy as the default. update-time : The strategy compares the last update time of the partition with the current time. What is the scenario for this strategy:  Your partition value is non-date formatted. You only want to keep data that has been updated in the last n days/months/years. Data initialization imports a large amount of historical data.    Note: After the partition expires, it is logically deleted and the latest snapshot cannot query its data. But the files in the file system are not immediately physically deleted, it depends on when the corresponding snapshot expires. See Expire Snapshots.  An example for single partition field:\nvalues-time strategy.\nCREATE TABLE t (...) PARTITIONED BY (dt) WITH ( \u0026#39;partition.expiration-time\u0026#39; = \u0026#39;7 d\u0026#39;, \u0026#39;partition.expiration-check-interval\u0026#39; = \u0026#39;1 d\u0026#39;, \u0026#39;partition.timestamp-formatter\u0026#39; = \u0026#39;yyyyMMdd\u0026#39; -- this is required in `values-time` strategy. ); -- Let\u0026#39;s say now the date is 2024-07-09，so before the date of 2024-07-02 will expire. insert into t values(\u0026#39;pk\u0026#39;, \u0026#39;2024-07-01\u0026#39;); -- An example for multiple partition fields CREATE TABLE t (...) PARTITIONED BY (other_key, dt) WITH ( \u0026#39;partition.expiration-time\u0026#39; = \u0026#39;7 d\u0026#39;, \u0026#39;partition.expiration-check-interval\u0026#39; = \u0026#39;1 d\u0026#39;, \u0026#39;partition.timestamp-formatter\u0026#39; = \u0026#39;yyyyMMdd\u0026#39;, \u0026#39;partition.timestamp-pattern\u0026#39; = \u0026#39;$dt\u0026#39; ); update-time strategy.\nCREATE TABLE t (...) PARTITIONED BY (dt) WITH ( \u0026#39;partition.expiration-time\u0026#39; = \u0026#39;7 d\u0026#39;, \u0026#39;partition.expiration-check-interval\u0026#39; = \u0026#39;1 d\u0026#39;, \u0026#39;partition.expiration-strategy\u0026#39; = \u0026#39;update-time\u0026#39; ); -- The last update time of the partition is now, so it will not expire. insert into t values(\u0026#39;pk\u0026#39;, \u0026#39;2024-01-01\u0026#39;); -- Support non-date formatted partition. insert into t values(\u0026#39;pk\u0026#39;, \u0026#39;par-1\u0026#39;); More options:\n  Option Default Type Description     partition.expiration-strategy values-time String  Specifies the expiration strategy for partition expiration. Possible values: values-time: The strategy compares the time extracted from the partition value with the current time. update-time: The strategy compares the last update time of the partition with the current time.    partition.expiration-check-interval 1 h Duration The check interval of partition expiration.   partition.expiration-time (none) Duration The expiration interval of a partition. A partition will be expired if it‘s lifetime is over this value. Partition time is extracted from the partition value.   partition.timestamp-formatter (none) String The formatter to format timestamp from string. It can be used with 'partition.timestamp-pattern' to create a formatter using the specified value.Default formatter is 'yyyy-MM-dd HH:mm:ss' and 'yyyy-MM-dd'.Supports multiple partition fields like '$year-$month-$day $hour:00:00'.The timestamp-formatter is compatible with Java's DateTimeFormatter.   partition.timestamp-pattern (none) String You can specify a pattern to get a timestamp from partitions. The formatter pattern is defined by 'partition.timestamp-formatter'.By default, read from the first field.If the timestamp in the partition is a single field called 'dt', you can use '$dt'.If it is spread across multiple fields for year, month, day, and hour, you can use '$year-$month-$day $hour:00:00'.If the timestamp is in fields dt and hour, you can use '$dt $hour:00:00'.   end-input.check-partition-expire false Boolean Whether check partition expire after batch mode or bounded stream job finish.    Partition Mark Done #  You can use the option 'partition.mark-done-action' to configure the action when a partition needs to be mark done.\n success-file: add \u0026lsquo;_success\u0026rsquo; file to directory. done-partition: add \u0026lsquo;xxx.done\u0026rsquo; partition to metastore. mark-event: mark partition event to metastore. http-report: report partition mark done to remote http server. custom: use policy class to create a mark-partition policy. These actions can be configured at the same time: \u0026lsquo;done-partition,success-file,mark-event,custom\u0026rsquo;.  Paimon partition mark done can be triggered both by streaming write and batch write.\nStreaming Mark Done #  You can use the options 'partition.idle-time-to-done' to set a partition idle time to done duration. When a partition has no new data after this time duration, the mark done action will be triggered to indicate that the data is ready.\nBy default, Flink will use process time as idle time to trigger partition mark done. You can also use watermark to trigger partition mark done. This will make the partition mark done time more accurate when data is delayed. You can enable this by setting 'partition.mark-done-action.mode' = 'watermark'.\nBatch Mark Done #  For batch mode, you can trigger partition mark done when end input by setting 'partition.end-input-to-done'='true'.\n"});index.add({'id':91,'href':'/docs/1.1/ecosystem/','title':"Ecosystem",'section':"Apache Paimon",'content':""});index.add({'id':92,'href':'/docs/1.1/cdc-ingestion/','title':"CDC Ingestion",'section':"Apache Paimon",'content':""});index.add({'id':93,'href':'/docs/1.1/flink/clone-tables/','title':"Clone Tables",'section':"Engine Flink",'content':"Clone Tables #  Paimon supports cloning tables for data migration. Currently, only table files used by the latest snapshot will be cloned.\nTo clone a table, run the following command to submit a clone job. If the table you clone is not modified at the same time, it is recommended to submit a Flink batch job for better performance. However, if you want to clone the table while writing it at the same time, submit a Flink streaming job for automatic failure recovery.\nFlink SQL CALL sys.clone( warehouse =\u0026gt; \u0026#39;source_warehouse_path\u0026#39;, [`database` =\u0026gt; \u0026#39;source_database_name\u0026#39;,] [`table` =\u0026gt; \u0026#39;source_table_name\u0026#39;,] target_warehouse =\u0026gt; \u0026#39;target_warehouse_path\u0026#39;, [target_database =\u0026gt; \u0026#39;target_database_name\u0026#39;,] [target_table =\u0026gt; \u0026#39;target_table_name\u0026#39;,] [parallelism =\u0026gt; \u0026lt;parallelism\u0026gt;] );  If database is not specified, all tables in all databases of the specified warehouse will be cloned. If table is not specified, all tables of the specified database will be cloned.   Example: Clone test_db.test_table from source warehouse to target warehouse.\nCALL sys.clone( `warehouse` =\u0026gt; \u0026#39;s3:///path/to/warehouse_source\u0026#39;, `database` =\u0026gt; \u0026#39;test_db\u0026#39;, `table` =\u0026gt; \u0026#39;test_table\u0026#39;, `catalog_conf` =\u0026gt; \u0026#39;s3.endpoint=https://****.com;s3.access-key=*****;s3.secret-key=*****\u0026#39;, `target_warehouse` =\u0026gt; \u0026#39;s3:///path/to/warehouse_target\u0026#39;, `target_database` =\u0026gt; \u0026#39;test_db\u0026#39;, `target_table` =\u0026gt; \u0026#39;test_table\u0026#39;, `target_catalog_conf` =\u0026gt; \u0026#39;s3.endpoint=https://****.com;s3.access-key=*****;s3.secret-key=*****\u0026#39; ); Flink Action \u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  clone \\  --warehouse \u0026lt;source-warehouse-path\u0026gt; \\  [--database \u0026lt;source-database-name\u0026gt;] \\  [--table \u0026lt;source-table-name\u0026gt;] \\  [--catalog_conf \u0026lt;source-paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;source-paimon-catalog-conf\u0026gt; ...]] \\  --target_warehouse \u0026lt;target-warehouse-path\u0026gt; \\  [--target_database \u0026lt;target-database\u0026gt;] \\  [--target_table \u0026lt;target-table-name\u0026gt;] \\  [--target_catalog_conf \u0026lt;target-paimon-catalog-conf\u0026gt; [--target_catalog_conf \u0026lt;target-paimon-catalog-conf\u0026gt; ...]] [--parallelism \u0026lt;parallelism\u0026gt;]  If database is not specified, all tables in all databases of the specified warehouse will be cloned. If table is not specified, all tables of the specified database will be cloned.   Example: Clone test_db.test_table from source warehouse to target warehouse.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  clone \\  --warehouse s3:///path/to/warehouse_source \\  --database test_db \\  --table test_table \\  --catalog_conf s3.endpoint=https://****.com \\  --catalog_conf s3.access-key=***** \\  --catalog_conf s3.secret-key=***** \\  --target_warehouse s3:///path/to/warehouse_target \\  --target_database test_db \\  --target_table test_table \\  --target_catalog_conf s3.endpoint=https://****.com \\  --target_catalog_conf s3.access-key=***** \\  --target_catalog_conf s3.secret-key=***** For more usage of the clone action, see\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  clone --help  "});index.add({'id':94,'href':'/docs/1.1/maintenance/','title':"Maintenance",'section':"Apache Paimon",'content':""});index.add({'id':95,'href':'/docs/1.1/program-api/','title':"Program API",'section':"Apache Paimon",'content':""});index.add({'id':96,'href':'/docs/1.1/migration/','title':"Migration",'section':"Apache Paimon",'content':""});index.add({'id':97,'href':'/docs/1.1/flink/procedures/','title':"Procedures",'section':"Engine Flink",'content':"Procedures #  Flink 1.18 and later versions support Call Statements, which make it easier to manipulate data and metadata of Paimon table by writing SQLs instead of submitting Flink jobs.\nIn 1.18, the procedure only supports passing arguments by position. You must pass all arguments in order, and if you don\u0026rsquo;t want to pass some arguments, you must use '' as placeholder. For example, if you want to compact table default.t with parallelism 4, but you don\u0026rsquo;t want to specify partitions and sort strategy, the call statement should be\nCALL sys.compact('default.t', '', '', '', 'sink.parallelism=4').\nIn higher versions, the procedure supports passing arguments by name. You can pass arguments in any order and any optional argument can be omitted. For the above example, the call statement is\nCALL sys.compact(`table` =\u0026gt; 'default.t', options =\u0026gt; 'sink.parallelism=4').\nSpecify partitions: we use string to represent partition filter. \u0026ldquo;,\u0026rdquo; means \u0026ldquo;AND\u0026rdquo; and \u0026ldquo;;\u0026rdquo; means \u0026ldquo;OR\u0026rdquo;. For example, if you want to specify two partitions date=01 and date=02, you need to write \u0026lsquo;date=01;date=02\u0026rsquo;; If you want to specify one partition with date=01 and day=01, you need to write \u0026lsquo;date=01,day=01\u0026rsquo;.\nTable options syntax: we use string to represent table options. The format is \u0026lsquo;key1=value1,key2=value2\u0026hellip;\u0026rsquo;.\nAll available procedures are listed below.\n  Procedure Name Usage Explanation Example    compact  -- Use named argument CALL [catalog.]sys.compact( `table` = 'table', partitions = 'partitions', order_strategy = 'order_strategy', order_by = 'order_by', options = 'options', `where` = 'where', partition_idle_time = 'partition_idle_time', compact_strategy = 'compact_strategy')  -- Use indexed argument CALL [catalog.]sys.compact('table')  CALL [catalog.]sys.compact('table', 'partitions')  CALL [catalog.]sys.compact('table', 'order_strategy', 'order_by')  CALL [catalog.]sys.compact('table', 'partitions', 'order_strategy', 'order_by')  CALL [catalog.]sys.compact('table', 'partitions', 'order_strategy', 'order_by', 'options')  CALL [catalog.]sys.compact('table', 'partitions', 'order_strategy', 'order_by', 'options', 'where')  CALL [catalog.]sys.compact('table', 'partitions', 'order_strategy', 'order_by', 'options', 'where', 'partition_idle_time')  CALL [catalog.]sys.compact('table', 'partitions', 'order_strategy', 'order_by', 'options', 'where', 'partition_idle_time', 'compact_strategy')    To compact a table. Arguments: table(required): the target table identifier. partitions(optional): partition filter. order_strategy(optional): 'order' or 'zorder' or 'hilbert' or 'none'. order_by(optional): the columns need to be sort. Left empty if 'order_strategy' is 'none'. options(optional): additional dynamic options of the table. where(optional): partition predicate(Can't be used together with \"partitions\"). Note: as where is a keyword,a pair of backticks need to add around like `where`. partition_idle_time(optional): this is used to do a full compaction for partition which had not received any new data for 'partition_idle_time'. And only these partitions will be compacted. This argument can not be used with order compact. compact_strategy(optional): this determines how to pick files to be merged, the default is determined by the runtime execution mode. 'full' strategy only supports batch mode. All files will be selected for merging. 'minor' strategy: Pick the set of files that need to be merged based on specified conditions.   -- use partition filter  CALL sys.compact(`table` = 'default.T', partitions = 'p=0', order_strategy = 'zorder', order_by = 'a,b', options = 'sink.parallelism=4')  -- use partition predicate  CALL sys.compact(`table` = 'default.T', `where` = 'dt10 and h'zorder', order_by = 'a,b', options = 'sink.parallelism=4')    compact_database  -- Use named argument CALL [catalog.]sys.compact_database( including_databases = 'includingDatabases', mode = 'mode', including_tables = 'includingTables', excluding_tables = 'excludingTables', table_options = 'tableOptions', partition_idle_time = 'partitionIdleTime', compact_strategy = 'compact_strategy')  -- Use indexed argument CALL [catalog.]sys.compact_database()  CALL [catalog.]sys.compact_database('includingDatabases')  CALL [catalog.]sys.compact_database('includingDatabases', 'mode')  CALL [catalog.]sys.compact_database('includingDatabases', 'mode', 'includingTables')  CALL [catalog.]sys.compact_database('includingDatabases', 'mode', 'includingTables', 'excludingTables')  CALL [catalog.]sys.compact_database('includingDatabases', 'mode', 'includingTables', 'excludingTables', 'tableOptions')  CALL [catalog.]sys.compact_database('includingDatabases', 'mode', 'includingTables', 'excludingTables', 'tableOptions', 'partitionIdleTime') CALL [catalog.]sys.compact_database('includingDatabases', 'mode', 'includingTables', 'excludingTables', 'tableOptions', 'partitionIdleTime', 'compact_strategy')   To compact databases. Arguments: includingDatabases: to specify databases. You can use regular expression. mode: compact mode. \"divided\": start a sink for each table, detecting the new table requires restarting the job; \"combined\" (default): start a single combined sink for all tables, the new table will be automatically detected.  includingTables: to specify tables. You can use regular expression. excludingTables: to specify tables that are not compacted. You can use regular expression. tableOptions: additional dynamic options of the table. partition_idle_time: this is used to do a full compaction for partition which had not received any new data for 'partition_idle_time'. And only these partitions will be compacted. compact_strategy(optional): this determines how to pick files to be merged, the default is determined by the runtime execution mode. 'full' strategy only supports batch mode. All files will be selected for merging. 'minor' strategy: Pick the set of files that need to be merged based on specified conditions.   CALL sys.compact_database( including_databases = 'db1|db2', mode = 'combined', including_tables = 'table_.*', excluding_tables = 'ignore', table_options = 'sink.parallelism=4', compat_strategy = 'full')    create_tag  -- Use named argument -- based on the specified snapshot  CALL [catalog.]sys.create_tag(`table` = 'identifier', tag = 'tagName', snapshot_id = snapshotId)  -- based on the latest snapshot  CALL [catalog.]sys.create_tag(`table` = 'identifier', tag = 'tagName')  -- Use indexed argument -- based on the specified snapshot  CALL [catalog.]sys.create_tag('identifier', 'tagName', snapshotId)  -- based on the latest snapshot  CALL [catalog.]sys.create_tag('identifier', 'tagName')   To create a tag based on given snapshot. Arguments: table: the target table identifier. Cannot be empty. tagName: name of the new tag. snapshotId (Long): id of the snapshot which the new tag is based on. time_retained: The maximum time retained for newly created tags.   CALL sys.create_tag(`table` = 'default.T', tag = 'my_tag', snapshot_id = cast(10 as bigint), time_retained = '1 d')    create_tag_from_timestamp  -- Create a tag from the first snapshot whose commit-time greater than the specified timestamp.  -- Use named argument CALL [catalog.]sys.create_tag_from_timestamp(`table` = 'identifier', tag = 'tagName', timestamp = timestamp, time_retained = time_retained)  -- Use indexed argument CALL [catalog.]sys.create_tag_from_timestamp('identifier', 'tagName', timestamp, time_retained)   To create a tag based on given timestamp. Arguments: table: the target table identifier. Cannot be empty. tag: name of the new tag. timestamp (Long): Find the first snapshot whose commit-time greater than this timestamp. time_retained : The maximum time retained for newly created tags.   -- for Flink 1.18 CALL sys.create_tag_from_timestamp('default.T', 'my_tag', 1724404318750, '1 d') -- for Flink 1.19 and later CALL sys.create_tag_from_timestamp(`table` = 'default.T', `tag` = 'my_tag', `timestamp` = 1724404318750, time_retained = '1 d')    create_tag_from_watermark  -- Create a tag from the first snapshot whose watermark greater than the specified timestamp. -- Use named argument CALL [catalog.]sys.create_tag_from_watermark(`table` = 'identifier', tag = 'tagName', watermark = watermark, time_retained = time_retained)  -- Use indexed argument CALL [catalog.]sys.create_tag_from_watermark('identifier', 'tagName', watermark, time_retained)   To create a tag based on given watermark timestamp. Arguments: table: the target table identifier. Cannot be empty. tag: name of the new tag. watermark (Long): Find the first snapshot whose watermark greater than the specified watermark. time_retained : The maximum time retained for newly created tags.   -- for Flink 1.18 CALL sys.create_tag_from_watermark('default.T', 'my_tag', 1724404318750, '1 d') -- for Flink 1.19 and later CALL sys.create_tag_from_watermark(`table` = 'default.T', `tag` = 'my_tag', `watermark` = 1724404318750, time_retained = '1 d')    delete_tag  -- Use named argument CALL [catalog.]sys.delete_tag(`table` = 'identifier', tag = 'tagName')  -- Use indexed argument CALL [catalog.]sys.delete_tag('identifier', 'tagName')   To delete a tag. Arguments: table: the target table identifier. Cannot be empty. tagName: name of the tag to be deleted. If you specify multiple tags, delimiter is ','.   CALL sys.delete_tag(`table` = 'default.T', tag = 'my_tag')    replace_tag  -- Use named argument -- replace tag with new time retained  CALL [catalog.]sys.replace_tag(`table` = 'identifier', tag = 'tagName', time_retained = 'timeRetained')  -- replace tag with new snapshot id and time retained  CALL [catalog.]sys.replace_tag(`table` = 'identifier', snapshot_id = 'snapshotId')  -- Use indexed argument -- replace tag with new snapshot id and time retained  CALL [catalog.]sys.replace_tag('identifier', 'tagName', 'snapshotId', 'timeRetained')    To replace an existing tag with new tag info. Arguments: table: the target table identifier. Cannot be empty. tag: name of the existed tag. Cannot be empty. snapshot(Long): id of the snapshot which the tag is based on, it is optional. time_retained: The maximum time retained for the existing tag, it is optional.   -- for Flink 1.18 CALL sys.replace_tag('default.T', 'my_tag', 5, '1 d') -- for Flink 1.19 and later CALL sys.replace_tag(`table` = 'default.T', tag = 'my_tag', snapshot_id = 5, time_retained = '1 d')    expire_tags  CALL [catalog.]sys.expire_tags('identifier', 'older_than')   To expire tags by time. Arguments: table: the target table identifier. Cannot be empty. older_than: tagCreateTime before which tags will be removed.   CALL sys.expire_tags(table = 'default.T', older_than = '2024-09-06 11:00:00')    merge_into  -- for Flink 1.18 CALL [catalog.]sys.merge_into('identifier','targetAlias', 'sourceSqls','sourceTable','mergeCondition', 'matchedUpsertCondition','matchedUpsertSetting', 'notMatchedInsertCondition','notMatchedInsertValues', 'matchedDeleteCondition') -- for Flink 1.19 and later  CALL [catalog.]sys.merge_into( target_table = 'identifier', target_alias = 'targetAlias', source_sqls = 'sourceSqls', source_table = 'sourceTable', merge_condition = 'mergeCondition', matched_upsert_condition = 'matchedUpsertCondition', matched_upsert_setting = 'matchedUpsertSetting', not_matched_insert_condition = 'notMatchedInsertCondition', not_matched_insert_values = 'notMatchedInsertValues', matched_delete_condition = 'matchedDeleteCondition', not_matched_by_source_upsert_condition = 'notMatchedBySourceUpsertCondition', not_matched_by_source_upsert_setting = 'notMatchedBySourceUpsertSetting', not_matched_by_source_delete_condition = 'notMatchedBySourceDeleteCondition')    To perform \"MERGE INTO\" syntax. See merge_into action for details of arguments.   -- for matched order rows, -- increase the price, -- and if there is no match, -- insert the order from -- the source table -- for Flink 1.18 CALL sys.merge_into('default.T','','','default.S','T.id=S.order_id','','price=T.price+20','','*','') -- for Flink 1.19 and later  CALL sys.merge_into( target_table = 'default.T', source_table = 'default.S', merge_condition = 'T.id=S.order_id', matched_upsert_setting = 'price=T.price+20', not_matched_insert_values = '*')    remove_orphan_files  -- Use named argument CALL [catalog.]sys.remove_orphan_files(`table` = 'identifier', older_than = 'olderThan', dry_run = 'dryRun', mode = 'mode')  -- Use indexed argument CALL [catalog.]sys.remove_orphan_files('identifier') CALL [catalog.]sys.remove_orphan_files('identifier', 'olderThan') CALL [catalog.]sys.remove_orphan_files('identifier', 'olderThan', 'dryRun') CALL [catalog.]sys.remove_orphan_files('identifier', 'olderThan', 'dryRun','parallelism') CALL [catalog.]sys.remove_orphan_files('identifier', 'olderThan', 'dryRun','parallelism','mode')   To remove the orphan data files and metadata files. Arguments: table: the target table identifier. Cannot be empty, you can use database_name.* to clean whole database. olderThan: to avoid deleting newly written files, this procedure only deletes orphan files older than 1 day by default. This argument can modify the interval.  dryRun: when true, view only orphan files, don't actually remove files. Default is false. parallelism: The maximum number of concurrent deleting files. By default is the number of processors available to the Java virtual machine. mode: The mode of remove orphan clean procedure (local or distributed) . By default is distributed.  CALL sys.remove_orphan_files(`table` = 'default.T', older_than = '2023-10-31 12:00:00') CALL sys.remove_orphan_files(`table` = 'default.*', older_than = '2023-10-31 12:00:00') CALL sys.remove_orphan_files(`table` = 'default.T', older_than = '2023-10-31 12:00:00', dry_run = true) CALL sys.remove_orphan_files(`table` = 'default.T', older_than = '2023-10-31 12:00:00', dry_run = false, parallelism = '5') CALL sys.remove_orphan_files(`table` = 'default.T', older_than = '2023-10-31 12:00:00', dry_run = false, parallelism = '5', mode = 'local')    remove_unexisting_files  -- Use named argument CALL [catalog.]sys.remove_unexisting_files(`table` = 'identifier', dry_run = 'dryRun', parallelism = 'parallelism')  -- Use indexed argument CALL [catalog.]sys.remove_unexisting_files('identifier') CALL [catalog.]sys.remove_unexisting_files('identifier', 'dryRun', 'parallelism')   Procedure to remove unexisting data files from manifest entries. See Java docs for detailed use cases. Arguments: table: the target table identifier. Cannot be empty, you can use database_name.* to clean whole database. dry_run (optional): only check what files will be removed, but not really remove them. Default is false. parallelism (optional): number of parallelisms to check files in the manifests. Note that user is on his own risk using this procedure, which may cause data loss when used outside from the use cases listed in Java docs.   -- remove unexisting data files in the table `mydb.myt` CALL sys.remove_unexisting_files(`table` = 'mydb.myt') -- only check what files will be removed, but not really remove them (dry run) CALL sys.remove_unexisting_files(`table` = 'mydb.myt', `dry_run` = true)    reset_consumer  -- Use named argument CALL [catalog.]sys.reset_consumer(`table` = 'identifier', consumer_id = 'consumerId', next_snapshot_id = 'nextSnapshotId')  -- Use indexed argument -- reset the new next snapshot id in the consumer CALL [catalog.]sys.reset_consumer('identifier', 'consumerId', nextSnapshotId) -- delete consumer CALL [catalog.]sys.reset_consumer('identifier', 'consumerId')   To reset or delete consumer. Arguments: table: the target table identifier. Cannot be empty. consumerId: consumer to be reset or deleted. nextSnapshotId (Long): the new next snapshot id of the consumer.  CALL sys.reset_consumer(`table` = 'default.T', consumer_id = 'myid', next_snapshot_id = cast(10 as bigint))   clear_consumers  -- Use named argument CALL [catalog.]sys.clear_consumers(`table` = 'identifier', including_consumers = 'includingConsumers', excluding_consumers = 'excludingConsumers')  -- Use indexed argument -- clear all consumers in the table CALL [catalog.]sys.clear_consumers('identifier') -- clear some consumers in the table (accept regular expression) CALL [catalog.]sys.clear_consumers('identifier', 'includingConsumers') -- exclude some consumers (accept regular expression) CALL [catalog.]sys.clear_consumers('identifier', 'includingConsumers', 'excludingConsumers')   To reset or delete consumer. Arguments: table: the target table identifier. Cannot be empty. includingConsumers: consumers to be cleared. excludingConsumers: consumers which not to be cleared.  CALL sys.clear_consumers(`table` = 'default.T') CALL sys.clear_consumers(`table` = 'default.T', including_consumers = 'myid.*') CALL sys.clear_consumers(table = 'default.T', including_consumers = '', excluding_consumers = 'myid1.*') CALL sys.clear_consumers(table = 'default.T', including_consumers = 'myid.*', excluding_consumers = 'myid1.*')    rollback_to  -- for Flink 1.18 -- rollback to a snapshot CALL [catalog.]sys.rollback_to('identifier', snapshotId) -- rollback to a tag CALL [catalog.]sys.rollback_to('identifier', 'tagName') -- for Flink 1.19 and later -- rollback to a snapshot CALL [catalog.]sys.rollback_to(`table` = 'identifier', snapshot_id = snapshotId) -- rollback to a tag CALL [catalog.]sys.rollback_to(`table` = 'identifier', tag = 'tagName')   To rollback to a specific version of target table. Argument: table: the target table identifier. Cannot be empty. snapshotId (Long): id of the snapshot that will roll back to. tagName: name of the tag that will roll back to.   -- for Flink 1.18 CALL sys.rollback_to('default.T', 10) -- for Flink 1.19 and later CALL sys.rollback_to(`table` = 'default.T', snapshot_id = 10)    rollback_to_timestamp  -- for Flink 1.18 -- rollback to the snapshot which earlier or equal than timestamp. CALL [catalog.]sys.rollback_to_timestamp('identifier', timestamp) -- for Flink 1.19 and later -- rollback to the snapshot which earlier or equal than timestamp. CALL [catalog.]sys.rollback_to_timestamp(`table` = 'default.T', `timestamp` = timestamp)   To rollback to the snapshot which earlier or equal than timestamp. Argument: table: the target table identifier. Cannot be empty. timestamp (Long): Roll back to the snapshot which earlier or equal than timestamp.   -- for Flink 1.18 CALL sys.rollback_to_timestamp('default.T', 10) -- for Flink 1.19 and later CALL sys.rollback_to_timestamp(`table` = 'default.T', timestamp = 1730292023000)    rollback_to_watermark  -- for Flink 1.18 -- rollback to the snapshot which earlier or equal than watermark. CALL [catalog.]sys.rollback_to_watermark('identifier', watermark) -- for Flink 1.19 and later -- rollback to the snapshot which earlier or equal than watermark. CALL [catalog.]sys.rollback_to_watermark(`table` = 'default.T', `watermark` = watermark)   To rollback to the snapshot which earlier or equal than watermark. Argument: table: the target table identifier. Cannot be empty. watermark (Long): Roll back to the snapshot which earlier or equal than watermark.   -- for Flink 1.18 CALL sys.rollback_to_watermark('default.T', 1730292023000) -- for Flink 1.19 and later CALL sys.rollback_to_watermark(`table` = 'default.T', watermark = 1730292023000)    purge_files  -- clear table with purge files. CALL [catalog.]sys.purge_files('identifier')   To clear table with purge files. Argument: table: the target table identifier. Cannot be empty.   CALL sys.purge_files('default.T')    migrate_database  -- for Flink 1.18 -- migrate all hive tables in database to paimon tables. CALL [catalog.]sys.migrate_database('connector', 'dbIdentifier', 'options'[, \u0026ltparallelism\u0026gt]) -- for Flink 1.19 and later -- migrate all hive tables in database to paimon tables. CALL [catalog.]sys.migrate_database(connector = 'connector', source_database = 'dbIdentifier', options = 'options'[, \u0026ltparallelism = parallelism\u0026gt])   To migrate all hive tables in database to paimon table. Argument: connector: the origin database's type to be migrated, such as hive. Cannot be empty. source_database: name of the origin database to be migrated. Cannot be empty. options: the table options of the paimon table to migrate. parallelism: the parallelism for migrate process, default is core numbers of machine.   -- for Flink 1.18 CALL sys.migrate_database('hive', 'db01', 'file.format=parquet', 6) -- for Flink 1.19 and later CALL sys.migrate_database(connector = 'hive', source_database = 'db01', options = 'file.format=parquet', parallelism = 6)    migrate_table  -- migrate hive table to a paimon table. CALL [catalog.]sys.migrate_table(connector = 'connector', source_table = 'tableIdentifier', options = 'options'[, \u0026ltparallelism = parallelism\u0026gt])   To migrate hive table to a paimon table. Argument: connector: the origin table's type to be migrated, such as hive. Cannot be empty. source_table: name of the origin table to be migrated. Cannot be empty. target_table: name of the target paimon table to migrate. If not set would keep the same name with origin table options: the table options of the paimon table to migrate. parallelism: the parallelism for migrate process, default is core numbers of machine. delete_origin: If had set target_table, can set delete_origin to decide whether delete the origin table metadata from hms after migrate. Default is true   CALL sys.migrate_table(connector = 'hive', source_table = 'db01.t1', options = 'file.format=parquet', parallelism = 6)    migrate_iceberg_table  -- Use named argument CALL sys.migrate_iceberg_table(source_table = 'database_name.table_name', iceberg_options = 'iceberg_options', options = 'paimon_options', parallelism = parallelism); -- Use indexed argument CALL sys.migrate_iceberg_table('source_table','iceberg_options', 'options', 'parallelism');   To migrate iceberg table to paimon. Arguments: source_table: string type, is used to specify the source iceberg table to migrate, it's required. iceberg_options: string type, is used to specify the configuration of migration, multiple configuration items are separated by commas. it's required. options: string type, is used to specify the additional options for the target paimon table, it's optional. parallelism: integer type, is used to specify the parallelism of the migration job, it's optional.   CALL sys.migrate_iceberg_table(source_table = 'iceberg_db.iceberg_tbl',iceberg_options = 'metadata.iceberg.storage=hadoop-catalog,iceberg_warehouse=/path/to/iceberg/warehouse');    expire_snapshots  -- Use named argument CALL [catalog.]sys.expire_snapshots( `table` = 'identifier',  retain_max = 'retain_max',  retain_min = 'retain_min',  older_than = 'older_than',  max_deletes = 'max_deletes')  -- Use indexed argument -- for Flink 1.18 CALL [catalog.]sys.expire_snapshots(table, retain_max) -- for Flink 1.19 and later CALL [catalog.]sys.expire_snapshots(table, retain_max, retain_min, older_than, max_deletes)   To expire snapshots. Argument: table: the target table identifier. Cannot be empty. retain_max: the maximum number of completed snapshots to retain. retain_min: the minimum number of completed snapshots to retain. order_than: timestamp before which snapshots will be removed. max_deletes: the maximum number of snapshots that can be deleted at once.   -- for Flink 1.18 CALL sys.expire_snapshots('default.T', 2) -- for Flink 1.19 and later CALL sys.expire_snapshots(`table` = 'default.T', retain_max = 2) CALL sys.expire_snapshots(`table` = 'default.T', older_than = '2024-01-01 12:00:00') CALL sys.expire_snapshots(`table` = 'default.T', older_than = '2024-01-01 12:00:00', retain_min = 10) CALL sys.expire_snapshots(`table` = 'default.T', older_than = '2024-01-01 12:00:00', max_deletes = 10)    expire_changelogs  -- Use named argument CALL [catalog.]sys.expire_changelogs( `table` = 'identifier',  retain_max = 'retain_max',  retain_min = 'retain_min',  older_than = 'older_than',  max_deletes = 'max_deletes')  delete_all = 'delete_all')  -- Use indexed argument -- for Flink 1.18 CALL [catalog.]sys.expire_changelogs(table, retain_max, retain_min, older_than, max_deletes) CALL [catalog.]sys.expire_changelogs(table, delete_all) -- for Flink 1.19 and later CALL [catalog.]sys.expire_changelogs(table, retain_max, retain_min, older_than, max_deletes, delete_all)   To expire changelogs. Argument: table: the target table identifier. Cannot be empty. retain_max: the maximum number of completed changelogs to retain. retain_min: the minimum number of completed changelogs to retain. order_than: timestamp before which changelogs will be removed. max_deletes: the maximum number of changelogs that can be deleted at once. delete_all: whether to delete all separated changelogs.   -- for Flink 1.18 CALL sys.expire_changelogs('default.T', 4, 2, '2024-01-01 12:00:00', 2) CALL sys.expire_changelogs('default.T', true) -- for Flink 1.19 and later CALL sys.expire_changelogs(`table` = 'default.T', retain_max = 2) CALL sys.expire_changelogs(`table` = 'default.T', older_than = '2024-01-01 12:00:00') CALL sys.expire_changelogs(`table` = 'default.T', older_than = '2024-01-01 12:00:00', retain_min = 10) CALL sys.expire_changelogs(`table` = 'default.T', older_than = '2024-01-01 12:00:00', max_deletes = 10) CALL sys.expire_changelogs(`table` = 'default.T', delete_all = true)    expire_partitions  CALL [catalog.]sys.expire_partitions(table, expiration_time, timestamp_formatter, expire_strategy)   To expire partitions. Argument: table: the target table identifier. Cannot be empty. expiration_time: the expiration interval of a partition. A partition will be expired if it‘s lifetime is over this value. Partition time is extracted from the partition value. timestamp_formatter: the formatter to format timestamp from string. timestamp_pattern: the pattern to get a timestamp from partitions. expire_strategy: specifies the expiration strategy for partition expiration, possible values: 'values-time' or 'update-time' , 'values-time' as default. max_expires: The maximum of limited expired partitions, it is optional.   -- for Flink 1.18 CALL sys.expire_partitions('default.T', '1 d', 'yyyy-MM-dd', '$dt', 'values-time') -- for Flink 1.19 and later CALL sys.expire_partitions(`table` = 'default.T', expiration_time = '1 d', timestamp_formatter = 'yyyy-MM-dd', expire_strategy = 'values-time') CALL sys.expire_partitions(`table` = 'default.T', expiration_time = '1 d', timestamp_formatter = 'yyyy-MM-dd HH:mm', timestamp_pattern = '$dt $hm', expire_strategy = 'values-time')    repair  -- repair all databases and tables in catalog CALL [catalog.]sys.repair() -- repair all tables in a specific database CALL [catalog.]sys.repair('databaseName') -- repair a table CALL [catalog.]sys.repair('databaseName.tableName') -- repair database and table in a string if you specify multiple tags, delimiter is ',' CALL [catalog.]sys.repair('databaseName01,database02.tableName01,database03')   Synchronize information from the file system to Metastore. Argument: empty: all databases and tables in catalog. databaseName : the target database name. tableName: the target table identifier.  CALL sys.repair(`table` = 'test_db.T')   rewrite_file_index  -- Use named argument CALL [catalog.]sys.rewrite_file_index(\u0026lt`table` = identifier\u0026gt [, \u0026ltpartitions = partitions\u0026gt]) -- Use indexed argument CALL [catalog.]sys.rewrite_file_index(\u0026ltidentifier\u0026gt [, \u0026ltpartitions\u0026gt])   Rewrite the file index for the table. Argument: table: \u0026ltdatabaseName\u0026gt.\u0026lttableName\u0026gt. partitions : specific partitions.   -- rewrite the file index for the whole table CALL sys.rewrite_file_index(`table` = 'test_db.T') -- repair all tables in a specific partition CALL sys.rewrite_file_index(`table` = 'test_db.T', partitions = 'pt=a')   create_branch  -- Use named argument CALL [catalog.]sys.create_branch(`table` = 'identifier', branch = 'branchName', tag = 'tagName') -- Use indexed argument -- based on the specified tag  CALL [catalog.]sys.create_branch('identifier', 'branchName', 'tagName') -- based on the specified branch's tag  CALL [catalog.]sys.create_branch('branch_table', 'branchName', 'tagName') -- create empty branch  CALL [catalog.]sys.create_branch('identifier', 'branchName')   To create a branch based on given tag, or just create empty branch. Arguments: table: the target table identifier or branch identifier. Cannot be empty. branchName: name of the new branch. tagName: name of the tag which the new branch is based on.   CALL sys.create_branch(`table` = 'default.T', branch = 'branch1', tag = 'tag1') -- based on the specified branch's tag  CALL sys.create_branch(`table` = 'default.T$branch_existBranchName', branch = 'branch1', tag = 'tag1') CALL sys.create_branch(`table` = 'default.T', branch = 'branch1')    delete_branch  -- Use named argument CALL [catalog.]sys.delete_branch(`table` = 'identifier', branch = 'branchName') -- Use indexed argument CALL [catalog.]sys.delete_branch('identifier', 'branchName')   To delete a branch. Arguments: table: the target table identifier. Cannot be empty. branchName: name of the branch to be deleted. If you specify multiple branches, delimiter is ','.   CALL sys.delete_branch(`table` = 'default.T', branch = 'branch1')    fast_forward  -- Use named argument CALL [catalog.]sys.fast_forward(`table` = 'identifier', branch = 'branchName') -- Use indexed argument CALL [catalog.]sys.fast_forward('identifier', 'branchName')   To fast_forward a branch to main branch. Arguments: table: the target table identifier. Cannot be empty. branchName: name of the branch to be merged.   CALL sys.fast_forward(`table` = 'default.T', branch = 'branch1')    refresh_object_table  CALL [catalog.]sys.refresh_object_table('identifier')   To refresh_object_table a object table. Arguments: table: the target table identifier. Cannot be empty.   CALL sys.refresh_object_table('default.T')    compact_manifest  CALL [catalog.]sys.compact_manifest(`table` = 'identifier')   To compact_manifest the manifests. Arguments: table: the target table identifier. Cannot be empty.   CALL sys.compact_manifest(`table` = 'default.T')    rescale  CALL [catalog.]sys.rescale(`table` = 'identifier', `bucket_num` = bucket_num, `partition` = 'partition', `scan_parallelism` = 'scan_parallelism', `sink_parallelism` = 'sink_parallelism')   Rescale one partition of a table. Arguments: table: The target table identifier. Cannot be empty. bucket_num: Resulting bucket number after rescale. The default value of argument bucket_num is the current bucket number of the table. Cannot be empty for postpone bucket tables. partition: What partition to rescale. For partitioned table this argument cannot be empty. scan_parallelism: Parallelism of source operator. The default value is the current bucket number of the partition. sink_parallelism: Parallelism of sink operator. The default value is equal to bucket_num.   CALL sys.rescale(`table` = 'default.T', `bucket_num` = 16, `partition` = 'dt=20250217,hh=08')    alter_view_dialect  -- add dialect in the view CALL [catalog.]sys.alter_view_dialect('view_identifier', 'add', 'flink', 'query') CALL [catalog.]sys.alter_view_dialect(`view` = 'view_identifier', `action` = 'add', `query` = 'query') -- update dialect in the view CALL [catalog.]sys.alter_view_dialect('view_identifier', 'update', 'flink', 'query') CALL [catalog.]sys.alter_view_dialect(`view` = 'view_identifier', `action` = 'update', `query` = 'query') -- drop dialect in the view CALL [catalog.]sys.alter_view_dialect('view_identifier', 'drop', 'flink') CALL [catalog.]sys.alter_view_dialect(`view` = 'view_identifier', `action` = 'drop')   To alter view dialect. Arguments: view: the target view identifier. Cannot be empty. action: define change action like: add, update, drop. Cannot be empty. engine: when engine which is not flink need define it. query: query for the dialect when action is add and update it couldn't be empty.   -- add dialect in the view CALL sys.alter_view_dialect('view_identifier', 'add', 'flink', 'query') CALL sys.alter_view_dialect(`view` = 'view_identifier', `action` = 'add', `query` = 'query') -- update dialect in the view CALL sys.alter_view_dialect('view_identifier', 'update', 'flink', 'query') CALL sys.alter_view_dialect(`view` = 'view_identifier', `action` = 'update', `query` = 'query') -- drop dialect in the view CALL sys.alter_view_dialect('view_identifier', 'drop', 'flink') CALL sys.alter_view_dialect(`view` = 'view_identifier', `action` = 'drop')     "});index.add({'id':98,'href':'/docs/1.1/flink/action-jars/','title':"Action Jars",'section':"Engine Flink",'content':"Action Jars #  After the Flink Local Cluster has been started, you can execute the action jar by using the following command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  \u0026lt;action\u0026gt; \u0026lt;args\u0026gt; The following command is used to compact a table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  compact \\  --path \u0026lt;TABLE_PATH\u0026gt; Merging into table #  Paimon supports \u0026ldquo;MERGE INTO\u0026rdquo; via submitting the \u0026lsquo;merge_into\u0026rsquo; job through flink run.\nImportant table properties setting:\n Only primary key table supports this feature. The action won\u0026rsquo;t produce UPDATE_BEFORE, so it\u0026rsquo;s not recommended to set \u0026lsquo;changelog-producer\u0026rsquo; = \u0026lsquo;input\u0026rsquo;.   The design referenced such syntax:\nMERGE INTO target-table USING source_table | source-expr AS source-alias ON merge-condition WHEN MATCHED [AND matched-condition] THEN UPDATE SET xxx WHEN MATCHED [AND matched-condition] THEN DELETE WHEN NOT MATCHED [AND not_matched_condition] THEN INSERT VALUES (xxx) WHEN NOT MATCHED BY SOURCE [AND not-matched-by-source-condition] THEN UPDATE SET xxx WHEN NOT MATCHED BY SOURCE [AND not-matched-by-source-condition] THEN DELETE The merge_into action use \u0026ldquo;upsert\u0026rdquo; semantics instead of \u0026ldquo;update\u0026rdquo;, which means if the row exists, then do update, else do insert. For example, for non-primary-key table, you can update every column, but for primary key table, if you want to update primary keys, you have to insert a new row which has different primary keys from rows in the table. In this scenario, \u0026ldquo;upsert\u0026rdquo; is useful.\nRun the following command to submit a \u0026lsquo;merge_into\u0026rsquo; job for the table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  merge_into \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;target-table\u0026gt; \\  [--target_as \u0026lt;target-table-alias\u0026gt;] \\  --source_table \u0026lt;source_table-name\u0026gt; \\  [--source_sql \u0026lt;sql\u0026gt; ...]\\  --on \u0026lt;merge-condition\u0026gt; \\  --merge_actions \u0026lt;matched-upsert,matched-delete,not-matched-insert,not-matched-by-source-upsert,not-matched-by-source-delete\u0026gt; \\  --matched_upsert_condition \u0026lt;matched-condition\u0026gt; \\  --matched_upsert_set \u0026lt;upsert-changes\u0026gt; \\  --matched_delete_condition \u0026lt;matched-condition\u0026gt; \\  --not_matched_insert_condition \u0026lt;not-matched-condition\u0026gt; \\  --not_matched_insert_values \u0026lt;insert-values\u0026gt; \\  --not_matched_by_source_upsert_condition \u0026lt;not-matched-by-source-condition\u0026gt; \\  --not_matched_by_source_upsert_set \u0026lt;not-matched-upsert-changes\u0026gt; \\  --not_matched_by_source_delete_condition \u0026lt;not-matched-by-source-condition\u0026gt; \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] You can pass sqls by \u0026#39;--source_sql \u0026lt;sql\u0026gt; [, --source_sql \u0026lt;sql\u0026gt; ...]\u0026#39; to config environment and create source table at runtime. -- Examples: -- Find all orders mentioned in the source table, then mark as important if the price is above 100 -- or delete if the price is under 10. ./flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  merge_into \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table T \\  --source_table S \\  --on \u0026#34;T.id = S.order_id\u0026#34; \\  --merge_actions \\  matched-upsert,matched-delete \\  --matched_upsert_condition \u0026#34;T.price \u0026gt; 100\u0026#34; \\  --matched_upsert_set \u0026#34;mark = \u0026#39;important\u0026#39;\u0026#34; \\  --matched_delete_condition \u0026#34;T.price \u0026lt; 10\u0026#34; -- For matched order rows, increase the price, and if there is no match, insert the order from the -- source table: ./flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  merge_into \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table T \\  --source_table S \\  --on \u0026#34;T.id = S.order_id\u0026#34; \\  --merge_actions \\  matched-upsert,not-matched-insert \\  --matched_upsert_set \u0026#34;price = T.price + 20\u0026#34; \\  --not_matched_insert_values * -- For not matched by source order rows (which are in the target table and does not match any row in the -- source table based on the merge-condition), decrease the price or if the mark is \u0026#39;trivial\u0026#39;, delete them: ./flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  merge_into \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table T \\  --source_table S \\  --on \u0026#34;T.id = S.order_id\u0026#34; \\  --merge_actions \\  not-matched-by-source-upsert,not-matched-by-source-delete \\  --not_matched_by_source_upsert_condition \u0026#34;T.mark \u0026lt;\u0026gt; \u0026#39;trivial\u0026#39;\u0026#34; \\  --not_matched_by_source_upsert_set \u0026#34;price = T.price - 20\u0026#34; \\  --not_matched_by_source_delete_condition \u0026#34;T.mark = \u0026#39;trivial\u0026#39;\u0026#34; -- A --source_sql example: -- Create a temporary view S in new catalog and use it as source table ./flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  merge_into \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table T \\  --source_sql \u0026#34;CREATE CATALOG test_cat WITH (...)\u0026#34; \\  --source_sql \u0026#34;CREATE TEMPORARY VIEW test_cat.`default`.S AS SELECT order_id, price, \u0026#39;important\u0026#39; FROM important_order\u0026#34; \\  --source_table test_cat.default.S \\  --on \u0026#34;T.id = S.order_id\u0026#34; \\  --merge_actions not-matched-insert\\  --not_matched_insert_values * The term \u0026lsquo;matched\u0026rsquo; explanation:\n matched: changed rows are from target table and each can match a source table row based on merge-condition and optional matched-condition (source ∩ target). not matched: changed rows are from source table and all rows cannot match any target table row based on merge-condition and optional not_matched_condition (source - target). not matched by source: changed rows are from target table and all row cannot match any source table row based on merge-condition and optional not-matched-by-source-condition (target - source).  Parameters format:\n matched_upsert_changes:\ncol = \u0026lt;source_table\u0026gt;.col | expression [, \u0026hellip;] (Means setting \u0026lt;target_table\u0026gt;.col with given value. Do not add \u0026lsquo;\u0026lt;target_table\u0026gt;.\u0026rsquo; before \u0026lsquo;col\u0026rsquo;.)\nEspecially, you can use \u0026lsquo;*\u0026rsquo; to set columns with all source columns (require target table\u0026rsquo;s schema is equal to source\u0026rsquo;s). not_matched_upsert_changes is similar to matched_upsert_changes, but you cannot reference source table\u0026rsquo;s column or use \u0026lsquo;*\u0026rsquo;. insert_values:\ncol1, col2, \u0026hellip;, col_end\nMust specify values of all columns. For each column, you can reference \u0026lt;source_table\u0026gt;.col or use an expression.\nEspecially, you can use \u0026lsquo;*\u0026rsquo; to insert with all source columns (require target table\u0026rsquo;s schema is equal to source\u0026rsquo;s). not_matched_condition cannot use target table\u0026rsquo;s columns to construct condition expression. not_matched_by_source_condition cannot use source table\u0026rsquo;s columns to construct condition expression.   Target alias cannot be duplicated with existed table name. If the source table is not in the current catalog and current database, the source-table-name must be qualified (database.table or catalog.database.table if created a new catalog). For examples:\n(1) If source table \u0026lsquo;my_source\u0026rsquo; is in \u0026lsquo;my_db\u0026rsquo;, qualify it:\n--source_table \u0026ldquo;my_db.my_source\u0026rdquo;\n(2) Example for sqls:\nWhen sqls changed current catalog and database, it\u0026rsquo;s OK to not qualify the source table name:\n--source_sql \u0026ldquo;CREATE CATALOG my_cat WITH (\u0026hellip;)\u0026quot;\n--source_sql \u0026ldquo;USE CATALOG my_cat\u0026rdquo;\n--source_sql \u0026ldquo;CREATE DATABASE my_db\u0026rdquo;\n--source_sql \u0026ldquo;USE my_db\u0026rdquo;\n--source_sql \u0026ldquo;CREATE TABLE S \u0026hellip;\u0026quot;\n--source_table S\nbut you must qualify it in the following case:\n--source_sql \u0026ldquo;CREATE CATALOG my_cat WITH (\u0026hellip;)\u0026quot;\n--source_sql \u0026ldquo;CREATE TABLE my_cat.`default`.S \u0026hellip;\u0026quot;\n--source_table my_cat.default.S\nYou can use just \u0026lsquo;S\u0026rsquo; as source table name in following arguments. At least one merge action must be specified. If both matched-upsert and matched-delete actions are present, their conditions must both be present too (same to not-matched-by-source-upsert and not-matched-by-source-delete). Otherwise, all conditions are optional. All conditions, set changes and values should use Flink SQL syntax. To ensure the whole command runs normally in Shell, please quote them with \u0026quot;\u0026quot; to escape blank spaces and use \u0026lsquo;\\\u0026rsquo; to escape special characters in statement. For example:\n--source_sql \u0026ldquo;CREATE TABLE T (k INT) WITH (\u0026lsquo;special-key\u0026rsquo; = \u0026lsquo;123\\!')\u0026rdquo;   For more information of \u0026lsquo;merge_into\u0026rsquo;, see\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  merge_into --help Deleting from table #  In Flink 1.16 and previous versions, Paimon only supports deleting records via submitting the \u0026lsquo;delete\u0026rsquo; job through flink run.\nRun the following command to submit a \u0026lsquo;delete\u0026rsquo; job for the table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  delete \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  --where \u0026lt;filter_spec\u0026gt; \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] filter_spec is equal to the \u0026#39;WHERE\u0026#39; clause in SQL DELETE statement. Examples: age \u0026gt;= 18 AND age \u0026lt;= 60 animal \u0026lt;\u0026gt; \u0026#39;cat\u0026#39; id \u0026gt; (SELECT count(*) FROM employee) For more information of \u0026lsquo;delete\u0026rsquo;, see\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  delete --help Drop Partition #  Run the following command to submit a \u0026lsquo;drop_partition\u0026rsquo; job for the table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  drop_partition \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  [--partition \u0026lt;partition_spec\u0026gt; [--partition \u0026lt;partition_spec\u0026gt; ...]] \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] partition_spec: key1=value1,key2=value2... For more information of \u0026lsquo;drop_partition\u0026rsquo;, see\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  drop_partition --help Rewrite File Index #  Run the following command to submit a \u0026lsquo;rewrite_file_index\u0026rsquo; job for the table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  rewrite_file_index \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --identifier \u0026lt;database.table\u0026gt; \\  [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog_conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] For more information of \u0026lsquo;rewrite_file_index\u0026rsquo;, see\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-1.1.1.jar \\  rewrite_file_index --help "});index.add({'id':99,'href':'/docs/1.1/project/','title':"Project",'section':"Apache Paimon",'content':""});index.add({'id':100,'href':'/docs/1.1/learn-paimon/','title':"Learn Paimon",'section':"Apache Paimon",'content':""});index.add({'id':101,'href':'/docs/1.1/spark/procedures/','title':"Procedures",'section':"Engine Spark",'content':"Procedures #  This section introduce all available spark procedures about paimon.\n  Procedure Name Explanation Example    compact  To compact files. Argument: table: the target table identifier. Cannot be empty. partitions: partition filter. the comma (\",\") represents \"AND\", the semicolon (\";\") represents \"OR\". If you want to compact one partition with date=01 and day=01, you need to write 'date=01,day=01'. Left empty for all partitions. (Can't be used together with \"where\") where: partition predicate. Left empty for all partitions. (Can't be used together with \"partitions\") order_strategy: 'order' or 'zorder' or 'hilbert' or 'none'. Left empty for 'none'. order_columns: the columns need to be sort. Left empty if 'order_strategy' is 'none'. partition_idle_time: this is used to do a full compaction for partition which had not received any new data for 'partition_idle_time'. And only these partitions will be compacted. This argument can not be used with order compact. compact_strategy: this determines how to pick files to be merged, the default is determined by the runtime execution mode. 'full' strategy only supports batch mode. All files will be selected for merging. 'minor' strategy: Pick the set of files that need to be merged based on specified conditions.   SET spark.sql.shuffle.partitions=10; --set the compact parallelism  CALL sys.compact(table = 'T', partitions = 'p=0;p=1', order_strategy = 'zorder', order_by = 'a,b')  CALL sys.compact(table = 'T', where = 'p0 and p'zorder', order_by = 'a,b')  CALL sys.compact(table = 'T', partition_idle_time = '60s') CALL sys.compact(table = 'T', compact_strategy = 'minor')    expire_snapshots  To expire snapshots. Argument: table: the target table identifier. Cannot be empty. retain_max: the maximum number of completed snapshots to retain. retain_min: the minimum number of completed snapshots to retain. older_than: timestamp before which snapshots will be removed. max_deletes: the maximum number of snapshots that can be deleted at once.  CALL sys.expire_snapshots(table = 'default.T', retain_max = 10)   expire_partitions  To expire partitions. Argument: table: the target table identifier. Cannot be empty. expiration_time: the expiration interval of a partition. A partition will be expired if it‘s lifetime is over this value. Partition time is extracted from the partition value. timestamp_formatter: the formatter to format timestamp from string. timestamp_pattern: the pattern to get a timestamp from partitions. expire_strategy: specifies the expiration strategy for partition expiration, possible values: 'values-time' or 'update-time' , 'values-time' as default. max_expires: The maximum of limited expired partitions, it is optional.  CALL sys.expire_partitions(table = 'default.T', expiration_time = '1 d', timestamp_formatter = 'yyyy-MM-dd', timestamp_pattern = '$dt', expire_strategy = 'values-time')   create_tag  To create a tag based on given snapshot. Arguments: table: the target table identifier. Cannot be empty. tag: name of the new tag. Cannot be empty. snapshot(Long): id of the snapshot which the new tag is based on. time_retained: The maximum time retained for newly created tags.   -- based on snapshot 10 with 1d  CALL sys.create_tag(table = 'default.T', tag = 'my_tag', snapshot = 10, time_retained = '1 d')  -- based on the latest snapshot  CALL sys.create_tag(table = 'default.T', tag = 'my_tag')    create_tag_from_timestamp  To create a tag based on given timestamp. Arguments: table: the target table identifier. Cannot be empty. tag: name of the new tag. timestamp (Long): Find the first snapshot whose commit-time is greater than this timestamp. time_retained : The maximum time retained for newly created tags.   CALL sys.create_tag_from_timestamp(`table` = 'default.T', `tag` = 'my_tag', `timestamp` = 1724404318750, time_retained = '1 d')    rename_tag  Rename a tag with a new tag name. Arguments: table: the target table identifier. Cannot be empty. tag: name of the tag. Cannot be empty. target_tag: the new tag name to rename. Cannot be empty.   CALL sys.rename_tag(table = 'default.T', tag = 'tag1', target_tag = 'tag2')    replace_tag  Replace an existing tag with new tag info. Arguments: table: the target table identifier. Cannot be empty. tag: name of the existed tag. Cannot be empty. snapshot(Long): id of the snapshot which the tag is based on, it is optional. time_retained: The maximum time retained for the existing tag, it is optional.   CALL sys.replace_tag(table = 'default.T', tag_name = 'tag1', snapshot = 10, time_retained = '1 d')    delete_tag  To delete a tag. Arguments: table: the target table identifier. Cannot be empty. tag: name of the tag to be deleted. If you specify multiple tags, delimiter is ','.  CALL sys.delete_tag(table = 'default.T', tag = 'my_tag')   expire_tags  To expire tags by time. Arguments: table: the target table identifier. Cannot be empty. older_than: tagCreateTime before which tags will be removed.   CALL sys.expire_tags(table = 'default.T', older_than = '2024-09-06 11:00:00')    rollback  To rollback to a specific version of target table, note version/snapshot/tag must set one of them. Argument: table: the target table identifier. Cannot be empty. version: id of the snapshot or name of tag that will roll back to, version would be Deprecated. snapshot: snapshot that will roll back to. tag: tag that will roll back to.   CALL sys.rollback(table = 'default.T', version = 'my_tag') CALL sys.rollback(table = 'default.T', version = 10) CALL sys.rollback(table = 'default.T', tag = 'tag1') CALL sys.rollback(table = 'default.T', snapshot = 2)    rollback_to_timestamp  To rollback to the snapshot which earlier or equal than timestamp. Argument: table: the target table identifier. Cannot be empty. timestamp: roll back to the snapshot which earlier or equal than timestamp.   CALL sys.rollback_to_timestamp(table = 'default.T', timestamp = 1730292023000)    rollback_to_watermark  To rollback to the snapshot which earlier or equal than watermark. Argument: table: the target table identifier. Cannot be empty. watermark: roll back to the snapshot which earlier or equal than watermark.   CALL sys.rollback_to_watermark(table = 'default.T', watermark = 1730292023000)    purge_files  To clear table with purge files. Argument: table: the target table identifier. Cannot be empty.   CALL sys.purge_files(table = 'default.T')    migrate_database  Migrate all hive tables in database to paimon tables. Arguments: source_type: the origin database's type to be migrated, such as hive. Cannot be empty. database: name of the origin database to be migrated. Cannot be empty. options: the table options of the paimon table to migrate. options_map: Options map for adding key-value options which is a map. parallelism: the parallelism for migrate process, default is core numbers of machine.  CALL sys.migrate_database(source_type = 'hive', database = 'db01', options = 'file.format=parquet', options_map = map('k1','v1'), parallelism = 6)   migrate_table  Migrate hive table to a paimon table. Arguments: source_type: the origin table's type to be migrated, such as hive. Cannot be empty. table: name of the origin table to be migrated. Cannot be empty. options: the table options of the paimon table to migrate. target_table: name of the target paimon table to migrate. If not set would keep the same name with origin table delete_origin: If had set target_table, can set delete_origin to decide whether delete the origin table metadata from hms after migrate. Default is true options_map: Options map for adding key-value options which is a map. parallelism: the parallelism for migrate process, default is core numbers of machine.  CALL sys.migrate_table(source_type = 'hive', table = 'default.T', options = 'file.format=parquet', options_map = map('k1','v1'), parallelism = 6)   remove_orphan_files  To remove the orphan data files and metadata files. Arguments: table: the target table identifier. Cannot be empty, you can use database_name.* to clean whole database. older_than: to avoid deleting newly written files, this procedure only deletes orphan files older than 1 day by default. This argument can modify the interval. dry_run: when true, view only orphan files, don't actually remove files. Default is false. parallelism: The maximum number of concurrent deleting files. By default is the number of processors available to the Java virtual machine. mode: The mode of remove orphan clean procedure (local or distributed) . By default is distributed.   CALL sys.remove_orphan_files(table = 'default.T', older_than = '2023-10-31 12:00:00') CALL sys.remove_orphan_files(table = 'default.*', older_than = '2023-10-31 12:00:00') CALL sys.remove_orphan_files(table = 'default.T', older_than = '2023-10-31 12:00:00', dry_run = true) CALL sys.remove_orphan_files(table = 'default.T', older_than = '2023-10-31 12:00:00', dry_run = true, parallelism = '5') CALL sys.remove_orphan_files(table = 'default.T', older_than = '2023-10-31 12:00:00', dry_run = true, parallelism = '5', mode = 'local')    remove_unexisting_files  Procedure to remove unexisting data files from manifest entries. See Java docs for detailed use cases. Arguments: table: the target table identifier. Cannot be empty, you can use database_name.* to clean whole database. dry_run (optional): only check what files will be removed, but not really remove them. Default is false. parallelism (optional): number of parallelisms to check files in the manifests. Note that user is on his own risk using this procedure, which may cause data loss when used outside from the use cases listed in Java docs.   -- remove unexisting data files in the table `mydb.myt` CALL sys.remove_unexisting_files(table = 'mydb.myt') -- only check what files will be removed, but not really remove them (dry run) CALL sys.remove_unexisting_files(table = 'mydb.myt', dry_run = true)    repair  Synchronize information from the file system to Metastore. Argument: database_or_table: empty or the target database name or the target table identifier, if you specify multiple tags, delimiter is ','   CALL sys.repair('test_db.T') CALL sys.repair('test_db.T,test_db01,test_db.T2')    create_branch  To merge a branch to main branch. Arguments: table: the target table identifier or branch identifier. Cannot be empty. branch: name of the branch to be merged. tag: name of the new tag. Cannot be empty.   CALL sys.create_branch(table = 'test_db.T', branch = 'test_branch') CALL sys.create_branch(table = 'test_db.T', branch = 'test_branch', tag = 'my_tag') CALL sys.create_branch(table = 'test_db.T$branch_existBranchName', branch = 'test_branch', tag = 'my_tag')    delete_branch  To merge a branch to main branch. Arguments: table: the target table identifier. Cannot be empty. branch: name of the branch to be merged. If you specify multiple branches, delimiter is ','.   CALL sys.delete_branch(table = 'test_db.T', branch = 'test_branch')    fast_forward  To fast_forward a branch to main branch. Arguments: table: the target table identifier. Cannot be empty. branch: name of the branch to be merged.   CALL sys.fast_forward(table = 'test_db.T', branch = 'test_branch')    reset_consumer  To reset or delete consumer. Arguments: table: the target table identifier. Cannot be empty. consumerId: consumer to be reset or deleted. nextSnapshotId (Long): the new next snapshot id of the consumer.   -- reset the new next snapshot id in the consumer CALL sys.reset_consumer(table = 'default.T', consumerId = 'myid', nextSnapshotId = 10) -- delete consumer CALL sys.reset_consumer(table = 'default.T', consumerId = 'myid')    clear_consumers  To clear consumers. Arguments: table: the target table identifier. Cannot be empty. includingConsumers: consumers to be cleared. excludingConsumers: consumers which not to be cleared.   -- clear all consumers in the table CALL sys.clear_consumers(table = 'default.T') -- clear some consumers in the table (accept regular expression) CALL sys.clear_consumers(table = 'default.T', includingConsumers = 'myid.*') -- clear all consumers except excludingConsumers in the table (accept regular expression) CALL sys.clear_consumers(table = 'default.T', includingConsumers = '', excludingConsumers = 'myid1.*') -- clear all consumers with includingConsumers and excludingConsumers (accept regular expression) CALL sys.clear_consumers(table = 'default.T', includingConsumers = 'myid.*', excludingConsumers = 'myid1.*')    mark_partition_done  To mark partition to be done. Arguments: table: the target table identifier. Cannot be empty. partitions: partitions need to be mark done, If you specify multiple partitions, delimiter is ';'.   -- mark single partition done CALL sys.mark_partition_done(table = 'default.T', parititions = 'day=2024-07-01') -- mark multiple partitions done CALL sys.mark_partition_done(table = 'default.T', parititions = 'day=2024-07-01;day=2024-07-02')    refresh_object_table  To refresh_object_table a object table. Arguments: table: the target table identifier. Cannot be empty.   CALL sys.refresh_object_table('default.T')    compact_manifest  To compact_manifest the manifests. Arguments: table: the target table identifier. Cannot be empty.   CALL sys.compact_manifest(`table` = 'default.T')    alter_view_dialect  To alter view dialect. Arguments: view: the target view identifier. Cannot be empty. action: define change action like: add, update, drop. Cannot be empty. engine: when engine which is not spark need define it. query: query for the dialect when action is add and update it couldn't be empty.   -- add dialect in the view CALL sys.alter_view_dialect('view_identifier', 'add', 'spark', 'query') CALL sys.alter_view_dialect(`view` = 'view_identifier', `action` = 'add', `query` = 'query') -- update dialect in the view CALL sys.alter_view_dialect('view_identifier', 'update', 'spark', 'query') CALL sys.alter_view_dialect(`view` = 'view_identifier', `action` = 'update', `query` = 'query') -- drop dialect in the view CALL sys.alter_view_dialect('view_identifier', 'drop', 'spark') CALL sys.alter_view_dialect(`view` = 'view_identifier', `action` = 'drop')     "});index.add({'id':102,'href':'/docs/1.1/flink/savepoint/','title':"Savepoint",'section':"Engine Flink",'content':"Savepoint #  Paimon has its own snapshot management, this may conflict with Flink\u0026rsquo;s checkpoint management, causing exceptions when restoring from savepoint (don\u0026rsquo;t worry, it will not cause the storage to be damaged).\nIt is recommended that you use the following methods to savepoint:\n Use Flink Stop with savepoint. Use Paimon Tag with Flink Savepoint, and rollback-to-tag before restoring from savepoint.  Stop with savepoint #  This feature of Flink ensures that the last checkpoint is fully processed, which means there will be no more uncommitted metadata left. This is very safe, so we recommend using this feature to stop and start job.\nTag with Savepoint #  In Flink, we may consume from Kafka and then write to Paimon. Since Flink\u0026rsquo;s checkpoint only retains a limited number, we will trigger a savepoint at certain time (such as code upgrades, data updates, etc.) to ensure that the state can be retained for a longer time, so that the job can be restored incrementally.\nPaimon\u0026rsquo;s snapshot is similar to Flink\u0026rsquo;s checkpoint, and both will automatically expire, but the tag feature of Paimon allows snapshots to be retained for a long time. Therefore, we can combine the two features of Paimon\u0026rsquo;s tag and Flink\u0026rsquo;s savepoint to achieve incremental recovery of job from the specified savepoint.\nStarting from Flink 1.15 intermediate savepoints (savepoints other than created with stop-with-savepoint) are not used for recovery and do not commit any side effects.\nFor savepoint created with stop-with-savepoint, tags will be created automatically. For other savepoints, tags will be created after the next checkpoint succeeds.\n Step 1: Enable automatically create tags for savepoint.\nYou can set sink.savepoint.auto-tag to true to enable the feature of automatically creating tags for savepoint.\nStep 2: Trigger savepoint.\nYou can refer to Flink savepoint to learn how to configure and trigger savepoint.\nStep 3: Choose the tag corresponding to the savepoint.\nThe tag corresponding to the savepoint will be named in the form of savepoint-${savepointID}. You can refer to Tags Table to query.\nStep 4: Rollback the paimon table.\nRollback the Paimon table to the specified tag.\nStep 5: Restart from the savepoint.\nYou can refer to here to learn how to restart from a specified savepoint.\n"});index.add({'id':103,'href':'/docs/1.1/maintenance/configurations/','title':"Configurations",'section':"Maintenance",'content':"Configuration #  CoreOptions #  Core options for paimon.\n  Key Default Type Description     aggregation.remove-record-on-delete false Boolean Whether to remove the whole row in aggregation engine when -D records are received.   async-file-write true Boolean Whether to enable asynchronous IO writing when writing files.   auto-create false Boolean Whether to create underlying storage when reading and writing the table.   bucket -1 Integer Bucket number for file store.\nIt should either be equal to -1 (dynamic bucket mode), -2 (postpone bucket mode), or it must be greater than 0 (fixed bucket mode).   bucket-key (none) String Specify the paimon distribution policy. Data is assigned to each bucket according to the hash value of bucket-key.\nIf you specify multiple fields, delimiter is ','.\nIf not specified, the primary key will be used; if there is no primary key, the full row will be used.   cache-page-size 64 kb MemorySize Memory page size for caching.   changelog-file.compression (none) String Changelog file compression.   changelog-file.format (none) String Specify the message format of changelog files, currently parquet, avro and orc are supported.   changelog-file.prefix \"changelog-\" String Specify the file name prefix of changelog files.   changelog-file.stats-mode (none) String Changelog file metadata stats collection. none, counts, truncate(16), full is available.   changelog-producer none Enum\n Whether to double write to a changelog file. This changelog file keeps the details of data changes, it can be read directly during stream reads. This can be applied to tables with primary keys. Possible values:\"none\": No changelog file.\"input\": Double write to a changelog file when flushing memory table, the changelog is from input.\"full-compaction\": Generate changelog files with each full compaction.\"lookup\": Generate changelog files through 'lookup' before committing the data writing.   changelog-producer.row-deduplicate false Boolean Whether to generate -U, +U changelog for the same record. This configuration is only valid for the changelog-producer is lookup or full-compaction.   changelog-producer.row-deduplicate-ignore-fields (none) String Fields that are ignored for comparison while generating -U, +U changelog for the same record. This configuration is only valid for the changelog-producer.row-deduplicate is true.   changelog.num-retained.max (none) Integer The maximum number of completed changelog to retain. Should be greater than or equal to the minimum number.   changelog.num-retained.min (none) Integer The minimum number of completed changelog to retain. Should be greater than or equal to 1.   changelog.time-retained (none) Duration The maximum time of completed changelog to retain.   commit.callback.#.param (none) String Parameter string for the constructor of class #. Callback class should parse the parameter by itself.   commit.callbacks (none) String A list of commit callback classes to be called after a successful commit. Class names are connected with comma (example: com.test.CallbackA,com.sample.CallbackB).   commit.force-compact false Boolean Whether to force a compaction before commit.   commit.force-create-snapshot false Boolean Whether to force create snapshot on commit.   commit.max-retries 10 Integer Maximum number of retries when commit failed.   commit.timeout (none) Duration Timeout duration of retry when commit failed.   commit.user-prefix (none) String Specifies the commit user prefix.   compaction.force-up-level-0 false Boolean If set to true, compaction strategy will always include all level 0 files in candidates.   compaction.max-size-amplification-percent 200 Integer The size amplification is defined as the amount (in percentage) of additional storage needed to store a single byte of data in the merge tree for changelog mode table.   compaction.min.file-num 5 Integer For file set [f_0,...,f_N], the minimum file number to trigger a compaction for append-only table.   compaction.optimization-interval (none) Duration Implying how often to perform an optimization compaction, this configuration is used to ensure the query timeliness of the read-optimized system table.   compaction.size-ratio 1 Integer Percentage flexibility while comparing sorted run size for changelog mode table. If the candidate sorted run(s) size is 1% smaller than the next sorted run's size, then include next sorted run into this candidate set.   consumer-id (none) String Consumer id for recording the offset of consumption in the storage.   consumer.expiration-time (none) Duration The expiration interval of consumer files. A consumer file will be expired if it's lifetime after last modification is over this value.   consumer.ignore-progress false Boolean Whether to ignore consumer progress for the newly started job.   consumer.mode exactly-once Enum\n Specify the consumer consistency mode for table.\nPossible values:\"exactly-once\": Readers consume data at snapshot granularity, and strictly ensure that the snapshot-id recorded in the consumer is the snapshot-id + 1 that all readers have exactly consumed.\"at-least-once\": Each reader consumes snapshots at a different rate, and the snapshot with the slowest consumption progress among all readers will be recorded in the consumer.   continuous.discovery-interval 10 s Duration The discovery interval of continuous reading.   cross-partition-upsert.bootstrap-parallelism 10 Integer The parallelism for bootstrap in a single task for cross partition upsert.   cross-partition-upsert.index-ttl (none) Duration The TTL in rocksdb index for cross partition upsert (primary keys not contain all partition fields), this can avoid maintaining too many indexes and lead to worse and worse performance, but please note that this may also cause data duplication.   data-file.external-paths (none) String The external paths where the data of this table will be written, multiple elements separated by commas.   data-file.external-paths.specific-fs (none) String The specific file system of the external path when data-file.external-paths.strategy is set to specific-fs, should be the prefix scheme of the external path, now supported are s3 and oss.   data-file.external-paths.strategy none Enum\n The strategy of selecting an external path when writing data.\nPossible values:\"none\": Do not choose any external storage, data will still be written to the default warehouse path.\"specific-fs\": Select a specific file system as the external path. Currently supported are S3 and OSS.\"round-robin\": When writing a new file, a path is chosen from data-file.external-paths in turn.   data-file.path-directory (none) String Specify the path directory of data files.   data-file.prefix \"data-\" String Specify the file name prefix of data files.   data-file.thin-mode false Boolean Enable data file thin mode to avoid duplicate columns storage.   delete-file.thread-num (none) Integer The maximum number of concurrent deleting files. By default is the number of processors available to the Java virtual machine.   delete.force-produce-changelog false Boolean Force produce changelog in delete sql, or you can use 'streaming-read-overwrite' to read changelog from overwrite commit.   deletion-vector.index-file.target-size 2 mb MemorySize The target size of deletion vector index file.   deletion-vectors.enabled false Boolean Whether to enable deletion vectors mode. In this mode, index files containing deletion vectors are generated when data is written, which marks the data for deletion. During read operations, by applying these index files, merging can be avoided.   dynamic-bucket.assigner-parallelism (none) Integer Parallelism of assigner operator for dynamic bucket mode, it is related to the number of initialized bucket, too small will lead to insufficient processing speed of assigner.   dynamic-bucket.initial-buckets (none) Integer Initial buckets for a partition in assigner operator for dynamic bucket mode.   dynamic-bucket.max-buckets -1 Integer Max buckets for a partition in dynamic bucket mode, It should either be equal to -1 (unlimited), or it must be greater than 0 (fixed upper bound).   dynamic-bucket.target-row-num 2000000 Long If the bucket is -1, for primary key table, is dynamic bucket mode, this option controls the target row number for one bucket.   dynamic-partition-overwrite true Boolean Whether only overwrite dynamic partition when overwriting a partitioned table with dynamic partition columns. Works only when the table has partition keys.   end-input.check-partition-expire false Boolean Optional endInput check partition expire used in case of batch mode or bounded stream.   fields.default-aggregate-function (none) String Default aggregate function of all fields for partial-update and aggregate merge function.   file-index.in-manifest-threshold 500 bytes MemorySize The threshold to store file index bytes in manifest.   file-index.read.enabled true Boolean Whether enabled read file index.   file-reader-async-threshold 10 mb MemorySize The threshold for read file async.   file.block-size (none) MemorySize File block size of format, default value of orc stripe is 64 MB, and parquet row group is 128 MB.   file.compression \"zstd\" String Default file compression. For faster read and write, it is recommended to use zstd.   file.compression.per.level  Map Define different compression policies for different level, you can add the conf like this: 'file.compression.per.level' = '0:lz4,1:zstd'.   file.compression.zstd-level 1 Integer Default file compression zstd level. For higher compression rates, it can be configured to 9, but the read and write speed will significantly decrease.   file.format \"parquet\" String Specify the message format of data files, currently orc, parquet and avro are supported.   file.format.per.level  Map Define different file format for different level, you can add the conf like this: 'file.format.per.level' = '0:avro,3:parquet', if the file format for level is not provided, the default format which set by `file.format` will be used.   file.suffix.include.compression false Boolean Whether to add file compression type in the file name of data file and changelog file.   force-lookup false Boolean Whether to force the use of lookup for compaction.   full-compaction.delta-commits (none) Integer Full compaction will be constantly triggered after delta commits.   ignore-delete false Boolean Whether to ignore delete records.   incremental-between (none) String Read incremental changes between start snapshot (exclusive) and end snapshot (inclusive), for example, '5,10' means changes between snapshot 5 and snapshot 10.   incremental-between-scan-mode auto Enum\n Scan kind when Read incremental changes between start snapshot (exclusive) and end snapshot (inclusive). Possible values:\"auto\": Scan changelog files for the table which produces changelog files. Otherwise, scan newly changed files.\"delta\": Scan newly changed files between snapshots.\"changelog\": Scan changelog files between snapshots.\"diff\": Get diff by comparing data of end snapshot with data of start snapshot.   incremental-between-timestamp (none) String Read incremental changes between start timestamp (exclusive) and end timestamp (inclusive), for example, 't1,t2' means changes between timestamp t1 and timestamp t2.   incremental-to-auto-tag (none) String Used to specify the end tag (inclusive), and Paimon will find an earlier tag and return changes between them. If the tag doesn't exist or the earlier tag doesn't exist, return empty.    local-merge-buffer-size (none) MemorySize Local merge will buffer and merge input records before they're shuffled by bucket and written into sink. The buffer will be flushed when it is full. Mainly to resolve data skew on primary keys. We recommend starting with 64 mb when trying out this feature.   local-sort.max-num-file-handles 128 Integer The maximal fan-in for external merge sort. It limits the number of file handles. If it is too small, may cause intermediate merging. But if it is too large, it will cause too many files opened at the same time, consume memory and lead to random reading.   lookup-compact RADICAL Enum\n Lookup compact mode used for lookup compaction.\nPossible values:\"RADICAL\"\"GENTLE\"   lookup-compact.max-interval (none) Integer The max interval for a gentle mode lookup compaction to be triggered. For every interval, a forced lookup compaction will be performed to flush L0 files to higher level. This option is only valid when lookup-compact mode is gentle.   lookup-wait true Boolean When need to lookup, commit will wait for compaction by lookup.   lookup.cache-file-retention 1 h Duration The cached files retention time for lookup. After the file expires, if there is a need for access, it will be re-read from the DFS to build an index on the local disk.   lookup.cache-max-disk-size infinite MemorySize Max disk size for lookup cache, you can use this option to limit the use of local disks.   lookup.cache-max-memory-size 256 mb MemorySize Max memory size for lookup cache.   lookup.cache-spill-compression \"zstd\" String Spill compression for lookup cache, currently zstd, none, lz4 and lzo are supported.   lookup.cache.bloom.filter.enabled true Boolean Whether to enable the bloom filter for lookup cache.   lookup.cache.bloom.filter.fpp 0.05 Double Define the default false positive probability for lookup cache bloom filters.   lookup.cache.high-priority-pool-ratio 0.25 Double The fraction of cache memory that is reserved for high-priority data like index, filter.   lookup.hash-load-factor 0.75 Float The index load factor for lookup.   lookup.local-file-type sort Enum\n The local file type for lookup.\nPossible values:\"sort\": Construct a sorted file for lookup.\"hash\": Construct a hash file for lookup.   manifest.compression \"zstd\" String Default file compression for manifest.   manifest.delete-file-drop-stats false Boolean For DELETE manifest entry in manifest file, drop stats to reduce memory and storage. Default value is false only for compatibility of old reader.   manifest.format \"avro\" String Specify the message format of manifest files.   manifest.full-compaction-threshold-size 16 mb MemorySize The size threshold for triggering full compaction of manifest.   manifest.merge-min-count 30 Integer To avoid frequent manifest merges, this parameter specifies the minimum number of ManifestFileMeta to merge.   manifest.target-file-size 8 mb MemorySize Suggested file size of a manifest file.   merge-engine deduplicate Enum\n Specify the merge engine for table with primary key.\nPossible values:\"deduplicate\": De-duplicate and keep the last row.\"partial-update\": Partial update non-null fields.\"aggregation\": Aggregate fields with same primary key.\"first-row\": De-duplicate and keep the first row.   metadata.stats-dense-store true Boolean Whether to store statistic densely in metadata (manifest files), which will significantly reduce the storage size of metadata when the none statistic mode is set.\nNote, when this mode is enabled with 'metadata.stats-mode:none', the Paimon sdk in reading engine requires at least version 0.9.1 or 1.0.0 or higher.   metadata.stats-mode \"truncate(16)\" String The mode of metadata stats collection. none, counts, truncate(16), full is available.\n\"none\": means disable the metadata stats collection.\"counts\" means only collect the null count.\"full\": means collect the null count, min/max value.\"truncate(16)\": means collect the null count, min/max value with truncated length of 16.Field level stats mode can be specified by fields.{field_name}.stats-mode   metadata.stats-mode.per.level  Map Define different 'metadata.stats-mode' for different level, you can add the conf like this: 'metadata.stats-mode.per.level' = '0:none', if the metadata.stats-mode for level is not provided, the default mode which set by `metadata.stats-mode` will be used.   metastore.partitioned-table false Boolean Whether to create this table as a partitioned table in metastore. For example, if you want to list all partitions of a Paimon table in Hive, you need to create this table as a partitioned table in Hive metastore. This config option does not affect the default filesystem metastore.   metastore.tag-to-partition (none) String Whether to create this table as a partitioned table for mapping non-partitioned table tags in metastore. This allows the Hive engine to view this table in a partitioned table view and use partitioning field to read specific partitions (specific tags).   metastore.tag-to-partition.preview none Enum\n Whether to preview tag of generated snapshots in metastore. This allows the Hive engine to query specific tag before creation.\nPossible values:\"none\": No automatically created tags.\"process-time\": Based on the time of the machine, create TAG once the processing time passes period time plus delay.\"watermark\": Based on the watermark of the input, create TAG once the watermark passes period time plus delay.\"batch\": In the batch processing scenario, the tag corresponding to the current snapshot is generated after the task is completed.   num-levels (none) Integer Total level number, for example, there are 3 levels, including 0,1,2 levels.   num-sorted-run.compaction-trigger 5 Integer The sorted run number to trigger compaction. Includes level0 files (one file one sorted run) and high-level runs (one level one sorted run).   num-sorted-run.stop-trigger (none) Integer The number of sorted runs that trigger the stopping of writes, the default value is 'num-sorted-run.compaction-trigger' + 3.   object-location (none) String The object location for object table.   page-size 64 kb MemorySize Memory page size.   parquet.enable.dictionary (none) Integer Turn off the dictionary encoding for all fields in parquet.   partial-update.remove-record-on-delete false Boolean Whether to remove the whole row in partial-update engine when -D records are received.   partial-update.remove-record-on-sequence-group (none) String When -D records of the given sequence groups are received, remove the whole row.   partition (none) String Define partition by table options, cannot define partition on DDL and table options at the same time.   partition.default-name \"__DEFAULT_PARTITION__\" String The default partition name in case the dynamic partition column value is null/empty string.   partition.end-input-to-done false Boolean Whether mark the done status to indicate that the data is ready when end input.   partition.expiration-check-interval 1 h Duration The check interval of partition expiration.   partition.expiration-max-num 100 Integer The default deleted num of partition expiration.   partition.expiration-strategy values-time Enum\n The strategy determines how to extract the partition time and compare it with the current time.\nPossible values:\"values-time\": This strategy compares the time extracted from the partition value with the current time.\"update-time\": This strategy compares the last update time of the partition with the current time.   partition.expiration-time (none) Duration The expiration interval of a partition. A partition will be expired if it‘s lifetime is over this value. Partition time is extracted from the partition value.   partition.idle-time-to-report-statistic 0 ms Duration Set a time duration when a partition has no new data after this time duration, start to report the partition statistics to hms.   partition.legacy-name true Boolean The legacy partition name is using `toString` fpr all types. If false, using cast to string for all types.   partition.mark-done-action \"success-file\" String Action to mark a partition done is to notify the downstream application that the partition has finished writing, the partition is ready to be read.\n1. 'success-file': add '_success' file to directory.\n2. 'done-partition': add 'xxx.done' partition to metastore.\n3. 'mark-event': mark partition event to metastore.\n4. 'http-report': report partition mark done to remote http server.\n5. 'custom': use policy class to create a mark-partition policy.\nBoth can be configured at the same time: 'done-partition,success-file,mark-event,custom'.   partition.mark-done-action.custom.class (none) String The partition mark done class for implement PartitionMarkDoneAction interface. Only work in custom mark-done-action.   partition.mark-done-action.http.params (none) String Http client request parameters will be written to the request body, this can only be used by http-report partition mark done action.   partition.mark-done-action.http.timeout 5 s Duration Http client connection timeout, this can only be used by http-report partition mark done action.   partition.mark-done-action.http.url (none) String Mark done action will reports the partition to the remote http server, this can only be used by http-report partition mark done action.   partition.sink-strategy NONE Enum\n This is only for partitioned unaware-buckets append table, and the purpose is to reduce small files and improve write performance. Through this repartitioning strategy to reduce the number of partitions written by each task to as few as possible.none: Rebalanced or Forward partitioning, this is the default behavior, this strategy is suitable for the number of partitions you write in a batch is much smaller than write parallelism.hash: Hash the partitions value, this strategy is suitable for the number of partitions you write in a batch is greater equals than write parallelism.\nPossible values:\"NONE\"\"HASH\"   partition.timestamp-formatter (none) String The formatter to format timestamp from string. It can be used with 'partition.timestamp-pattern' to create a formatter using the specified value.Default formatter is 'yyyy-MM-dd HH:mm:ss' and 'yyyy-MM-dd'.Supports multiple partition fields like '$year-$month-$day $hour:00:00'.The timestamp-formatter is compatible with Java's DateTimeFormatter.   partition.timestamp-pattern (none) String You can specify a pattern to get a timestamp from partitions. The formatter pattern is defined by 'partition.timestamp-formatter'.By default, read from the first field.If the timestamp in the partition is a single field called 'dt', you can use '$dt'.If it is spread across multiple fields for year, month, day, and hour, you can use '$year-$month-$day $hour:00:00'.If the timestamp is in fields dt and hour, you can use '$dt $hour:00:00'.   primary-key (none) String Define primary key by table options, cannot define primary key on DDL and table options at the same time.   read.batch-size 1024 Integer Read batch size for any file format if it supports.   record-level.expire-time (none) Duration Record level expire time for primary key table, expiration happens in compaction, there is no strong guarantee to expire records in time. You must specific 'record-level.time-field' too.   record-level.time-field (none) String Time field for record level expire. It supports the following types: `timestamps in seconds with INT`,`timestamps in seconds with BIGINT`, `timestamps in milliseconds with BIGINT` or `timestamp`.   rowkind.field (none) String The field that generates the row kind for primary key table, the row kind determines which data is '+I', '-U', '+U' or '-D'.   scan.bounded.watermark (none) Long End condition \"watermark\" for bounded streaming mode. Stream reading will end when a larger watermark snapshot is encountered.   scan.fallback-branch (none) String When a batch job queries from a table, if a partition does not exist in the current branch, the reader will try to get this partition from this fallback branch.   scan.file-creation-time-millis (none) Long After configuring this time, only the data files created after this time will be read. It is independent of snapshots, but it is imprecise filtering (depending on whether or not compaction occurs).   scan.manifest.parallelism (none) Integer The parallelism of scanning manifest files, default value is the size of cpu processor. Note: Scale-up this parameter will increase memory usage while scanning manifest files. We can consider downsize it when we encounter an out of memory exception while scanning   scan.max-splits-per-task 10 Integer Max split size should be cached for one task while scanning. If splits size cached in enumerator are greater than tasks size multiply by this value, scanner will pause scanning.   scan.mode default Enum\n Specify the scanning behavior of the source.\nPossible values:\"default\": Determines actual startup mode according to other table properties. If \"scan.timestamp-millis\" is set the actual startup mode will be \"from-timestamp\", and if \"scan.snapshot-id\" or \"scan.tag-name\" is set the actual startup mode will be \"from-snapshot\". Otherwise the actual startup mode will be \"latest-full\".\"latest-full\": For streaming sources, produces the latest snapshot on the table upon first startup, and continue to read the latest changes. For batch sources, just produce the latest snapshot but does not read new changes.\"full\": Deprecated. Same as \"latest-full\".\"latest\": For streaming sources, continuously reads latest changes without producing a snapshot at the beginning. For batch sources, behaves the same as the \"latest-full\" startup mode.\"compacted-full\": For streaming sources, produces a snapshot after the latest compaction on the table upon first startup, and continue to read the latest changes. For batch sources, just produce a snapshot after the latest compaction but does not read new changes. Snapshots of full compaction are picked when scheduled full-compaction is enabled.\"from-timestamp\": For streaming sources, continuously reads changes starting from timestamp specified by \"scan.timestamp-millis\", without producing a snapshot at the beginning. For batch sources, produces a snapshot at timestamp specified by \"scan.timestamp-millis\" but does not read new changes.\"from-file-creation-time\": For streaming and batch sources, produces a snapshot and filters the data files by creation time. For streaming sources, upon first startup, and continue to read the latest changes.\"from-snapshot\": For streaming sources, continuously reads changes starting from snapshot specified by \"scan.snapshot-id\", without producing a snapshot at the beginning. For batch sources, produces a snapshot specified by \"scan.snapshot-id\" or \"scan.tag-name\" but does not read new changes.\"from-snapshot-full\": For streaming sources, produces from snapshot specified by \"scan.snapshot-id\" on the table upon first startup, and continuously reads changes. For batch sources, produces a snapshot specified by \"scan.snapshot-id\" but does not read new changes.\"incremental\": Read incremental changes between start and end snapshot or timestamp.   scan.plan-sort-partition false Boolean Whether to sort plan files by partition fields, this allows you to read according to the partition order, even if your partition writes are out of order.\nIt is recommended that you use this for streaming read of the 'append-only' table. By default, streaming read will read the full snapshot first. In order to avoid the disorder reading for partitions, you can open this option.   scan.snapshot-id (none) Long Optional snapshot id used in case of \"from-snapshot\" or \"from-snapshot-full\" scan mode   scan.tag-name (none) String Optional tag name used in case of \"from-snapshot\" scan mode.   scan.timestamp (none) String Optional timestamp used in case of \"from-timestamp\" scan mode, it will be automatically converted to timestamp in unix milliseconds, use local time zone   scan.timestamp-millis (none) Long Optional timestamp used in case of \"from-timestamp\" scan mode. If there is no snapshot earlier than this time, the earliest snapshot will be chosen.   scan.watermark (none) Long Optional watermark used in case of \"from-snapshot\" scan mode. If there is no snapshot later than this watermark, will throw an exceptions.   sequence.field (none) String The field that generates the sequence number for primary key table, the sequence number determines which data is the most recent.   sequence.field.sort-order ascending Enum\n Specify the order of sequence.field.\nPossible values:\"ascending\": specifies sequence.field sort order is ascending.\"descending\": specifies sequence.field sort order is descending.   sink.watermark-time-zone \"UTC\" String The time zone to parse the long watermark value to TIMESTAMP value. The default value is 'UTC', which means the watermark is defined on TIMESTAMP column or not defined. If the watermark is defined on TIMESTAMP_LTZ column, the time zone of watermark is user configured time zone, the value should be the user configured local time zone. The option value is either a full name such as 'America/Los_Angeles', or a custom timezone id such as 'GMT-08:00'.   snapshot.clean-empty-directories false Boolean Whether to try to clean empty directories when expiring snapshots, if enabled, please note:hdfs: may print exceptions in NameNode.oss/s3: may cause performance issue.   snapshot.expire.execution-mode sync Enum\n Specifies the execution mode of expire.\nPossible values:\"sync\": Execute expire synchronously. If there are too many files, it may take a long time and block stream processing.\"async\": Execute expire asynchronously. If the generation of snapshots is greater than the deletion, there will be a backlog of files.   snapshot.expire.limit 50 Integer The maximum number of snapshots allowed to expire at a time.   snapshot.num-retained.max infinite Integer The maximum number of completed snapshots to retain. Should be greater than or equal to the minimum number.   snapshot.num-retained.min 10 Integer The minimum number of completed snapshots to retain. Should be greater than or equal to 1.   snapshot.time-retained 1 h Duration The maximum time of completed snapshots to retain.   snapshot.watermark-idle-timeout (none) Duration In watermarking, if a source remains idle beyond the specified timeout duration, it triggers snapshot advancement and facilitates tag creation.   sort-compaction.local-sample.magnification 1000 Integer The magnification of local sample for sort-compaction.The size of local sample is sink parallelism * magnification.   sort-compaction.range-strategy QUANTITY Enum\n The range strategy of sort compaction, the default value is quantity. If the data size allocated for the sorting task is uneven,which may lead to performance bottlenecks, the config can be set to size.\nPossible values:\"SIZE\"\"QUANTITY\"   sort-engine loser-tree Enum\n Specify the sort engine for table with primary key.\nPossible values:\"min-heap\": Use min-heap for multiway sorting.\"loser-tree\": Use loser-tree for multiway sorting. Compared with heapsort, loser-tree has fewer comparisons and is more efficient.   sort-spill-buffer-size 64 mb MemorySize Amount of data to spill records to disk in spilled sort.   sort-spill-threshold (none) Integer If the maximum number of sort readers exceeds this value, a spill will be attempted. This prevents too many readers from consuming too much memory and causing OOM.   source.split.open-file-cost 4 mb MemorySize Open file cost of a source file. It is used to avoid reading too many files with a source split, which can be very slow.   source.split.target-size 128 mb MemorySize Target size of a source split when scanning a bucket.   spill-compression \"zstd\" String Compression for spill, currently zstd, lzo and zstd are supported.   spill-compression.zstd-level 1 Integer Default spill compression zstd level. For higher compression rates, it can be configured to 9, but the read and write speed will significantly decrease.   streaming-read-mode (none) Enum\n The mode of streaming read that specifies to read the data of table file or log.\nPossible values:\"log\": Read from the data of table log store.\"file\": Read from the data of table file store.   streaming-read-overwrite false Boolean Whether to read the changes from overwrite in streaming mode. Cannot be set to true when changelog producer is full-compaction or lookup because it will read duplicated changes.   streaming.read.snapshot.delay (none) Duration The delay duration of stream read when scan incremental snapshots.   tag.automatic-completion false Boolean Whether to automatically complete missing tags.   tag.automatic-creation none Enum\n Whether to create tag automatically. And how to generate tags.\nPossible values:\"none\": No automatically created tags.\"process-time\": Based on the time of the machine, create TAG once the processing time passes period time plus delay.\"watermark\": Based on the watermark of the input, create TAG once the watermark passes period time plus delay.\"batch\": In the batch processing scenario, the tag corresponding to the current snapshot is generated after the task is completed.   tag.batch.customized-name (none) String Use customized name when creating tags in Batch mode.   tag.callback.#.param (none) String Parameter string for the constructor of class #. Callback class should parse the parameter by itself.   tag.callbacks (none) String A list of commit callback classes to be called after a successful tag. Class names are connected with comma (example: com.test.CallbackA,com.sample.CallbackB).   tag.create-success-file false Boolean Whether to create tag success file for new created tags.   tag.creation-delay 0 ms Duration How long is the delay after the period ends before creating a tag. This can allow some late data to enter the Tag.   tag.creation-period daily Enum\n What frequency is used to generate tags.\nPossible values:\"daily\": Generate a tag every day.\"hourly\": Generate a tag every hour.\"two-hours\": Generate a tag every two hours.   tag.creation-period-duration (none) Duration The period duration for tag auto create periods.If user set it, tag.creation-period would be invalid.   tag.default-time-retained (none) Duration The default maximum time retained for newly created tags. It affects both auto-created tags and manually created (by procedure) tags.   tag.num-retained-max (none) Integer The maximum number of tags to retain. It only affects auto-created tags.   tag.period-formatter with_dashes Enum\n The date format for tag periods.\nPossible values:\"with_dashes\": Dates and hours with dashes, e.g., 'yyyy-MM-dd HH'\"without_dashes\": Dates and hours without dashes, e.g., 'yyyyMMdd HH'\"without_dashes_and_spaces\": Dates and hours without dashes and spaces, e.g., 'yyyyMMddHH'   target-file-size (none) MemorySize Target size of a file.primary key table: the default value is 128 MB.append table: the default value is 256 MB.   type table Enum\n Type of the table.\nPossible values:\"table\": Normal Paimon table.\"format-table\": A file format table refers to a directory that contains multiple files of the same format.\"materialized-table\": A materialized table combines normal Paimon table and materialized SQL.\"object-table\": An object table combines normal Paimon table and object location.   write-buffer-for-append false Boolean This option only works for append-only table. Whether the write use write buffer to avoid out-of-memory error.   write-buffer-size 256 mb MemorySize Amount of data to build up in memory before converting to a sorted on-disk file.   write-buffer-spill.max-disk-size infinite MemorySize The max disk to use for write buffer spill. This only work when the write buffer spill is enabled   write-buffer-spillable (none) Boolean Whether the write buffer can be spillable. Enabled by default when using object storage or when 'target-file-size' is greater than 'write-buffer-size'.   write-manifest-cache 0 bytes MemorySize Cache size for reading manifest files for write initialization.   write-max-writers-to-spill 10 Integer When in batch append inserting, if the writer number is greater than this option, we open the buffer cache and spill function to avoid out-of-memory.    write-only false Boolean If set to true, compactions and snapshot expiration will be skipped. This option is used along with dedicated compact jobs.   write.batch-size 1024 Integer Write batch size for any file format if it supports.   zorder.var-length-contribution 8 Integer The bytes of types (CHAR, VARCHAR, BINARY, VARBINARY) devote to the zorder sort.    CatalogOptions #  Options for paimon catalog.\n  Key Default Type Description     cache-enabled true Boolean Controls whether the catalog will cache databases, tables, manifests and partitions.   cache.expiration-interval 10 min Duration Controls the duration for which databases and tables in the catalog are cached.   cache.manifest.max-memory (none) MemorySize Controls the maximum memory to cache manifest content.   cache.manifest.small-file-memory 128 mb MemorySize Controls the cache memory to cache small manifest files.   cache.manifest.small-file-threshold 1 mb MemorySize Controls the threshold of small manifest file.   cache.partition.max-num 0 Long Controls the max number for which partitions in the catalog are cached.   cache.snapshot.max-num-per-table 20 Integer Controls the max number for snapshots per table in the catalog are cached.   case-sensitive (none) Boolean Indicates whether this catalog is case-sensitive.   client-pool-size 2 Integer Configure the size of the connection pool.   file-io.allow-cache true Boolean Whether to allow static cache in file io implementation. If not allowed, this means that there may be a large number of FileIO instances generated, enabling caching can lead to resource leakage.   file-io.populate-meta false Boolean Whether to populate file metadata while listing or getting file status.   format-table.enabled true Boolean Whether to support format tables, format table corresponds to a regular csv, parquet or orc table, allowing read and write operations. However, during these processes, it does not connect to the metastore; hence, newly added partitions will not be reflected in the metastore and need to be manually added as separate partition operations.   lock-acquire-timeout 8 min Duration The maximum time to wait for acquiring the lock.   lock-check-max-sleep 8 s Duration The maximum sleep time when retrying to check the lock.   lock.enabled (none) Boolean Enable Catalog Lock.   lock.type (none) String The Lock Type for Catalog, such as 'hive', 'zookeeper'.   metastore \"filesystem\" String Metastore of paimon catalog, supports filesystem, hive and jdbc.   resolving-file-io.enabled false Boolean Whether to enable resolving fileio, when this option is enabled, in conjunction with the table's property data-file.external-paths, Paimon can read and write to external storage paths, such as OSS or S3. In order to access these external paths correctly, you also need to configure the corresponding access key and secret key.   sync-all-properties true Boolean Sync all table properties to hive metastore   table.type managed Enum\n Type of table.\nPossible values:\"managed\": Paimon owned table where the entire lifecycle of the table data is managed.\"external\": The table where Paimon has loose coupling with the data stored in external locations.   uri (none) String Uri of metastore server.   warehouse (none) String The warehouse root path of catalog.    HiveCatalogOptions #  Options for Hive catalog.\n  Key Default Type Description     client-pool-cache.eviction-interval-ms 300000 Long Setting the client's pool cache eviction interval(ms).    client-pool-cache.keys (none) String Specify client cache key, multiple elements separated by commas.\n\"ugi\": the Hadoop UserGroupInformation instance that represents the current user using the cache.\"user_name\" similar to UGI but only includes the user's name determined by UserGroupInformation#getUserName.\"conf\": name of an arbitrary configuration. The value of the configuration will be extracted from catalog properties and added to the cache key. A conf element should start with a \"conf:\" prefix which is followed by the configuration name. E.g. specifying \"conf:a.b.c\" will add \"a.b.c\" to the key, and so that configurations with different default catalog wouldn't share the same client pool. Multiple conf elements can be specified.   hadoop-conf-dir (none) String File directory of the core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml. Currently, only local file system paths are supported. If not configured, try to load from 'HADOOP_CONF_DIR' or 'HADOOP_HOME' system environment. Configure Priority: 1.from 'hadoop-conf-dir' 2.from HADOOP_CONF_DIR 3.from HADOOP_HOME/conf 4.HADOOP_HOME/etc/hadoop.    hive-conf-dir (none) String File directory of the hive-site.xml , used to create HiveMetastoreClient and security authentication, such as Kerberos, LDAP, Ranger and so on. If not configured, try to load from 'HIVE_CONF_DIR' env.    location-in-properties false Boolean Setting the location in properties of hive table/database. If you don't want to access the location by the filesystem of hive when using a object storage such as s3,oss you can set this option to true.    metastore.client.class \"org.apache.hadoop.hive.metastore.HiveMetaStoreClient\" String Class name of Hive metastore client. NOTE: This class must directly implements org.apache.hadoop.hive.metastore.IMetaStoreClient.    JdbcCatalogOptions #  Options for Jdbc catalog.\n  Key Default Type Description     catalog-key \"jdbc\" String Custom jdbc catalog store key.   lock-key-max-length 255 Integer Set the maximum length of the lock key. The 'lock-key' is composed of concatenating three fields : 'catalog-key', 'database', and 'table'.    FlinkCatalogOptions #  Flink catalog options for paimon.\n  Key Default Type Description     default-database \"default\" String    disable-create-table-in-default-db false Boolean If true, creating table in default database is not allowed. Default is false.    FlinkConnectorOptions #  Flink connector options for paimon.\n  Key Default Type Description     changelog.precommit-compact.thread-num (none) Integer Maximum number of threads to copy bytes from small changelog files. By default is the number of processors available to the Java virtual machine.   end-input.watermark (none) Long Optional endInput watermark used in case of batch mode or bounded stream.   lookup.async false Boolean Whether to enable async lookup join.   lookup.async-thread-number 16 Integer The thread number for lookup async.   lookup.bootstrap-parallelism 4 Integer The parallelism for bootstrap in a single task for lookup join.   lookup.cache AUTO Enum\n The cache mode of lookup join.\nPossible values:\"AUTO\"\"FULL\"   lookup.dynamic-partition.refresh-interval 1 h Duration Specific dynamic partition refresh interval for lookup, scan all partitions and obtain corresponding partition.   lookup.refresh.async false Boolean Whether to refresh lookup table in an async thread.   lookup.refresh.async.pending-snapshot-count 5 Integer If the pending snapshot count exceeds the threshold, lookup operator will refresh the table in sync.   lookup.refresh.time-periods-blacklist (none) String The blacklist contains several time periods. During these time periods, the lookup table's cache refreshing is forbidden. Blacklist format is start1-\u0026gt;end1,start2-\u0026gt;end2,... , and the time format is yyyy-MM-dd HH:mm. Only used when lookup table is FULL cache mode.   partition.idle-time-to-done (none) Duration Set a time duration when a partition has no new data after this time duration, mark the done status to indicate that the data is ready.   partition.mark-done-action.mode process-time Enum\n How to trigger partition mark done action.\nPossible values:\"process-time\": Based on the time of the machine, mark the partition done once the processing time passes period time plus delay.\"watermark\": Based on the watermark of the input, mark the partition done once the watermark passes period time plus delay.   partition.time-interval (none) Duration You can specify time interval for partition, for example, daily partition is '1 d', hourly partition is '1 h'.   postpone.default-bucket-num 1 Integer Bucket number for the partitions compacted for the first time in postpone bucket tables.   precommit-compact false Boolean If true, it will add a compact coordinator and worker operator after the writer operator,in order to compact several changelog files (for primary key tables) or newly created data files (for unaware bucket tables) from the same partition into large ones, which can decrease the number of small files.   scan.bounded (none) Boolean Bounded mode for Paimon consumer. By default, Paimon automatically selects bounded mode based on the mode of the Flink job.   scan.infer-parallelism true Boolean If it is false, parallelism of source are set by global parallelism. Otherwise, source parallelism is inferred from splits number (batch mode) or bucket number(streaming mode).   scan.infer-parallelism.max 1024 Integer If scan.infer-parallelism is true, limit the parallelism of source through this option.   scan.parallelism (none) Integer Define a custom parallelism for the scan source. By default, if this option is not defined, the planner will derive the parallelism for each statement individually by also considering the global configuration. If user enable the scan.infer-parallelism, the planner will derive the parallelism by inferred parallelism.   scan.partitions (none) String Specify the partitions to scan. Partitions should be given in the form of key1=value1,key2=value2. Partition keys not specified will be filled with the value of partition.default-name. Multiple partitions should be separated by semicolon (;). This option can support normal source tables and lookup join tables. For lookup joins, two special values max_pt() and max_two_pt() are also supported, specifying the (two) partition(s) with the largest partition value.   scan.remove-normalize false Boolean Whether to force the removal of the normalize node when streaming read. Note: This is dangerous and is likely to cause data errors if downstream is used to calculate aggregation and the input is not complete changelog.   scan.split-enumerator.batch-size 10 Integer How many splits should assign to subtask per batch in StaticFileStoreSplitEnumerator to avoid exceed `akka.framesize` limit.   scan.split-enumerator.mode fair Enum\n The mode used by StaticFileStoreSplitEnumerator to assign splits.\nPossible values:\"fair\": Distribute splits evenly when batch reading to prevent a few tasks from reading all.\"preemptive\": Distribute splits preemptively according to the consumption speed of the task.   scan.watermark.alignment.group (none) String A group of sources to align watermarks.   scan.watermark.alignment.max-drift (none) Duration Maximal drift to align watermarks, before we pause consuming from the source/task/partition.   scan.watermark.alignment.update-interval 1 s Duration How often tasks should notify coordinator about the current watermark and how often the coordinator should announce the maximal aligned watermark.   scan.watermark.emit.strategy on-event Enum\n Emit strategy for watermark generation.\nPossible values:\"on-periodic\": Emit watermark periodically, interval is controlled by Flink 'pipeline.auto-watermark-interval'.\"on-event\": Emit watermark per record.   scan.watermark.idle-timeout (none) Duration If no records flow in a partition of a stream for that amount of time, then that partition is considered \"idle\" and will not hold back the progress of watermarks in downstream operators.   sink.clustering.by-columns (none) String Specifies the column name(s) used for comparison during range partitioning, in the format 'columnName1,columnName2'. If not set or set to an empty string, it indicates that the range partitioning feature is not enabled. This option will be effective only for bucket unaware table without primary keys and batch execution mode.   sink.clustering.sample-factor 100 Integer Specifies the sample factor. Let S represent the total number of samples, F represent the sample factor, and P represent the sink parallelism, then S=F×P. The minimum allowed sample factor is 20.   sink.clustering.sort-in-cluster true Boolean Indicates whether to further sort data belonged to each sink task after range partitioning.   sink.clustering.strategy \"auto\" String Specifies the comparison algorithm used for range partitioning, including 'zorder', 'hilbert', and 'order', corresponding to the z-order curve algorithm, hilbert curve algorithm, and basic type comparison algorithm, respectively. When not configured, it will automatically determine the algorithm based on the number of columns in 'sink.clustering.by-columns'. 'order' is used for 1 column, 'zorder' for less than 5 columns, and 'hilbert' for 5 or more columns.   sink.committer-cpu 1.0 Double Sink committer cpu to control cpu cores of global committer.   sink.committer-memory (none) MemorySize Sink committer memory to control heap memory of global committer.   sink.committer-operator-chaining true Boolean Allow sink committer and writer operator to be chained together   sink.cross-partition.managed-memory 256 mb MemorySize Weight of managed memory for RocksDB in cross-partition update, Flink will compute the memory size according to the weight, the actual memory used depends on the running environment.   sink.managed.writer-buffer-memory 256 mb MemorySize Weight of writer buffer in managed memory, Flink will compute the memory size for writer according to the weight, the actual memory used depends on the running environment.   sink.operator-uid.suffix (none) String Set the uid suffix for the writer, dynamic bucket assigner and committer operators. The uid format is ${UID_PREFIX}_${TABLE_NAME}_${USER_UID_SUFFIX}. If the uid suffix is not set, flink will automatically generate the operator uid, which may be incompatible when the topology changes.   sink.parallelism (none) Integer Defines a custom parallelism for the sink. By default, if this option is not defined, the planner will derive the parallelism for each statement individually by also considering the global configuration.   sink.savepoint.auto-tag false Boolean If true, a tag will be automatically created for the snapshot created by flink savepoint.   sink.use-managed-memory-allocator false Boolean If true, flink sink will use managed memory for merge tree; otherwise, it will create an independent memory allocator.   source.checkpoint-align.enabled false Boolean Whether to align the flink checkpoint with the snapshot of the paimon table, If true, a checkpoint will only be made if a snapshot is consumed.   source.checkpoint-align.timeout 30 s Duration If the new snapshot has not been generated when the checkpoint starts to trigger, the enumerator will block the checkpoint and wait for the new snapshot. Set the maximum waiting time to avoid infinite waiting, if timeout, the checkpoint will fail. Note that it should be set smaller than the checkpoint timeout.   source.operator-uid.suffix (none) String Set the uid suffix for the source operators. After setting, the uid format is ${UID_PREFIX}_${TABLE_NAME}_${USER_UID_SUFFIX}. If the uid suffix is not set, flink will automatically generate the operator uid, which may be incompatible when the topology changes.   streaming-read.shuffle-bucket-with-partition true Boolean Whether shuffle by partition and bucket when streaming read.   unaware-bucket.compaction.parallelism (none) Integer Defines a custom parallelism for the unaware-bucket table compaction job. By default, if this option is not defined, the planner will derive the parallelism for each statement individually by also considering the global configuration.    SparkCatalogOptions #  Spark catalog options for paimon.\n  Key Default Type Description     catalog.create-underlying-session-catalog false Boolean If true, create and use an underlying session catalog instead of default session catalog when use SparkGenericCatalog.   defaultDatabase \"default\" String The default database name.    SparkConnectorOptions #  Spark connector options for paimon.\n  Key Default Type Description     read.changelog false Boolean Whether to read row in the form of changelog (add rowkind column in row to represent its change type).   read.stream.maxBytesPerTrigger (none) Long The maximum number of bytes returned in a single batch.   read.stream.maxFilesPerTrigger (none) Integer The maximum number of files returned in a single batch.   read.stream.maxRowsPerTrigger (none) Long The maximum number of rows returned in a single batch.   read.stream.maxTriggerDelayMs (none) Long The maximum delay between two adjacent batches, which used to create MinRowsReadLimit with read.stream.minRowsPerTrigger together.   read.stream.minRowsPerTrigger (none) Long The minimum number of rows returned in a single batch, which used to create MinRowsReadLimit with read.stream.maxTriggerDelayMs together.   requiredSparkConfsCheck.enabled true Boolean Whether to verify SparkSession is initialized with required configurations.   write.merge-schema false Boolean If true, merge the data schema and the table schema automatically before write data.   write.merge-schema.explicit-cast false Boolean If true, allow to merge data types if the two types meet the rules for explicit casting.    ORC Options #    Key Default Type Description     orc.column.encoding.direct (none) Integer Comma-separated list of fields for which dictionary encoding is to be skipped in orc.   orc.dictionary.key.threshold 0.8 Double If the number of distinct keys in a dictionary is greater than this fraction of the total number of non-null rows, turn off dictionary encoding in orc. Use 0 to always disable dictionary encoding. Use 1 to always use dictionary encoding.   orc.timestamp-ltz.legacy.type true Boolean This option is used to be compatible with the paimon-orc‘s old behavior for the `timestamp_ltz` data type.    RocksDB Options #  The following options allow users to finely adjust RocksDB for better performance. You can either specify them in table properties or in dynamic table hints.\n  Key Default Type Description     lookup.cache-rows 10000 Long The maximum number of rows to store in the cache.   lookup.continuous.discovery-interval (none) Duration The discovery interval of lookup continuous reading. This is used as an SQL hint. If it's not configured, the lookup function will fallback to 'continuous.discovery-interval'.   rocksdb.block.blocksize 4 kb MemorySize The approximate size (in bytes) of user data packed per block. The default blocksize is '4KB'.   rocksdb.block.cache-size 128 mb MemorySize The amount of the cache for data blocks in RocksDB.   rocksdb.block.metadata-blocksize 4 kb MemorySize Approximate size of partitioned metadata packed per block. Currently applied to indexes block when partitioned index/filters option is enabled. The default blocksize is '4KB'.   rocksdb.bloom-filter.bits-per-key 10.0 Double Bits per key that bloom filter will use, this only take effect when bloom filter is used. The default value is 10.0.   rocksdb.bloom-filter.block-based-mode false Boolean If true, RocksDB will use block-based filter instead of full filter, this only take effect when bloom filter is used. The default value is 'false'.   rocksdb.compaction.level.max-size-level-base 256 mb MemorySize The upper-bound of the total size of level base files in bytes. The default value is '256MB'.   rocksdb.compaction.level.target-file-size-base 64 mb MemorySize The target file size for compaction, which determines a level-1 file size. The default value is '64MB'.   rocksdb.compaction.level.use-dynamic-size false Boolean If true, RocksDB will pick target size of each level dynamically. From an empty DB, RocksDB would make last level the base level, which means merging L0 data into the last level, until it exceeds max_bytes_for_level_base. And then repeat this process for second last level and so on. The default value is 'false'. For more information, please refer to RocksDB's doc.   rocksdb.compaction.style LEVEL Enum\n The specified compaction style for DB. Candidate compaction style is LEVEL, FIFO, UNIVERSAL or NONE, and Flink chooses 'LEVEL' as default style.\nPossible values:\"LEVEL\"\"UNIVERSAL\"\"FIFO\"\"NONE\"   rocksdb.compression.type LZ4_COMPRESSION Enum\n The compression type.\nPossible values:\"NO_COMPRESSION\"\"SNAPPY_COMPRESSION\"\"ZLIB_COMPRESSION\"\"BZLIB2_COMPRESSION\"\"LZ4_COMPRESSION\"\"LZ4HC_COMPRESSION\"\"XPRESS_COMPRESSION\"\"ZSTD_COMPRESSION\"\"DISABLE_COMPRESSION_OPTION\"   rocksdb.files.open -1 Integer The maximum number of open files (per stateful operator) that can be used by the DB, '-1' means no limit. The default value is '-1'.   rocksdb.thread.num 2 Integer The maximum number of concurrent background flush and compaction jobs (per stateful operator). The default value is '2'.   rocksdb.use-bloom-filter false Boolean If true, every newly created SST file will contain a Bloom filter. It is disabled by default.   rocksdb.writebuffer.count 2 Integer The maximum number of write buffers that are built up in memory. The default value is '2'.   rocksdb.writebuffer.number-to-merge 1 Integer The minimum number of write buffers that will be merged together before writing to storage. The default value is '1'.   rocksdb.writebuffer.size 64 mb MemorySize The amount of data built up in memory (backed by an unsorted log on disk) before converting to a sorted on-disk files. The default writebuffer size is '64MB'.    "});index.add({'id':104,'href':'/docs/1.1/concepts/rest/rest-api/','title':"REST API",'section':"RESTCatalog",'content':"   Redoc.init('\\/\\/paimon.apache.org\\/docs\\/1.1/rest-catalog-open-api.yaml', { disableSearch: true }, document.getElementById('redoc-container'));   "});index.add({'id':105,'href':'/docs/1.1/versions/','title':"Versions",'section':"Apache Paimon",'content':"Versions #  An appendix of hosted documentation for all versions of Apache Paimon.\n master    stable    1.1    1.0    0.9    "});})();