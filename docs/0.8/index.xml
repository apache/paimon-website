<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Apache Paimon</title>
    <link>//paimon.apache.org/docs/0.8/</link>
    <description>Recent content on Apache Paimon</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="//paimon.apache.org/docs/0.8/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Append Table</title>
      <link>//paimon.apache.org/docs/0.8/append-table/append-table/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/append-table/append-table/</guid>
      <description>Append Table #  If a table does not have a primary key defined, it is an append table by default.
You can only insert a complete record into the table in streaming. This type of table is suitable for use cases that do not require streaming updates (such as log data synchronization).
Flink CREATE TABLE my_table ( product_id BIGINT, price DOUBLE, sales BIGINT );  Data Distribution #  By default, append table has no bucket concept.</description>
    </item>
    
    <item>
      <title>Java API</title>
      <link>//paimon.apache.org/docs/0.8/program-api/java-api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/program-api/java-api/</guid>
      <description>Java API #  We do not recommend using the Paimon API naked, unless you are a professional downstream ecosystem developer, and even if you do, there will be significant difficulties.
If you are only using Paimon, we strongly recommend using computing engines such as Flink SQL or Spark SQL.
The following documents are not detailed and are for reference only.
 Dependency #  Maven dependency:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.paimon&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;paimon-bundle&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;0.</description>
    </item>
    
    <item>
      <title>Migration From Hive</title>
      <link>//paimon.apache.org/docs/0.8/migration/migration-from-hive/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/migration/migration-from-hive/</guid>
      <description>Hive Table Migration #  Apache Hive supports ORC, Parquet file formats that could be migrated to Paimon. When migrating data to a paimon table, the origin table will be permanently disappeared. So please back up your data if you still need the original table. The migrated table will be append table.
Now, we can use paimon hive catalog with Migrate Table Procedure and Migrate File Procedure to totally migrate a table from hive to paimon.</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//paimon.apache.org/docs/0.8/concepts/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/concepts/overview/</guid>
      <description>Overview #  Apache Paimon&amp;rsquo;s Architecture:
As shown in the architecture above:
Read/Write: Paimon supports a versatile way to read/write data and perform OLAP queries.
 For reads, it supports consuming data  from historical snapshots (in batch mode), from the latest offset (in streaming mode), or reading incremental snapshots in a hybrid way.   For writes, it supports  streaming synchronization from the changelog of databases (CDC) batch insert/overwrite from offline data.</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//paimon.apache.org/docs/0.8/engines/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/engines/overview/</guid>
      <description>Overview #  Compatibility Matrix #     Engine Version Batch Read Batch Write Create Table Alter Table Streaming Write Streaming Read Batch Overwrite DELETE &amp;amp; UPDATE MERGE INTO     Flink 1.15 - 1.19 ✅ ✅ ✅ ✅(1.17+) ✅ ✅ ✅ ✅(1.17+) ❌   Spark 3.1 - 3.5 ✅ ✅(3.3+) ✅ ✅ ✅(3.3+) ✅(3.3+) ✅(3.2+) ✅(3.2+) ✅(3.2+)   Hive 2.1 - 3.1 ✅ ✅ ✅ ❌ ❌ ❌ ❌ ❌ ❌   Trino 420 - 439 ✅ ✅(427+) ✅(427+) ✅(427+) ❌ ❌ ❌ ❌ ❌   Presto 0.</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//paimon.apache.org/docs/0.8/filesystems/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/filesystems/overview/</guid>
      <description>Overview #  Apache Paimon utilizes the same pluggable file systems as Apache Flink. Users can follow the standard plugin mechanism to configure the plugin structure if using Flink as compute engine. However, for other engines like Spark or Hive, the provided opt jars (by Flink) may get conflicts and cannot be used directly. It is not convenient for users to fix class conflicts, thus Paimon provides the self-contained and engine-unified FileSystem pluggable jars for user to query tables from Spark/Hive side.</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//paimon.apache.org/docs/0.8/flink/cdc-ingestion/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/flink/cdc-ingestion/overview/</guid>
      <description>Overview #  Paimon supports a variety of ways to ingest data into Paimon tables with schema evolution. This means that the added columns are synchronized to the Paimon table in real time and the synchronization job will not be restarted for this purpose.
We currently support the following sync ways:
 MySQL Synchronizing Table: synchronize one or multiple tables from MySQL into one Paimon table. MySQL Synchronizing Database: synchronize the whole MySQL database into one Paimon database.</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//paimon.apache.org/docs/0.8/primary-key-table/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/primary-key-table/overview/</guid>
      <description>Overview #  If you define a table with primary key, you can insert, update or delete records in the table.
Primary keys consist of a set of columns that contain unique values for each record. Paimon enforces data ordering by sorting the primary key within each bucket, allowing users to achieve high performance by applying filtering conditions on the primary key. See CREATE TABLE.
Bucket #  Unpartitioned tables, or partitions in partitioned tables, are sub-divided into buckets, to provide extra structure to the data that may be used for more efficient querying.</description>
    </item>
    
    <item>
      <title>Quick Start</title>
      <link>//paimon.apache.org/docs/0.8/flink/quick-start/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/flink/quick-start/</guid>
      <description>Quick Start #  This documentation is a guide for using Paimon in Flink.
Jars #  Paimon currently supports Flink 1.19, 1.18, 1.17, 1.16, 1.15. We recommend the latest Flink version for a better experience.
Download the jar file with corresponding version.
 Currently, paimon provides two types jar: one of which(the bundled jar) is used for read/write data, and the other(action jar) for operations such as manually compaction,    Version Type Jar     Flink 1.</description>
    </item>
    
    <item>
      <title>Quick Start</title>
      <link>//paimon.apache.org/docs/0.8/spark/quick-start/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/spark/quick-start/</guid>
      <description>Quick Start #  Preparation #  Paimon currently supports Spark 3.5, 3.4, 3.3, 3.2 and 3.1. We recommend the latest Spark version for a better experience.
Download the jar file with corresponding version.
   Version Jar     Spark 3.5 paimon-spark-3.5-0.8.2.jar   Spark 3.4 paimon-spark-3.4-0.8.2.jar   Spark 3.3 paimon-spark-3.3-0.8.2.jar   Spark 3.2 paimon-spark-3.2-0.8.2.jar   Spark 3.1 paimon-spark-3.1-0.8.2.jar    You can also manually build bundled jar from the source code.</description>
    </item>
    
    <item>
      <title>System Tables</title>
      <link>//paimon.apache.org/docs/0.8/maintenance/system-tables/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/maintenance/system-tables/</guid>
      <description>System Tables #  Table Specified System Table #  Table specified system tables contain metadata and information about each table, such as the snapshots created and the options in use. Users can access system tables with batch queries.
Currently, Flink, Spark, Trino and StarRocks support querying system tables.
In some cases, the table name needs to be enclosed with back quotes to avoid syntax parsing conflicts, for example triple access mode:</description>
    </item>
    
    <item>
      <title>Understand Files</title>
      <link>//paimon.apache.org/docs/0.8/learn-paimon/understand-files/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/learn-paimon/understand-files/</guid>
      <description>Understand Files #  This article is specifically designed to clarify the impact that various file operations have on files.
This page provides concrete examples and practical tips for effectively managing them. Furthermore, through an in-depth exploration of operations such as commit and compact, we aim to offer insights into the creation and updates of files.
Prerequisite #  Before delving further into this page, please ensure that you have read through the following sections:</description>
    </item>
    
    <item>
      <title>Upsert To Partitioned</title>
      <link>//paimon.apache.org/docs/0.8/migration/upsert-to-partitioned/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/migration/upsert-to-partitioned/</guid>
      <description>Upsert To Partitioned #  The Tag Management will maintain the manifests and data files of the snapshot. A typical usage is creating tags daily, then you can maintain the historical data of each day for batch reading.
When using primary key tables, a non-partitioned approach is often used to maintain updates, in order to mirror and synchronize tables from upstream database tables. This allows users to query the latest data.</description>
    </item>
    
    <item>
      <title>Basic Concepts</title>
      <link>//paimon.apache.org/docs/0.8/concepts/basic-concepts/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/concepts/basic-concepts/</guid>
      <description>Basic Concepts #  File Layouts #  All files of a table are stored under one base directory. Paimon files are organized in a layered style. The following image illustrates the file layout. Starting from a snapshot file, Paimon readers can recursively access all records from the table.
Snapshot #  All snapshot files are stored in the snapshot directory.
A snapshot file is a JSON file containing information about this snapshot, including</description>
    </item>
    
    <item>
      <title>Data Distribution</title>
      <link>//paimon.apache.org/docs/0.8/primary-key-table/data-distribution/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/primary-key-table/data-distribution/</guid>
      <description>Data Distribution #  By default, Paimon table only has one bucket, which means it only provides single parallelism read and write. Please configure the bucket strategy to your table.  A bucket is the smallest storage unit for reads and writes, each bucket directory contains an LSM tree.
Fixed Bucket #  Configure a bucket greater than 0, using Fixed Bucket mode, according to Math.abs(key_hashcode % numBuckets) to compute the bucket of record.</description>
    </item>
    
    <item>
      <title>Download</title>
      <link>//paimon.apache.org/docs/0.8/project/download/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/project/download/</guid>
      <description>Download #  This documentation is a guide for downloading Paimon Jars.
Engine Jars #     Version Jar     Flink 1.19 paimon-flink-1.19-0.8.2.jar   Flink 1.18 paimon-flink-1.18-0.8.2.jar   Flink 1.17 paimon-flink-1.17-0.8.2.jar   Flink 1.16 paimon-flink-1.16-0.8.2.jar   Flink 1.15 paimon-flink-1.15-0.8.2.jar   Flink Action paimon-flink-action-0.8.2.jar   Spark 3.5 paimon-spark-3.5-0.8.2.jar   Spark 3.4 paimon-spark-3.4-0.8.2.jar   Spark 3.3 paimon-spark-3.3-0.8.2.jar   Spark 3.</description>
    </item>
    
    <item>
      <title>Flink API</title>
      <link>//paimon.apache.org/docs/0.8/program-api/flink-api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/program-api/flink-api/</guid>
      <description>Flink API #  We do not recommend using programming API. Paimon is designed for SQL first, unless you are a professional Flink developer, even if you do, it can be very difficult.
We strongly recommend that you use Flink SQL or Spark SQL, or simply use SQL APIs in programs.
The following documents are not detailed and are for reference only.
 Dependency #  Maven dependency:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.paimon&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;paimon-flink-1.</description>
    </item>
    
    <item>
      <title>HDFS</title>
      <link>//paimon.apache.org/docs/0.8/filesystems/hdfs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/filesystems/hdfs/</guid>
      <description>HDFS #  You don&amp;rsquo;t need any additional dependencies to access HDFS because you have already taken care of the Hadoop dependencies.
HDFS Configuration #  For HDFS, the most important thing is to be able to read your HDFS configuration.
Flink/Trino/JavaAPI You may not have to do anything, if you are in a hadoop environment. Otherwise pick one of the following ways to configure your HDFS:
 Set environment variable HADOOP_HOME or HADOOP_CONF_DIR.</description>
    </item>
    
    <item>
      <title>Mysql CDC</title>
      <link>//paimon.apache.org/docs/0.8/flink/cdc-ingestion/mysql-cdc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/flink/cdc-ingestion/mysql-cdc/</guid>
      <description>MySQL CDC #  Paimon supports synchronizing changes from different databases using change data capture (CDC). This feature requires Flink and its CDC connectors.
Prepare CDC Bundled Jar #  flink-sql-connector-mysql-cdc-*.jar Synchronizing Tables #  By using MySqlSyncTableAction in a Flink DataStream job or directly through flink run, users can synchronize one or multiple tables from MySQL into one Paimon table.
To use this feature through flink run, run the following shell command.</description>
    </item>
    
    <item>
      <title>Postgres CDC</title>
      <link>//paimon.apache.org/docs/0.8/flink/cdc-ingestion/postgres-cdc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/flink/cdc-ingestion/postgres-cdc/</guid>
      <description>Postgres CDC #  Paimon supports synchronizing changes from different databases using change data capture (CDC). This feature requires Flink and its CDC connectors.
Prepare CDC Bundled Jar #  flink-connector-postgres-cdc-*.jar Synchronizing Tables #  By using PostgresSyncTableAction in a Flink DataStream job or directly through flink run, users can synchronize one or multiple tables from PostgreSQL into one Paimon table.
To use this feature through flink run, run the following shell command.</description>
    </item>
    
    <item>
      <title>SQL DDL</title>
      <link>//paimon.apache.org/docs/0.8/flink/sql-ddl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/flink/sql-ddl/</guid>
      <description>SQL DDL #  Create Catalog #  Paimon catalogs currently support three types of metastores:
 filesystem metastore (default), which stores both metadata and table files in filesystems. hive metastore, which additionally stores metadata in Hive metastore. Users can directly access the tables from Hive. jdbc metastore, which additionally stores metadata in relational databases such as MySQL, Postgres, etc.  See CatalogOptions for detailed options when creating a catalog.</description>
    </item>
    
    <item>
      <title>SQL DDL</title>
      <link>//paimon.apache.org/docs/0.8/spark/sql-ddl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/spark/sql-ddl/</guid>
      <description>SQL DDL #  Create Catalog #  Paimon catalogs currently support three types of metastores:
 filesystem metastore (default), which stores both metadata and table files in filesystems. hive metastore, which additionally stores metadata in Hive metastore. Users can directly access the tables from Hive. jdbc metastore, which additionally stores metadata in relational databases such as MySQL, Postgres, etc.  See CatalogOptions for detailed options when creating a catalog.</description>
    </item>
    
    <item>
      <title>SQL Write</title>
      <link>//paimon.apache.org/docs/0.8/flink/sql-write/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/flink/sql-write/</guid>
      <description>SQL Write #  Syntax #  INSERT { INTO | OVERWRITE } table_identifier [ part_spec ] [ column_list ] { value_expr | query }; For more information, please check the syntax document:
Flink INSERT Statement
INSERT INTO #  Use INSERT INTO to apply records and changes to tables.
INSERT INTO my_table SELECT ... INSERT INTO supports both batch and streaming mode. In Streaming mode, by default, it will also perform compaction, snapshot expiration, and even partition expiration in Flink Sink (if it is configured).</description>
    </item>
    
    <item>
      <title>SQL Write</title>
      <link>//paimon.apache.org/docs/0.8/spark/sql-write/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/spark/sql-write/</guid>
      <description>SQL Write #  Syntax #  INSERT { INTO | OVERWRITE } table_identifier [ part_spec ] [ column_list ] { value_expr | query }; For more information, please check the syntax document:
Spark INSERT Statement
INSERT INTO #  Use INSERT INTO to apply records and changes to tables.
INSERT INTO my_table SELECT ... Overwriting the Whole Table #  Use INSERT OVERWRITE to overwrite the whole unpartitioned table.</description>
    </item>
    
    <item>
      <title>Write Performance</title>
      <link>//paimon.apache.org/docs/0.8/maintenance/write-performance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/maintenance/write-performance/</guid>
      <description>Write Performance #  Paimon&amp;rsquo;s write performance is closely related to checkpoint, so if you need greater write throughput:
 Flink Configuration (&#39;flink-conf.yaml&#39; or SET in SQL): Increase the checkpoint interval (&#39;execution.checkpointing.interval&#39;), increase max concurrent checkpoints to 3 (&#39;execution.checkpointing.max-concurrent-checkpoints&#39;), or just use batch mode. Increase write-buffer-size. Enable write-buffer-spillable. Rescale bucket number if you are using Fixed-Bucket mode.  Option &#39;changelog-producer&#39; = &#39;lookup&#39; or &#39;full-compaction&#39;, and option &#39;full-compaction.delta-commits&#39; have a large impact on write performance, if it is a snapshot / full synchronization phase you can unset these options and then enable them again in the incremental phase.</description>
    </item>
    
    <item>
      <title>Append Queue</title>
      <link>//paimon.apache.org/docs/0.8/append-table/append-queue/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/append-table/append-queue/</guid>
      <description>Append Queue #  Definition #  In this mode, you can regard append table as a queue separated by bucket. Every record in the same bucket is ordered strictly, streaming read will transfer the record to down-stream exactly in the order of writing. To use this mode, you do not need to config special configurations, all the data will go into one bucket as a queue. You can also define the bucket and bucket-key to enable larger parallelism and disperse data.</description>
    </item>
    
    <item>
      <title>Concurrency Control</title>
      <link>//paimon.apache.org/docs/0.8/concepts/concurrency-control/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/concepts/concurrency-control/</guid>
      <description>Concurrency Control #  Paimon supports optimistic concurrency for multiple concurrent write jobs.
Each job writes data at its own pace and generates a new snapshot based on the current snapshot by applying incremental files (deleting or adding files) at the time of committing.
There may be two types of commit failures here:
 Snapshot conflict: the snapshot id has been preempted, the table has generated a new snapshot from another job.</description>
    </item>
    
    <item>
      <title>Contributing</title>
      <link>//paimon.apache.org/docs/0.8/project/contributing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/project/contributing/</guid>
      <description>Contributing #  Apache Paimon is developed by an open and friendly community. Everybody is cordially welcome to join the community and contribute to Apache Paimon. There are several ways to interact with the community and contribute to Paimon including asking questions, filing bug reports, proposing new features, joining discussions on the mailing lists, contributing code or documentation, improving website, testing release candidates and writing corresponding blog etc.
What do you want to do?</description>
    </item>
    
    <item>
      <title>Dedicated Compaction</title>
      <link>//paimon.apache.org/docs/0.8/maintenance/dedicated-compaction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/maintenance/dedicated-compaction/</guid>
      <description>Dedicated Compaction #  Paimon&amp;rsquo;s snapshot management supports writing with multiple writers.
For S3-like object store, its &#39;RENAME&#39; does not have atomic semantic. We need to configure Hive metastore and enable &#39;lock.enabled&#39; option for the catalog.  By default, Paimon supports concurrent writing to different partitions. A recommended mode is that streaming job writes records to Paimon&amp;rsquo;s latest partition; Simultaneously batch job (overwrite) writes records to the historical partition.
So far, everything works very well, but if you need multiple writers to write records to the same partition, it will be a bit more complicated.</description>
    </item>
    
    <item>
      <title>Kafka CDC</title>
      <link>//paimon.apache.org/docs/0.8/flink/cdc-ingestion/kafka-cdc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/flink/cdc-ingestion/kafka-cdc/</guid>
      <description>Kafka CDC #  Prepare Kafka Bundled Jar #  flink-sql-connector-kafka-*.jar Supported Formats #  Flink provides several Kafka CDC formats: Canal, Debezium, Ogg and Maxwell JSON. If a message in a Kafka topic is a change event captured from another database using the Change Data Capture (CDC) tool, then you can use the Paimon Kafka CDC. Write the INSERT, UPDATE, DELETE messages parsed into the paimon table.
  Formats Supported     Canal CDC True   Debezium CDC True   Maxwell CDC True   OGG CDC True    The JSON sources possibly missing some information.</description>
    </item>
    
    <item>
      <title>Merge Engine</title>
      <link>//paimon.apache.org/docs/0.8/primary-key-table/merge-engine/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/primary-key-table/merge-engine/</guid>
      <description>Merge Engine #  When Paimon sink receives two or more records with the same primary keys, it will merge them into one record to keep primary keys unique. By specifying the merge-engine table property, users can choose how records are merged together.
Always set table.exec.sink.upsert-materialize to NONE in Flink SQL TableConfig, sink upsert-materialize may result in strange behavior. When the input is out of order, we recommend that you use Sequence Field to correct disorder.</description>
    </item>
    
    <item>
      <title>OSS</title>
      <link>//paimon.apache.org/docs/0.8/filesystems/oss/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/filesystems/oss/</guid>
      <description>OSS #  Download paimon-oss-0.8.2.jar. Flink If you have already configured oss access through Flink (Via Flink FileSystem), here you can skip the following configuration.  Put paimon-oss-0.8.2.jar into lib directory of your Flink home, and create catalog:
CREATE CATALOG my_catalog WITH ( &amp;#39;type&amp;#39; = &amp;#39;paimon&amp;#39;, &amp;#39;warehouse&amp;#39; = &amp;#39;oss://&amp;lt;bucket&amp;gt;/&amp;lt;path&amp;gt;&amp;#39;, &amp;#39;fs.oss.endpoint&amp;#39; = &amp;#39;oss-cn-hangzhou.aliyuncs.com&amp;#39;, &amp;#39;fs.oss.accessKeyId&amp;#39; = &amp;#39;xxx&amp;#39;, &amp;#39;fs.oss.accessKeySecret&amp;#39; = &amp;#39;yyy&amp;#39; ); Spark If you have already configured oss access through Spark (Via Hadoop FileSystem), here you can skip the following configuration.</description>
    </item>
    
    <item>
      <title>Changelog Producer</title>
      <link>//paimon.apache.org/docs/0.8/primary-key-table/changelog-producer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/primary-key-table/changelog-producer/</guid>
      <description>Changelog Producer #  Streaming write can continuously produce the latest changes for streaming read.
By specifying the changelog-producer table property when creating the table, users can choose the pattern of changes produced from table files.
The changelog-producer table property only affects changelog from table files. It does not affect the external log system.  None #  By default, no extra changelog producer will be applied to the writer of table.</description>
    </item>
    
    <item>
      <title>Manage Snapshots</title>
      <link>//paimon.apache.org/docs/0.8/maintenance/manage-snapshots/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/maintenance/manage-snapshots/</guid>
      <description>Manage Snapshots #  This section will describe the management and behavior related to snapshots.
Expire Snapshots #  Paimon writers generate one or two snapshot per commit. Each snapshot may add some new data files or mark some old data files as deleted. However, the marked data files are not truly deleted because Paimon also supports time traveling to an earlier snapshot. They are only deleted when the snapshot expires.</description>
    </item>
    
    <item>
      <title>Mongo CDC</title>
      <link>//paimon.apache.org/docs/0.8/flink/cdc-ingestion/mongo-cdc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/flink/cdc-ingestion/mongo-cdc/</guid>
      <description>Mongo CDC #  Prepare MongoDB Bundled Jar #  flink-sql-connector-mongodb-cdc-*.jar only cdc 2.4+ is supported
Synchronizing Tables #  By using MongoDBSyncTableAction in a Flink DataStream job or directly through flink run, users can synchronize one collection from MongoDB into one Paimon table.
To use this feature through flink run, run the following shell command.
&amp;lt;FLINK_HOME&amp;gt;/bin/flink run \  /path/to/paimon-flink-action-0.8.2.jar \  mongodb_sync_table --warehouse &amp;lt;warehouse-path&amp;gt; \  --database &amp;lt;database-name&amp;gt; \  --table &amp;lt;table-name&amp;gt; \  [--partition_keys &amp;lt;partition_keys&amp;gt;] \  [--computed_column &amp;lt;&amp;#39;column-name=expr-name(args[, .</description>
    </item>
    
    <item>
      <title>S3</title>
      <link>//paimon.apache.org/docs/0.8/filesystems/s3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/filesystems/s3/</guid>
      <description>S3 #  Download paimon-s3-0.8.2.jar. Flink If you have already configured s3 access through Flink (Via Flink FileSystem), here you can skip the following configuration.  Put paimon-s3-0.8.2.jar into lib directory of your Flink home, and create catalog:
CREATE CATALOG my_catalog WITH ( &amp;#39;type&amp;#39; = &amp;#39;paimon&amp;#39;, &amp;#39;warehouse&amp;#39; = &amp;#39;s3://&amp;lt;bucket&amp;gt;/&amp;lt;path&amp;gt;&amp;#39;, &amp;#39;s3.endpoint&amp;#39; = &amp;#39;your-endpoint-hostname&amp;#39;, &amp;#39;s3.access-key&amp;#39; = &amp;#39;xxx&amp;#39;, &amp;#39;s3.secret-key&amp;#39; = &amp;#39;yyy&amp;#39; ); Spark If you have already configured s3 access through Spark (Via Hadoop FileSystem), here you can skip the following configuration.</description>
    </item>
    
    <item>
      <title>Specification</title>
      <link>//paimon.apache.org/docs/0.8/concepts/specification/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/concepts/specification/</guid>
      <description>Specification #  This is the specification for the Paimon table format, this document standardizes the underlying file structure and design of Paimon.
Terms #   Schema: fields, primary keys definition, partition keys definition and options. Snapshot: the entrance to all data committed at some specific time point. Manifest list: includes several manifest files. Manifest: includes several data files or changelog files. Data File: contains incremental records. Changelog File: contains records produced by changelog-producer.</description>
    </item>
    
    <item>
      <title>SQL Query</title>
      <link>//paimon.apache.org/docs/0.8/flink/sql-query/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/flink/sql-query/</guid>
      <description>SQL Query #  Just like all other tables, Paimon tables can be queried with SELECT statement.
Batch Query #  Paimon&amp;rsquo;s batch read returns all the data in a snapshot of the table. By default, batch reads return the latest snapshot.
-- Flink SQL SET &amp;#39;execution.runtime-mode&amp;#39; = &amp;#39;batch&amp;#39;; Batch Time Travel #  Paimon batch reads with time travel can specify a snapshot or a tag and read the corresponding data.</description>
    </item>
    
    <item>
      <title>SQL Query</title>
      <link>//paimon.apache.org/docs/0.8/spark/sql-query/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/spark/sql-query/</guid>
      <description>SQL Query #  Just like all other tables, Paimon tables can be queried with SELECT statement.
Batch Query #  Paimon&amp;rsquo;s batch read returns all the data in a snapshot of the table. By default, batch reads return the latest snapshot.
Batch Time Travel #  Paimon batch reads with time travel can specify a snapshot or a tag and read the corresponding data.
Requires Spark 3.3+.
you can use VERSION AS OF and TIMESTAMP AS OF in query to do time travel:</description>
    </item>
    
    <item>
      <title>Doris</title>
      <link>//paimon.apache.org/docs/0.8/engines/doris/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/engines/doris/</guid>
      <description>Doris #  This documentation is a guide for using Paimon in Doris.
 More details can be found in Apache Doris Website
 Version #  Paimon currently supports Apache Doris 2.0.6 and above.
Create Paimon Catalog #  Use CREATE CATALOG statement in Apache Doris to create Paimon Catalog.
Doris support multi types of Paimon Catalogs. Here are some examples:
-- HDFS based Paimon Catalog CREATE CATALOG `paimon_hdfs` PROPERTIES ( &amp;#34;type&amp;#34; = &amp;#34;paimon&amp;#34;, &amp;#34;warehouse&amp;#34; = &amp;#34;hdfs://172.</description>
    </item>
    
    <item>
      <title>Hive</title>
      <link>//paimon.apache.org/docs/0.8/engines/hive/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/engines/hive/</guid>
      <description>Hive #  This documentation is a guide for using Paimon in Hive.
Version #  Paimon currently supports Hive 3.1, 2.3, 2.2, 2.1 and 2.1-cdh-6.3.
Execution Engine #  Paimon currently supports MR and Tez execution engine for Hive Read, and MR execution engine for Hive Write. Note If you use beeline, please restart the hive cluster.
Installation #  Download the jar file with corresponding version.
    Jar     Hive 3.</description>
    </item>
    
    <item>
      <title>Manage Partition</title>
      <link>//paimon.apache.org/docs/0.8/maintenance/manage-partition/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/maintenance/manage-partition/</guid>
      <description>Expiring Partitions #  You can set partition.expiration-time when creating a partitioned table. Paimon will periodically check the status of partitions and delete expired partitions according to time.
How to determine whether a partition has expired: compare the time extracted from the partition with the current time to see if survival time has exceeded the partition.expiration-time.
Note: After the partition expires, it is logically deleted and the latest snapshot cannot query its data.</description>
    </item>
    
    <item>
      <title>Pulsar CDC</title>
      <link>//paimon.apache.org/docs/0.8/flink/cdc-ingestion/pulsar-cdc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/flink/cdc-ingestion/pulsar-cdc/</guid>
      <description>Pulsar CDC #  Prepare Pulsar Bundled Jar #  flink-connector-pulsar-*.jar Supported Formats #  Flink provides several Pulsar CDC formats: Canal, Debezium, Ogg and Maxwell JSON. If a message in a pulsar topic is a change event captured from another database using the Change Data Capture (CDC) tool, then you can use the Paimon Pulsar CDC. Write the INSERT, UPDATE, DELETE messages parsed into the paimon table.
  Formats Supported     Canal CDC True   Debezium CDC True   Maxwell CDC True   OGG CDC True    The JSON sources possibly missing some information.</description>
    </item>
    
    <item>
      <title>Sequence &amp; Rowkind</title>
      <link>//paimon.apache.org/docs/0.8/primary-key-table/sequence-rowkind/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/primary-key-table/sequence-rowkind/</guid>
      <description>Sequence and Rowkind #  When creating a table, you can specify the &#39;sequence.field&#39; by specifying fields to determine the order of updates, or you can specify the &#39;rowkind.field&#39; to determine the changelog kind of record.
Sequence Field #  By default, the primary key table determines the merge order according to the input order (the last input record will be the last to merge). However, in distributed computing, there will be some cases that lead to data disorder.</description>
    </item>
    
    <item>
      <title>SQL Lookup</title>
      <link>//paimon.apache.org/docs/0.8/flink/sql-lookup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/flink/sql-lookup/</guid>
      <description>Lookup Joins #  Lookup Joins are a type of join in streaming queries. It is used to enrich a table with data that is queried from Paimon. The join requires one table to have a processing time attribute and the other table to be backed by a lookup source connector.
Paimon supports lookup joins on tables with primary keys and append tables in Flink. The following example illustrates this feature.</description>
    </item>
    
    <item>
      <title>StarRocks</title>
      <link>//paimon.apache.org/docs/0.8/engines/starrocks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/engines/starrocks/</guid>
      <description>StarRocks #  This documentation is a guide for using Paimon in StarRocks.
Version #  Paimon currently supports StarRocks 3.1 and above. Recommended version is StarRocks 3.2.6 or above.
Create Paimon Catalog #  Paimon catalogs are registered by executing a CREATE EXTERNAL CATALOG SQL in StarRocks. For example, you can use the following SQL to create a Paimon catalog named paimon_catalog.
CREATE EXTERNAL CATALOG paimon_catalog PROPERTIES ( &amp;#34;type&amp;#34; = &amp;#34;paimon&amp;#34;, &amp;#34;paimon.</description>
    </item>
    
    <item>
      <title>Trino</title>
      <link>//paimon.apache.org/docs/0.8/engines/trino/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/engines/trino/</guid>
      <description>Trino #  This documentation is a guide for using Paimon in Trino.
Version #  Paimon currently supports Trino 420 and above.
Filesystem #  From version 0.8, paimon share trino filesystem for all actions, which means, you should config trino filesystem before using trino-paimon. You can find information about how to config filesystems for trino on trino official website.
Preparing Paimon Jar File #  Download from master: https://paimon.</description>
    </item>
    
    <item>
      <title>Deletion Vectors</title>
      <link>//paimon.apache.org/docs/0.8/primary-key-table/deletion-vectors/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/primary-key-table/deletion-vectors/</guid>
      <description>Deletion Vectors #  Overview #  The Deletion Vectors mode is designed to takes into account both data reading and writing efficiency.
In this mode, additional overhead (looking up LSM Tree and generating the corresponding Deletion File) will be introduced during writing, but during reading, data can be directly retrieved by employing data with deletion vectors, avoiding additional merge costs between different files.
Furthermore, data reading concurrency is no longer limited, and non-primary key columns can also be used for filter push down.</description>
    </item>
    
    <item>
      <title>Presto</title>
      <link>//paimon.apache.org/docs/0.8/engines/presto/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/engines/presto/</guid>
      <description>Presto #  This documentation is a guide for using Paimon in Presto.
Version #  Paimon currently supports Presto 0.236 and above.
Preparing Paimon Jar File #  Download from master: https://paimon.apache.org/docs/master/project/download/ You can also manually build a bundled jar from the source code.
To build from the source code, clone the git repository.
Build presto connector plugin with the following command.
mvn clean install -DskipTests After the packaging is complete, you can choose the corresponding connector based on your own Presto version:</description>
    </item>
    
    <item>
      <title>SQL Alter</title>
      <link>//paimon.apache.org/docs/0.8/flink/sql-alter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/flink/sql-alter/</guid>
      <description>Altering Tables #  Changing/Adding Table Properties #  The following SQL sets write-buffer-size table property to 256 MB.
ALTER TABLE my_table SET ( &amp;#39;write-buffer-size&amp;#39; = &amp;#39;256 MB&amp;#39; ); Rename Table Name #  The following SQL rename the table name to new name.
ALTER TABLE my_table RENAME TO my_table_new; If you use object storage, such as S3 or OSS, please use this syntax carefully, because the renaming of object storage is not atomic, and only partial files may be moved in case of failure.</description>
    </item>
    
    <item>
      <title>SQL Alter</title>
      <link>//paimon.apache.org/docs/0.8/spark/sql-alter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/spark/sql-alter/</guid>
      <description>Altering Tables #  Changing/Adding Table Properties #  The following SQL sets write-buffer-size table property to 256 MB.
ALTER TABLE my_table SET TBLPROPERTIES ( &amp;#39;write-buffer-size&amp;#39; = &amp;#39;256 MB&amp;#39; ); Rename Table Name #  The following SQL rename the table name to new name.
The simplest sql to call is:
ALTER TABLE my_table RENAME TO my_table_new; Note that: we can rename paimon table in spark this way:
ALTER TABLE [catalog.</description>
    </item>
    
    <item>
      <title>Read Optimized</title>
      <link>//paimon.apache.org/docs/0.8/primary-key-table/read-optimized/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/primary-key-table/read-optimized/</guid>
      <description>Read Optimized #  Overview #  For Primary Key Table, it&amp;rsquo;s a &amp;lsquo;MergeOnRead&amp;rsquo; technology. When reading data, multiple layers of LSM data are merged, and the number of parallelism will be limited by the number of buckets. Although Paimon&amp;rsquo;s merge performance is efficient, it still cannot catch up with the ordinary AppendOnly table.
We recommend that you use Deletion Vectors mode.
If you don&amp;rsquo;t want to use Deletion Vectors mode, you want to query fast enough in certain scenarios, but can only find older data, you can also:</description>
    </item>
    
    <item>
      <title>Rescale Bucket</title>
      <link>//paimon.apache.org/docs/0.8/maintenance/rescale-bucket/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/maintenance/rescale-bucket/</guid>
      <description>Rescale Bucket #  Since the number of total buckets dramatically influences the performance, Paimon allows users to tune bucket numbers by ALTER TABLE command and reorganize data layout by INSERT OVERWRITE without recreating the table/partition. When executing overwrite jobs, the framework will automatically scan the data with the old bucket number and hash the record according to the current bucket number.
Rescale Overwrite #  -- rescale number of total buckets ALTER TABLE table_identifier SET (&amp;#39;bucket&amp;#39; = &amp;#39;.</description>
    </item>
    
    <item>
      <title>Manage Tags</title>
      <link>//paimon.apache.org/docs/0.8/maintenance/manage-tags/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/maintenance/manage-tags/</guid>
      <description>Manage Tags #  Paimon&amp;rsquo;s snapshots can provide an easy way to query historical data. But in most scenarios, a job will generate too many snapshots and table will expire old snapshots according to table configuration. Snapshot expiration will also delete old data files, and the historical data of expired snapshots cannot be queried anymore.
To solve this problem, you can create a tag based on a snapshot. The tag will maintain the manifests and data files of the snapshot.</description>
    </item>
    
    <item>
      <title>Metrics</title>
      <link>//paimon.apache.org/docs/0.8/maintenance/metrics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/maintenance/metrics/</guid>
      <description>Paimon Metrics #  Paimon has built a metrics system to measure the behaviours of reading and writing, like how many manifest files it scanned in the last planning, how long it took in the last commit operation, how many files it deleted in the last compact operation.
In Paimon&amp;rsquo;s metrics system, metrics are updated and reported at table granularity.
There are three types of metrics provided in the Paimon metric system, Gauge, Counter, Histogram.</description>
    </item>
    
    <item>
      <title>Manage Privileges</title>
      <link>//paimon.apache.org/docs/0.8/maintenance/manage-privileges/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/maintenance/manage-privileges/</guid>
      <description>Manage Privileges #  Paimon provides a privilege system on catalogs. Privileges determine which users can perform which operations on which objects, so that you can manage table access in a fine-grained manner.
Currently, Paimon adopts the identity-based access control (IBAC) privilege model. That is, privileges are directly assigned to users.
This privilege system only prevents unwanted users from accessing tables through catalogs. It does not block access through temporary table (by specifying table path on filesystem), nor does it prevent user from directly modifying data files on filesystem.</description>
    </item>
    
    <item>
      <title>Procedures</title>
      <link>//paimon.apache.org/docs/0.8/flink/procedures/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/flink/procedures/</guid>
      <description>Procedures #  Flink 1.18 and later versions support Call Statements, which make it easier to manipulate data and metadata of Paimon table by writing SQLs instead of submitting Flink jobs.
In 1.18, the procedure only supports passing arguments by position. You must pass all arguments in order, and if you don&amp;rsquo;t want to pass some arguments, you must use &#39;&#39; as placeholder. For example, if you want to compact table default.</description>
    </item>
    
    <item>
      <title>Action Jars</title>
      <link>//paimon.apache.org/docs/0.8/flink/action-jars/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/flink/action-jars/</guid>
      <description>Action Jars #  After the Flink Local Cluster has been started, you can execute the action jar by using the following command.
&amp;lt;FLINK_HOME&amp;gt;/bin/flink run \ /path/to/paimon-flink-action-0.8.2.jar \ &amp;lt;action&amp;gt; &amp;lt;args&amp;gt; The following command is used to compact a table.
&amp;lt;FLINK_HOME&amp;gt;/bin/flink run \ /path/to/paimon-flink-action-0.8.2.jar \ compact \ --path &amp;lt;TABLE_PATH&amp;gt; Merging into table #  Paimon supports &amp;ldquo;MERGE INTO&amp;rdquo; via submitting the &amp;lsquo;merge_into&amp;rsquo; job through flink run.
Important table properties setting:
 Only primary key table supports this feature.</description>
    </item>
    
    <item>
      <title>Procedures</title>
      <link>//paimon.apache.org/docs/0.8/spark/procedures/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/spark/procedures/</guid>
      <description>Procedures #  This section introduce all available spark procedures about paimon.
  Procedure Name Explanation Example    compact  To compact files. Argument: table: the target table identifier. Cannot be empty. partitions: partition filter. &#34;,&#34; means &#34;AND&#34;
&#34;;&#34; means &#34;OR&#34;.If you want to compact one partition with date=01 and day=01, you need to write &#39;date=01,day=01&#39;. Left empty for all partitions. (Can&#39;t be used together with &#34;where&#34;) where: partition predicate.</description>
    </item>
    
    <item>
      <title>Savepoint</title>
      <link>//paimon.apache.org/docs/0.8/flink/savepoint/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/flink/savepoint/</guid>
      <description>Savepoint #  Paimon has its own snapshot management, this may conflict with Flink&amp;rsquo;s checkpoint management, causing exceptions when restoring from savepoint (don&amp;rsquo;t worry, it will not cause the storage to be damaged).
It is recommended that you use the following methods to savepoint:
 Use Flink Stop with savepoint. Use Paimon Tag with Flink Savepoint, and rollback-to-tag before restoring from savepoint.  Stop with savepoint #  This feature of Flink ensures that the last checkpoint is fully processed, which means there will be no more uncommitted metadata left.</description>
    </item>
    
    <item>
      <title>Configurations</title>
      <link>//paimon.apache.org/docs/0.8/maintenance/configurations/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/maintenance/configurations/</guid>
      <description>Configuration #  CoreOptions #  Core options for paimon.
  Key Default Type Description     auto-create false Boolean Whether to create underlying storage when reading and writing the table.   bucket -1 Integer Bucket number for file store.
It should either be equal to -1 (dynamic bucket mode), or it must be greater than 0 (fixed bucket mode).   bucket-key (none) String Specify the paimon distribution policy.</description>
    </item>
    
    <item>
      <title>Versions</title>
      <link>//paimon.apache.org/docs/0.8/versions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/versions/</guid>
      <description>Versions #  An appendix of hosted documentation for all versions of Apache Paimon.
 master    stable    0.8    </description>
    </item>
    
  </channel>
</rss>
