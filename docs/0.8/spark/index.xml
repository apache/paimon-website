<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Engine Spark on Apache Paimon</title>
    <link>//paimon.apache.org/docs/0.8/spark/</link>
    <description>Recent content in Engine Spark on Apache Paimon</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="//paimon.apache.org/docs/0.8/spark/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Quick Start</title>
      <link>//paimon.apache.org/docs/0.8/spark/quick-start/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/spark/quick-start/</guid>
      <description>Quick Start #  Preparation #  Paimon currently supports Spark 3.5, 3.4, 3.3, 3.2 and 3.1. We recommend the latest Spark version for a better experience.
Download the jar file with corresponding version.
   Version Jar     Spark 3.5 paimon-spark-3.5-0.8.2.jar   Spark 3.4 paimon-spark-3.4-0.8.2.jar   Spark 3.3 paimon-spark-3.3-0.8.2.jar   Spark 3.2 paimon-spark-3.2-0.8.2.jar   Spark 3.1 paimon-spark-3.1-0.8.2.jar    You can also manually build bundled jar from the source code.</description>
    </item>
    
    <item>
      <title>SQL DDL</title>
      <link>//paimon.apache.org/docs/0.8/spark/sql-ddl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/spark/sql-ddl/</guid>
      <description>SQL DDL #  Create Catalog #  Paimon catalogs currently support three types of metastores:
 filesystem metastore (default), which stores both metadata and table files in filesystems. hive metastore, which additionally stores metadata in Hive metastore. Users can directly access the tables from Hive. jdbc metastore, which additionally stores metadata in relational databases such as MySQL, Postgres, etc.  See CatalogOptions for detailed options when creating a catalog.</description>
    </item>
    
    <item>
      <title>SQL Write</title>
      <link>//paimon.apache.org/docs/0.8/spark/sql-write/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/spark/sql-write/</guid>
      <description>SQL Write #  Syntax #  INSERT { INTO | OVERWRITE } table_identifier [ part_spec ] [ column_list ] { value_expr | query }; For more information, please check the syntax document:
Spark INSERT Statement
INSERT INTO #  Use INSERT INTO to apply records and changes to tables.
INSERT INTO my_table SELECT ... Overwriting the Whole Table #  Use INSERT OVERWRITE to overwrite the whole unpartitioned table.</description>
    </item>
    
    <item>
      <title>SQL Query</title>
      <link>//paimon.apache.org/docs/0.8/spark/sql-query/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/spark/sql-query/</guid>
      <description>SQL Query #  Just like all other tables, Paimon tables can be queried with SELECT statement.
Batch Query #  Paimon&amp;rsquo;s batch read returns all the data in a snapshot of the table. By default, batch reads return the latest snapshot.
Batch Time Travel #  Paimon batch reads with time travel can specify a snapshot or a tag and read the corresponding data.
Requires Spark 3.3+.
you can use VERSION AS OF and TIMESTAMP AS OF in query to do time travel:</description>
    </item>
    
    <item>
      <title>SQL Alter</title>
      <link>//paimon.apache.org/docs/0.8/spark/sql-alter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/spark/sql-alter/</guid>
      <description>Altering Tables #  Changing/Adding Table Properties #  The following SQL sets write-buffer-size table property to 256 MB.
ALTER TABLE my_table SET TBLPROPERTIES ( &amp;#39;write-buffer-size&amp;#39; = &amp;#39;256 MB&amp;#39; ); Rename Table Name #  The following SQL rename the table name to new name.
The simplest sql to call is:
ALTER TABLE my_table RENAME TO my_table_new; Note that: we can rename paimon table in spark this way:
ALTER TABLE [catalog.</description>
    </item>
    
    <item>
      <title>Procedures</title>
      <link>//paimon.apache.org/docs/0.8/spark/procedures/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/0.8/spark/procedures/</guid>
      <description>Procedures #  This section introduce all available spark procedures about paimon.
  Procedure Name Explanation Example    compact  To compact files. Argument: table: the target table identifier. Cannot be empty. partitions: partition filter. &#34;,&#34; means &#34;AND&#34;
&#34;;&#34; means &#34;OR&#34;.If you want to compact one partition with date=01 and day=01, you need to write &#39;date=01,day=01&#39;. Left empty for all partitions. (Can&#39;t be used together with &#34;where&#34;) where: partition predicate.</description>
    </item>
    
  </channel>
</rss>
