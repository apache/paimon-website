<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Engine Spark on Apache Paimon</title>
    <link>//paimon.apache.org/docs/1.3/spark/</link>
    <description>Recent content in Engine Spark on Apache Paimon</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="//paimon.apache.org/docs/1.3/spark/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Quick Start</title>
      <link>//paimon.apache.org/docs/1.3/spark/quick-start/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/1.3/spark/quick-start/</guid>
      <description>Quick Start #  Preparation #  Paimon currently supports Spark versions 4.x (including 4.0) and 3.x (including 3.5, 3.4, 3.3, and 3.2). We recommend using the latest Spark version for a better experience.
Note that Spark 4.x is pre-built with Java 17 and Scala 2.13. Spark 3.x is pre-built with Java 8 and Scala 2.12.
Download the jar file with corresponding version.
   Version Jar     Spark 4.</description>
    </item>
    
    <item>
      <title>SQL DDL</title>
      <link>//paimon.apache.org/docs/1.3/spark/sql-ddl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/1.3/spark/sql-ddl/</guid>
      <description>SQL DDL #  Catalog #  Create Catalog #  Paimon catalogs currently support three types of metastores:
 filesystem metastore (default), which stores both metadata and table files in filesystems. hive metastore, which additionally stores metadata in Hive metastore. Users can directly access the tables from Hive. jdbc metastore, which additionally stores metadata in relational databases such as MySQL, Postgres, etc.  See CatalogOptions for detailed options when creating a catalog.</description>
    </item>
    
    <item>
      <title>SQL Functions</title>
      <link>//paimon.apache.org/docs/1.3/spark/sql-functions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/1.3/spark/sql-functions/</guid>
      <description>SQL Functions #  This section introduce all available Paimon Spark functions.
Built-in Function #  max_pt #  sys.max_pt($table_name)
It accepts a string type literal to specify the table name and return a max-valid-toplevel partition value.
 valid: the partition which contains data files toplevel: only return the first partition value if the table has multi-partition columns  It would throw exception when:
 the table is not a partitioned table the partitioned table does not have partition all of the partitions do not contains data files  Example</description>
    </item>
    
    <item>
      <title>SQL Write</title>
      <link>//paimon.apache.org/docs/1.3/spark/sql-write/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/1.3/spark/sql-write/</guid>
      <description>SQL Write #  Insert Table #  The INSERT statement inserts new rows into a table or overwrites the existing data in the table. The inserted rows can be specified by value expressions or result from a query.
Syntax
INSERT { INTO | OVERWRITE } table_identifier [ part_spec ] [ column_list ] { value_expr | query }; Parameters
  table_identifier: Specifies a table name, which may be optionally qualified with a database name.</description>
    </item>
    
    <item>
      <title>SQL Query</title>
      <link>//paimon.apache.org/docs/1.3/spark/sql-query/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/1.3/spark/sql-query/</guid>
      <description>SQL Query #  Just like all other tables, Paimon tables can be queried with SELECT statement.
Batch Query #  Paimon&amp;rsquo;s batch read returns all the data in a snapshot of the table. By default, batch reads return the latest snapshot.
-- read all columns SELECT * FROM t; Paimon also supports reading some hidden metadata columns, currently supporting the following columns:
 __paimon_file_path: the file path of the record.</description>
    </item>
    
    <item>
      <title>SQL Alter</title>
      <link>//paimon.apache.org/docs/1.3/spark/sql-alter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/1.3/spark/sql-alter/</guid>
      <description>Altering Tables #  Changing/Adding Table Properties #  The following SQL sets write-buffer-size table property to 256 MB.
ALTER TABLE my_table SET TBLPROPERTIES ( &amp;#39;write-buffer-size&amp;#39; = &amp;#39;256 MB&amp;#39; ); Removing Table Properties #  The following SQL removes write-buffer-size table property.
ALTER TABLE my_table UNSET TBLPROPERTIES (&amp;#39;write-buffer-size&amp;#39;); Changing/Adding Table Comment #  The following SQL changes comment of table my_table to table comment.
ALTER TABLE my_table SET TBLPROPERTIES ( &amp;#39;comment&amp;#39; = &amp;#39;table comment&amp;#39; ); Removing Table Comment #  The following SQL removes table comment.</description>
    </item>
    
    <item>
      <title>Auxiliary</title>
      <link>//paimon.apache.org/docs/1.3/spark/auxiliary/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/1.3/spark/auxiliary/</guid>
      <description>Auxiliary Statements #  Set / Reset #  The SET command sets a property, returns the value of an existing property or returns all SQLConf properties with value and meaning. The RESET command resets runtime configurations specific to the current session which were set via the SET command to their default values.
To set dynamic options globally, you need add the spark.paimon. prefix. You can also set dynamic table options at this format: spark.</description>
    </item>
    
    <item>
      <title>Default Value</title>
      <link>//paimon.apache.org/docs/1.3/spark/default-value/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/1.3/spark/default-value/</guid>
      <description>Default Value #  Paimon allows specifying default values for columns. When users write to these tables without explicitly providing values for certain columns, Paimon automatically generates default values for these columns.
Create Table #  You can create a table with columns with default values using the following SQL:
CREATE TABLE my_table ( a BIGINT, b STRING DEFAULT &amp;#39;my_value&amp;#39;, c INT DEFAULT 5, tags ARRAY&amp;lt;STRING&amp;gt; DEFAULT ARRAY(&amp;#39;tag1&amp;#39;, &amp;#39;tag2&amp;#39;, &amp;#39;tag3&amp;#39;), properties MAP&amp;lt;STRING, STRING&amp;gt; DEFAULT MAP(&amp;#39;key1&amp;#39;, &amp;#39;value1&amp;#39;, &amp;#39;key2&amp;#39;, &amp;#39;value2&amp;#39;), nested STRUCT&amp;lt;x: INT, y: STRING&amp;gt; DEFAULT STRUCT(42, &amp;#39;default_value&amp;#39;) ); Insert Table #  For SQL commands that execute table writes, such as the INSERT, UPDATE, and MERGE commands, the DEFAULT keyword or NULL value is parsed into the default value specified for the corresponding column.</description>
    </item>
    
    <item>
      <title>DataFrame</title>
      <link>//paimon.apache.org/docs/1.3/spark/dataframe/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/1.3/spark/dataframe/</guid>
      <description>DataFrame #  Paimon supports creating table, inserting data, and querying through the Spark DataFrame API.
Create Table #  You can specify table properties with option or set partition columns with partitionBy if needed.
val data: DataFrame = Seq((1, &amp;#34;x1&amp;#34;, &amp;#34;p1&amp;#34;), (2, &amp;#34;x2&amp;#34;, &amp;#34;p2&amp;#34;)).toDF(&amp;#34;a&amp;#34;, &amp;#34;b&amp;#34;, &amp;#34;pt&amp;#34;) data.write.format(&amp;#34;paimon&amp;#34;) .option(&amp;#34;primary-key&amp;#34;, &amp;#34;a,pt&amp;#34;) .option(&amp;#34;k1&amp;#34;, &amp;#34;v1&amp;#34;) .partitionBy(&amp;#34;pt&amp;#34;) .saveAsTable(&amp;#34;test_tbl&amp;#34;) // or .save(&amp;#34;/path/to/default.db/test_tbl&amp;#34;) Insert #  Insert Into #  You can achieve INSERT INTO semantics by setting the mode to append.</description>
    </item>
    
    <item>
      <title>SQL Upsert</title>
      <link>//paimon.apache.org/docs/1.3/spark/sql-upsert/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/1.3/spark/sql-upsert/</guid>
      <description>SQL Upsert #  For table without primary key, Paimon supports upsert write mode: If the row with the same upsert key already exists, perform update; otherwise, perform insert.
Usage #  Specify the following table properties when creating the table
  upsert-key: Defines the key columns used for upsert, cannot be used together with primary key. Unlike primary key, the upsert key value can be null, and null-equality matching is supported.</description>
    </item>
    
    <item>
      <title>Structured Streaming</title>
      <link>//paimon.apache.org/docs/1.3/spark/structured-streaming/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/1.3/spark/structured-streaming/</guid>
      <description>Structured Streaming #  Paimon supports streaming data processing with Spark Structured Streaming, enabling both streaming write and streaming query.
Streaming Write #  Paimon Structured Streaming only supports the two append and complete modes.  // Create a paimon table if not exists. spark.sql(s&amp;#34;&amp;#34;&amp;#34; |CREATE TABLE T (k INT, v STRING) |TBLPROPERTIES (&amp;#39;primary-key&amp;#39;=&amp;#39;k&amp;#39;, &amp;#39;bucket&amp;#39;=&amp;#39;3&amp;#39;) |&amp;#34;&amp;#34;&amp;#34;.stripMargin) // Here we use MemoryStream to fake a streaming source. val inputData = MemoryStream[(Int, String)] val df = inputData.</description>
    </item>
    
    <item>
      <title>Procedures</title>
      <link>//paimon.apache.org/docs/1.3/spark/procedures/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//paimon.apache.org/docs/1.3/spark/procedures/</guid>
      <description>Procedures #  This section introduce all available spark procedures about paimon.
  Procedure Name Explanation Example    compact  To compact files. Argument: table: the target table identifier. Cannot be empty. partitions: partition filter. the comma (&#34;,&#34;) represents &#34;AND&#34;, the semicolon (&#34;;&#34;) represents &#34;OR&#34;. If you want to compact one partition with date=01 and day=01, you need to write &#39;date=01,day=01&#39;. Left empty for all partitions. (Can&#39;t be used together with &#34;</description>
    </item>
    
  </channel>
</rss>
