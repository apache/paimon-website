{"title":"Release 0.5","type":"release","content":"<p>September 06, 2023 - Jingsong Lee (<a href=\"mailto:jingsonglee0@gmail.com\" target=\"_blank\" title=\"undefined\" rel=\"noopener\">jingsonglee0@gmail.com</a>)</p>\n<p>We are happy to announce the availability of Paimon <a href=\"https://paimon.apache.org/docs/0.5/\" target=\"_blank\" title=\"null\" rel=\"noopener\">0.5.0-incubating</a>.</p>\n<p>Nearly 100 contributors have come to contribute release-0.5, we created 500+ commits together, bringing many exciting\nnew features and improvements to the community. Thank you all for your joint efforts!</p>\n<p>Highlight:</p>\n<ul>\n<li>CDC Data Ingestion into Lake has reached maturity.</li>\n<li>Introduce Tags to provide immutable view to Offline data warehouse. </li>\n<li>Dynamic Bucket mode for Primary Key Table is available in production.</li>\n<li>Introduce Append Only Scalable Table to replace Hive table.</li>\n</ul>\n<h2 id=\"cdc-ingestion\">CDC Ingestion</h2><p>Paimon supports a variety of ways to <a href=\"https://paimon.apache.org/docs/0.5/how-to/cdc-ingestion/\" target=\"_blank\" title=\"null\" rel=\"noopener\">ingest data into Paimon</a>\ntables with schema evolution. In release 0.5, a large number of new features have been added:</p>\n<ul>\n<li>MySQL Synchronizing Table<ul>\n<li>support synchronizing shards into one Paimon table</li>\n<li>support type-mapping to make all fields to string</li>\n</ul>\n</li>\n<li>MySQL Synchronizing Database<ul>\n<li>support merge multiple shards from multiple database</li>\n<li>support <code>--mode combined</code> to a unified sink to sync all tables, and sync newly added tables without restarting job</li>\n</ul>\n</li>\n<li>Kafka Synchronizing Table<ul>\n<li>synchronize one Kafka topicâ€™s table into one Paimon table.</li>\n<li>support Canal and OGG</li>\n</ul>\n</li>\n<li>Kafka Synchronizing Database<ul>\n<li>synchronize one Kafka topic containing multiple tables or multiple topics containing one table each into one Paimon database.</li>\n<li>support Canal and OGG</li>\n</ul>\n</li>\n<li>MongoDB Synchronizing Collection<ul>\n<li>synchronize one Collection from MongoDB into one Paimon table.</li>\n</ul>\n</li>\n<li>MongoDB Synchronizing Database <ul>\n<li>synchronize the whole MongoDB database into one Paimon database.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"primary-key-table\">Primary Key Table</h2><p>By specific Primary Key in creating table DDL, you can get a <a href=\"https://paimon.apache.org/docs/0.5/concepts/primary-key-table/\" target=\"_blank\" title=\"null\" rel=\"noopener\">Primary Key Table</a>,\nit accepts insert, update or delete records. </p>\n<h3 id=\"dynamic-bucket\">Dynamic Bucket</h3><p>Configure <code>&#39;bucket&#39; = &#39;-1&#39;</code>, Paimon dynamically maintains the index, automatic expansion of the number of buckets.</p>\n<ul>\n<li>Option1: <code>&#39;dynamic-bucket.target-row-num&#39;</code>: controls the target row number for one bucket.</li>\n<li>Option2: <code>&#39;dynamic-bucket.assigner-parallelism&#39;</code>: Parallelism of assigner operator, controls the number of initialized bucket.</li>\n</ul>\n<p>Dynamic Bucket mode uses HASH index to maintain mapping from key to bucket, it requires more memory than fixed bucket mode.\nFor performance:</p>\n<ol>\n<li>Generally speaking, there is no performance loss, but there will be some additional memory consumption, <strong>100 million</strong>\nentries in a partition takes up <strong>1 GB</strong> more memory, partitions that are no longer active do not take up memory.</li>\n<li>For tables with low update rates, this mode is recommended to significantly improve performance.</li>\n</ol>\n<h3 id=\"partial-update-sequence-group\">Partial-Update: Sequence Group</h3><p>A sequence-field may not solve the disorder problem of partial-update tables with multiple stream updates, because\nthe sequence-field may be overwritten by the latest data of another stream during multi-stream update. So we introduce\nsequence group mechanism for partial-update tables. It can solve:</p>\n<ol>\n<li>Disorder during multi-stream update. Each stream defines its own sequence-groups.</li>\n<li>A true partial-update, not just a non-null update.</li>\n<li>Accept delete records to retract partial columns.</li>\n</ol>\n<h3 id=\"first-row-merge-engine\">First Row Merge Engine</h3><p>By specifying <code>&#39;merge-engine&#39; = &#39;first-row&#39;</code>, users can keep the first row of the same primary key. It differs from the\n<code>deduplicate</code> merge engine that in the <code>first-row</code> merge engine, it will generate insert only changelog.</p>\n<p>This is of great help in replacing log deduplication in streaming computation.</p>\n<h3 id=\"lookup-changelog-producer\">Lookup Changelog-Producer</h3><p>Lookup Changelog-Producer is available in production, this can greatly reduce the delay for tables that need to\ngenerate changelogs.</p>\n<p>(Note: Please increase <code>&#39;execution.checkpointing.max-concurrent-checkpoints&#39;</code> Flink configuration, this is very\nimportant for performance).</p>\n<h3 id=\"sequence-auto-padding\">Sequence Auto Padding</h3><p>When the record is updated or deleted, the <code>sequence.field</code> must become larger and cannot remain unchanged.\nFor -U and +U, their sequence-fields must be different. If you cannot meet this requirement, Paimon provides\noption to automatically pad the sequence field for you.</p>\n<p>Configure <code>&#39;sequence.auto-padding&#39; = &#39;row-kind-flag&#39;</code>: If you are using same value for -U and +U, just like &quot;<code>op_ts</code>&quot;\n(the time that the change was made in the database) in Mysql Binlog. It is recommended to use the automatic\npadding for row kind flag, which will automatically distinguish between -U (-D) and +U (+I).</p>\n<h3 id=\"asynchronous-compaction\">Asynchronous Compaction</h3><p>Compaction is inherently asynchronous, but if you want it to be completely asynchronous and not blocking writing,\nexpect a mode to have maximum writing throughput, the compaction can be done slowly and not in a hurry.\nYou can use the following strategies for your table:</p>\n<pre class=\"code\"><code>num<span class=\"token operator\">-</span>sorted<span class=\"token operator\">-</span>run<span class=\"token punctuation\">.</span>stop<span class=\"token operator\">-</span>trigger <span class=\"token operator\">=</span> <span class=\"token number\">2147483647</span>\nsort<span class=\"token operator\">-</span>spill<span class=\"token operator\">-</span>threshold <span class=\"token operator\">=</span> <span class=\"token number\">10</span></code></pre><p>This configuration will generate more files during peak write periods and gradually merge into optimal read\nperformance during low write periods.</p>\n<h3 id=\"avro-file-format\">Avro File Format</h3><p>If you want to achieve ultimate compaction performance, you can consider using row storage file format AVRO.</p>\n<ul>\n<li>The advantage is that you can achieve high write throughput and compaction performance.</li>\n<li>The disadvantage is that your analysis queries will be slow, and the biggest problem with row storage is that it\ndoes not have the query projection. For example, if the table have 100 columns but only query a few columns, the\nIO of row storage cannot be ignored. Additionally, compression efficiency will decrease and storage costs will\nincrease.</li>\n</ul>\n<pre class=\"code\"><code>file<span class=\"token punctuation\">.</span>format <span class=\"token operator\">=</span> avro\nmetadata<span class=\"token punctuation\">.</span>stats<span class=\"token operator\">-</span>mode <span class=\"token operator\">=</span> none</code></pre><p>If you don&#39;t want to modify all files to Avro format, at least you can consider modifying the files in the previous\nlayers to Avro format. You can use <code>&#39;file.format.per.level&#39; = &#39;0:avro,1:avro&#39;</code> to specify the files in the first two\nlayers to be in Avro format.</p>\n<h2 id=\"append-only-table\">Append Only Table</h2><h3 id=\"append-only-scalable-table\">Append Only Scalable Table</h3><p>By defining <code>&#39;bucket&#39; = &#39;-1&#39;</code> to a non-pk table, you can assign an <a href=\"https://paimon.apache.org/docs/0.5/concepts/append-only-table/#append-for-scalable-table\" target=\"_blank\" title=\"null\" rel=\"noopener\">Append Only Scalable Table</a>.\nIn this mode, the table doesn&#39;t have the concept of bucket anymore, read and write are concurrent. We regard this table\nas a batch off-line table(although we can stream read and write still).</p>\n<p>Using this mode, you can replace your Hive table to lake table.</p>\n<p>We have auto small file compaction for this mode by default. And you can use <code>Sort Compact</code> action to sort whole partition,\nusing zorder sorter, this can greatly speed up data skipping when querying.</p>\n<h2 id=\"manage-tags\">Manage Tags</h2><p>Paimon&#39;s snapshots can provide an easy way to query historical data. But in most scenarios, a job will generate too many\nsnapshots and table will expire old snapshots according to table configuration. Snapshot expiration will also delete old\ndata files, and the historical data of expired snapshots cannot be queried anymore.</p>\n<p>To solve this problem, you can create a <a href=\"https://paimon.apache.org/docs/0.5/maintenance/manage-tags/\" target=\"_blank\" title=\"null\" rel=\"noopener\">Tag</a> based on a\nsnapshot. The tag will maintain the manifests and data files of the snapshot. A typical usage is creating tags daily,\nthen you can maintain the historical data of each day for batch reading.</p>\n<p>Paimon supports automatic creation of tags in writing job. You can use <code>&#39;tag.automatic-creation&#39;</code>to create tags automatically.</p>\n<p>And you can query the incremental data of Tags (or snapshots) too, both Flink and Spark support incremental queries. </p>\n<h2 id=\"engines\">Engines</h2><h3 id=\"flink\">Flink</h3><p>After Flink released <a href=\"https://flink.apache.org/2023/03/23/announcing-the-release-of-apache-flink-1.17/\" target=\"_blank\" title=\"null\" rel=\"noopener\">1.17</a>, Paimon\nunderwent very in-depth integration.</p>\n<ul>\n<li><a href=\"https://paimon.apache.org/docs/0.5/how-to/altering-tables/\" target=\"_blank\" title=\"null\" rel=\"noopener\">ALTER TABLE</a> syntax is enhanced by including the\nability to ADD/MODIFY/DROP columns, making it easier for users to maintain their table schema.</li>\n<li><a href=\"https://paimon.apache.org/docs/0.5/engines/flink/#quick-start\" target=\"_blank\" title=\"null\" rel=\"noopener\">FlinkGenericCatalog</a>, you need to use Hive metastore. \nThen, you can use all the tables from Paimon, Hive, and Flink Generic Tables (Kafka and other tables)!</li>\n<li><a href=\"https://paimon.apache.org/docs/0.5/how-to/writing-tables/#dynamic-overwrite\" target=\"_blank\" title=\"null\" rel=\"noopener\">Dynamic Partition Overwrite</a> Flinkâ€™s\ndefault overwrite mode is dynamic partition overwrite (that means Paimon only deletes the partitions appear in the\noverwritten data). You can configure dynamic-partition-overwrite to change it to static overwritten.</li>\n<li><a href=\"https://paimon.apache.org/docs/0.5/how-to/creating-catalogs/#synchronizing-partitions-into-hive-metastore\" target=\"_blank\" title=\"null\" rel=\"noopener\">Sync Partitions into Hive Metastore</a>\nBy default, Paimon does not synchronize newly created partitions into Hive metastore. If you want to see a partitioned\ntable in Hive and also synchronize newly created partitions into Hive metastore, please set the table property <code>metastore.partitioned-table</code> to true.</li>\n<li><a href=\"https://paimon.apache.org/docs/0.5/how-to/lookup-joins/\" target=\"_blank\" title=\"null\" rel=\"noopener\">Retry Lookup Join</a> support Retry Lookup and Async Retry Lookup.</li>\n</ul>\n<h3 id=\"spark\">Spark</h3><p>Spark is another computing engine that Paimon has in-depth integration and has taken a big step forward at 0.5, including the following features:</p>\n<ul>\n<li><a href=\"https://paimon.apache.org/docs/0.5/how-to/writing-tables/#overwriting-the-whole-table\" target=\"_blank\" title=\"null\" rel=\"noopener\">INSERT OVERWRITE</a> insert ovewrite\npartition, Sparkâ€™s default overwrite mode is static partition overwrite, you can enable dynamic overwritten too.</li>\n<li>Partition Management: Support <code>DROP PARTITION</code>, <code>SHOW PARTITIONS</code>.</li>\n<li>Supports saving a DataFrame to a paimon location.</li>\n<li>Schema merging write: You can set <code>write.merge-schema</code> to true to write with schema merging.</li>\n<li>Streaming sink: You can use Spark streaming <code>foreachBatch</code> API to streaming sink to Paimon.</li>\n</ul>\n<h2 id=\"download\">Download</h2><p>Download the release <a href=\"https://paimon.apache.org/docs/0.5/project/download/\" target=\"_blank\" title=\"null\" rel=\"noopener\">here</a>.</p>\n<h2 id=\"whats-next\">What's next?</h2><p>Paimon will be committed to solving the following scenarios for a long time:</p>\n<ol>\n<li>Acceleration of CDC data into the lake: real-time writing, real-time query, and offline immutable partition view by using Tags.</li>\n<li>Enrich Merge Engines to improve streaming computation: Partial-Update table, Aggregation table, First Row table.</li>\n<li>Changelog Streaming read, build incremental stream processing based on lake storage.</li>\n<li>Append mode accelerates Hive offline tables, writes in real time and brings query acceleration after sorting.</li>\n<li>Append mode replaces some message queue scenarios, stream reads in input order, and without data TTL.</li>\n</ol>\n","toc":[{"depth":2,"text":"CDC Ingestion","id":"cdc-ingestion"},{"depth":2,"text":"Primary Key Table","id":"primary-key-table"},{"depth":3,"text":"Dynamic Bucket","id":"dynamic-bucket"},{"depth":3,"text":"Partial-Update: Sequence Group","id":"partial-update-sequence-group"},{"depth":3,"text":"First Row Merge Engine","id":"first-row-merge-engine"},{"depth":3,"text":"Lookup Changelog-Producer","id":"lookup-changelog-producer"},{"depth":3,"text":"Sequence Auto Padding","id":"sequence-auto-padding"},{"depth":3,"text":"Asynchronous Compaction","id":"asynchronous-compaction"},{"depth":3,"text":"Avro File Format","id":"avro-file-format"},{"depth":2,"text":"Append Only Table","id":"append-only-table"},{"depth":3,"text":"Append Only Scalable Table","id":"append-only-scalable-table"},{"depth":2,"text":"Manage Tags","id":"manage-tags"},{"depth":2,"text":"Engines","id":"engines"},{"depth":3,"text":"Flink","id":"flink"},{"depth":3,"text":"Spark","id":"spark"},{"depth":2,"text":"Download","id":"download"},{"depth":2,"text":"What's next?","id":"whats-next"}],"alias":"release-0.5","version":"0.5.0","weight":50}