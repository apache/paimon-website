{"title":"Release 0.9","type":"release","content":"<p>Sep 13, 2024 - Jingsong Lee (<a href=\"mailto:jingsonglee0@gmail.com\" target=\"_blank\" title=\"undefined\" rel=\"noopener\">jingsonglee0@gmail.com</a>)</p>\n<p>The Apache Paimon PMC officially announces the release of Apache Paimon 0.9.0. This version was developed over the\ncourse of four months, with the participation of 80 contributors, resulting in more than 600 commits.</p>\n<p>Thank you to all contributors for your support!</p>\n<p>The community has decided that the next version will be 1.0, marking that most functionalities of Apache Paimon have\nbecome relatively mature and stable.</p>\n<h2 id=\"version-overview\">Version Overview</h2><p>Paimon&#39;s long-term plan is to become a unified lake storage format that meets the main requirements for minute level\nbig data: offline batch computing, real-time stream computing, and OLAP computing.</p>\n<p>Notable changes in this version include:</p>\n<ol>\n<li>Paimon Branch: The branch functionality is now officially production-ready, and the introduction of the &#39;scan.fallback-branch&#39;\nfeature helps to better unify stream and batch storage for businesses.</li>\n<li>Universal Format: This version introduces native Iceberg compatibility. You can enable Iceberg compatibility mode, and\nPaimon will generate Iceberg-compatible snapshots in real-time, allowing you to use the Iceberg ecosystem to read this Paimon table.</li>\n<li>Caching Catalog: This version introduces the implementation of Caching Catalog by default. Table metadata and manifest\nfiles will be cached in the catalog, which can accelerate OLAP query performance.</li>\n<li>Improvements in Bucketed Append Table Availability: The small file issue has been significantly alleviated, and it\ncan be applied to bucketed joins in Spark, reducing shuffles during joins.</li>\n<li>Support for DELETE &amp; UPDATE &amp; MERGEINTO in Append Tables: This version introduces support for DELETE, UPDATE, and\nMERGE INTO in append tables. You can modify and delete records in append tables using Spark SQL, and it also supports\nDeletion Vectors mode.</li>\n</ol>\n<h2 id=\"compatibility-changes\">Compatibility Changes</h2><p>The following changes may impact the compatibility of your usage.</p>\n<h3 id=\"bucketed-append-tables\">Bucketed Append Tables</h3><p>When defining a table without a primary key, if the number of &#39;bucket&#39; is defined, the table is considered a Bucketed \nAppend table, previously referred to as an Append Queue table, as it is more commonly used in ordered stream writes and\nreads. The issue of small files has been significantly alleviated, and it can be applied in Spark for Bucketed Joins, \nreducing shuffles during joins.</p>\n<p>Here are some changes to its default values:</p>\n<ol>\n<li>Bucketed Append tables are prohibited from being defined without a bucket-key. The behavior in previous versions was\nto hash the entire row to determine the corresponding bucket, which is an unintuitive behavior. We recommend using\nan older version of Paimon to refresh the data (write into a new valid table).</li>\n<li>The default value of the &#39;compaction.max.file-num&#39; option for Bucketed Append tables has been adjusted to 5, meaning\nthere will be fewer small files within a single bucket to avoid excessive small files affecting production usability.</li>\n</ol>\n<p>Despite this, we still recommend that you avoid defining Bucketed Append tables unless necessary; the default bucket -1\nmode is more user-friendly.</p>\n<h3 id=\"file-format-and-compression\">File Format and Compression</h3><p>The Paimon community is focused on improving overall performance under default options. The following option default\nvalues have been modified in version 0.9:</p>\n<ol>\n<li>File Format &#39;file.format&#39;: The default has changed from ORC to Parquet. There is no essential difference between these\nformats, but Parquet generally performs better, and the community has completed all capabilities for Parquet, including\nsupport for nested types, Filter PushDown, and more.</li>\n<li>File Size &#39;target-file-size&#39;: The size for primary key tables remains at 128MB, while the default for non-primary key\ntables (Append tables) has been adjusted to 256MB.</li>\n<li>Compression Default &#39;file.compression&#39;: The default has changed from LZ4 to ZSTD, with the default ZSTD compression\nlevel set to 1. You can adjust the compression level using &#39;file.compression.zstd-level&#39;, consuming more CPU for\ngreater compression rates.</li>\n<li>Local Spill Compression Level &#39;spill-compression.zstd-level&#39;: Likewise, local spill can also achieve greater\ncompression rates by adjusting the level.</li>\n</ol>\n<h3 id=\"cdc-ingestion\">CDC Ingestion</h3><p>The dependency for Flink CDC has been upgraded to version 3.1. Since Flink CDC has become a sub-project of Flink in this\nversion, the package names have been modified, rendering older versions of CDC unsupported. MySQL CDC, MongoDB CDC, and\nPostgres CDC will be affected.</p>\n<h2 id=\"paimon-branch\">Paimon Branch</h2><p>Branching is an interesting feature that allows us to manipulate Paimon tables in a manner similar to Git. It has\nreached a production-ready state in Paimon 0.9, and Alibaba&#39;s internal teams are already using it in production\nenvironments for tasks such as data correction and stream-batch integration.</p>\n<p>For example, you can use branches for data correction:</p>\n<pre class=\"code\"><code><span class=\"token comment\">-- create branch named 'branch1' from tag 'tag1'</span>\n<span class=\"token keyword\">CALL</span> sys<span class=\"token punctuation\">.</span>create_branch<span class=\"token punctuation\">(</span><span class=\"token string\">'default.T'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'branch1'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'tag1'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n<span class=\"token comment\">-- write to branch 'branch1'</span>\n<span class=\"token keyword\">INSERT</span> <span class=\"token keyword\">INTO</span> <span class=\"token identifier\"><span class=\"token punctuation\">`</span>t$branch_branch1<span class=\"token punctuation\">`</span></span> <span class=\"token keyword\">SELECT</span> <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>\n\n<span class=\"token comment\">-- read from branch 'branch1'</span>\n<span class=\"token keyword\">SELECT</span> <span class=\"token operator\">*</span> <span class=\"token keyword\">FROM</span> <span class=\"token identifier\"><span class=\"token punctuation\">`</span>t$branch_branch1<span class=\"token punctuation\">`</span></span><span class=\"token punctuation\">;</span>\n\n<span class=\"token comment\">-- replace master branch with 'branch1'</span>\n<span class=\"token keyword\">CALL</span> sys<span class=\"token punctuation\">.</span>fast_forward<span class=\"token punctuation\">(</span><span class=\"token string\">'default.T'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'branch1'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre><p>You can also use branches for unified stream-batch storage. You can set up a separate stream branch and then configure\n&#39;scan.fallback-branch&#39;. This way, when a batch processing job reads from the current branch and a partition is missing,\nit will attempt to read that partition from the fallback branch.</p>\n<p>Suppose you create a Paimon table partitioned by date. You have a long-running streaming job that inserts records into\nPaimon so that today&#39;s data can be queried in a timely manner. You also have a nightly batch processing job that overwrites\nthe partitions in Paimon to ensure data accuracy. When you query this Paimon table, you want to read first from the results\nof the batch processing job. However, if a specific partition (for example, today&#39;s partition) is missing in its results,\nyou want to read from the results of the streaming job. In this case, you can create a branch for the streaming job and \nset &#39;scan.fallback-branch&#39; to that streaming branch.</p>\n<pre class=\"code\"><code><span class=\"token comment\">-- create a branch for streaming job (realtime)</span>\n<span class=\"token keyword\">CALL</span> sys<span class=\"token punctuation\">.</span>create_branch<span class=\"token punctuation\">(</span><span class=\"token string\">'default.T'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'rt'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n<span class=\"token comment\">-- set primary key and bucket number for the branch</span>\n<span class=\"token keyword\">ALTER</span> <span class=\"token keyword\">TABLE</span> <span class=\"token identifier\"><span class=\"token punctuation\">`</span>T$branch_rt<span class=\"token punctuation\">`</span></span> <span class=\"token keyword\">SET</span> <span class=\"token punctuation\">(</span>\n    <span class=\"token string\">'primary-key'</span> <span class=\"token operator\">=</span> <span class=\"token string\">'dt,name'</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">'bucket'</span> <span class=\"token operator\">=</span> <span class=\"token string\">'2'</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">'changelog-producer'</span> <span class=\"token operator\">=</span> <span class=\"token string\">'lookup'</span>\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n<span class=\"token comment\">-- set fallback branch</span>\n<span class=\"token keyword\">ALTER</span> <span class=\"token keyword\">TABLE</span> T <span class=\"token keyword\">SET</span> <span class=\"token punctuation\">(</span>\n    <span class=\"token string\">'scan.fallback-branch'</span> <span class=\"token operator\">=</span> <span class=\"token string\">'rt'</span>\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n<span class=\"token keyword\">SELECT</span> <span class=\"token operator\">*</span> <span class=\"token keyword\">FROM</span> T<span class=\"token punctuation\">;</span></code></pre><h2 id=\"universal-format\">Universal Format</h2><p>Paimon&#39;s Universal Format allows you to use Iceberg clients or compute engines to read data within Paimon. By using the\n&#39;metadata.iceberg-compatible&#39; option, Paimon automatically generates Iceberg snapshots in the filesystem when creating\nsnapshots, without requiring any additional dependencies or concerns about governance-related issues.</p>\n<p>Notable points include:</p>\n<ol>\n<li>Iceberg metadata is stored in the file system (corresponding to Iceberg&#39;s HadoopCatalog). For example, you can read\nit using Spark DataFrame: spark.read.format(&quot;iceberg&quot;).load(&quot;path&quot;).</li>\n<li>Iceberg views are read-only, and writing through this method may corrupt the table.</li>\n<li>For primary key tables, Iceberg views can only access the highest level (LSM Level) files. You can configure\n&#39;compaction.optimization-interval&#39; to control the visibility of the data.</li>\n</ol>\n<h2 id=\"caching-catalog\">Caching Catalog</h2><p>Paimon&#39;s metadata is stored in the filesystem, which leads to frequent access to the filesystem during the planning\nphase in compute engines, potentially impacting single-point performance, especially in object storage where this cost\nis even higher.</p>\n<p>This version introduces the implementation of Caching Catalog by default, which will be enabled automatically\n(it only caches manifest files smaller than 1MB by default). This can accelerate the performance of OLAP queries.</p>\n<p>You can control the behavior of the cache using the following options:</p>\n<table>\n<thead>\n<tr>\n<th>option</th>\n<th>default</th>\n<th>description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>cache-enabled</td>\n<td>true</td>\n<td>Controls whether the catalog will cache databases, tables and manifests.</td>\n</tr>\n<tr>\n<td>cache.expiration-interval</td>\n<td>1 min</td>\n<td>Controls the duration for which databases and tables in the catalog are cached.</td>\n</tr>\n<tr>\n<td>cache.manifest.max-memory</td>\n<td>(none)</td>\n<td>Controls the maximum memory to cache manifest content.</td>\n</tr>\n<tr>\n<td>cache.manifest.small-file-memory</td>\n<td>128 mb</td>\n<td>Controls the cache memory to cache small manifest files.</td>\n</tr>\n<tr>\n<td>cache.manifest.small-file-threshold</td>\n<td>1 mb</td>\n<td>Controls the threshold of small manifest file.</td>\n</tr>\n</tbody></table>\n<h2 id=\"deletion-vectors\">Deletion Vectors</h2><p>The Deletion Vectors mode is fully available in version 0.9.</p>\n<p>For primary key tables, the Deletion Vectors mode now supports asynchronous compaction (defaulting to semi-synchronous),\nsignificantly enhancing its usability without heavily impacting checkpoints. Since the DV mode requires local disk usage,\nwe recommend using SSDs for local disks; performance can be quite poor with lower-quality HDDs.</p>\n<p>For non-primary key tables (Append tables), version 0.9 supports DELETE, UPDATE, and MERGE INTO operations via Spark SQL,\nmaking Paimon append tables resemble complete database tables, enabling fine-grained modifications and deletions for users.</p>\n<p>Moreover, non-primary key tables also support the Deletion Vectors mode. Before enabling it, deletions and modifications\nare done using copy-on-write; once the DV mode is enabled, deletions and modifications switch to merge-on-write.\nDeletion files will be removed during compaction.</p>\n<h2 id=\"core\">Core</h2><h3 id=\"new-aggregation-functions\">New Aggregation Functions</h3><p>The new aggregation functions include hll_sketch, theta_sketch, rbm32, and rbm64. You can use these sketch-related\nfunctions to estimate COUNT DISTINCT.</p>\n<p>Paimon does not support custom aggregation functions but encourages you to propose enhancements to the built-in function library in the community.</p>\n<h3 id=\"universal-file-index\">Universal File Index</h3><p>The universal file index now supports the Bitmap type and also supports the REWRITE CALL command, allowing you to regenerate the corresponding index.</p>\n<p>Bitmap indexes perform well under joint filter conditions across multiple fields.</p>\n<h3 id=\"historical-partition-compact\">Historical Partition Compact</h3><p>Additionally, if your table is a partitioned table, although Paimon has built-in automatic compaction, its historical\npartitions may not have undergone full compaction. In version 0.9, we introduced partition_idle_time to automatically\nselect partitions that haven&#39;t been updated for full compaction, reducing small files and improving query performance.</p>\n<h2 id=\"flink\">Flink</h2><h3 id=\"cluster\">Cluster</h3><p>Clustering allows you to organize data in an Append table based on the values of certain columns during the write process.\nThis data organization method can significantly improve the efficiency of downstream tasks when reading data, as it enables\nfaster and more targeted data queries. This feature only supports Append tables (with bucket = -1) and batch execution mode.</p>\n<pre class=\"code\"><code><span class=\"token keyword\">INSERT</span> <span class=\"token keyword\">INTO</span> my_table <span class=\"token comment\">/*+ OPTIONS('sink.clustering.by-columns' = 'a,b') */</span> <span class=\"token keyword\">SELECT</span> <span class=\"token operator\">*</span> <span class=\"token keyword\">FROM</span> source<span class=\"token punctuation\">;</span></code></pre><h3 id=\"partition-mark-done\">Partition Mark Done</h3><p>For partitioned tables, each partition may need to be scheduled to trigger downstream batch computations. Therefore,\nit is essential to choose the right timing to indicate that a partition is ready for scheduling, while minimizing\ndata drift during the scheduling process. We refer to this process as &quot;Marking a Partition as Done.&quot;</p>\n<pre class=\"code\"><code><span class=\"token keyword\">CREATE</span> <span class=\"token keyword\">TABLE</span> my_partitioned_table <span class=\"token punctuation\">(</span>\n    f0 <span class=\"token keyword\">INT</span><span class=\"token punctuation\">,</span>\n    f1 <span class=\"token keyword\">INT</span><span class=\"token punctuation\">,</span>\n    f2 <span class=\"token keyword\">INT</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>\n    dt STRING\n<span class=\"token punctuation\">)</span> PARTITIONED <span class=\"token keyword\">BY</span> <span class=\"token punctuation\">(</span>dt<span class=\"token punctuation\">)</span> <span class=\"token keyword\">WITH</span> <span class=\"token punctuation\">(</span>\n    <span class=\"token string\">'partition.timestamp-formatter'</span><span class=\"token operator\">=</span><span class=\"token string\">'yyyyMMdd'</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">'partition.timestamp-pattern'</span><span class=\"token operator\">=</span><span class=\"token string\">'$dt'</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">'partition.time-interval'</span><span class=\"token operator\">=</span><span class=\"token string\">'1 d'</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">'partition.idle-time-to-done'</span><span class=\"token operator\">=</span><span class=\"token string\">'15 m'</span>\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre><ol>\n<li>First, it is necessary to define the time parsing for the partition and the time interval between partitions to\ndetermine when it is appropriate to mark a partition as complete.</li>\n<li>Second, idle time needs to be defined, which determines how long a partition must wait without new data before it\ncan be marked as complete.</li>\n<li>Third, by default, marking a partition as complete will create a _SUCCESS file. You can also configure\npartition.mark-done-action to define specific actions.</li>\n</ol>\n<h3 id=\"table-clone\">Table Clone</h3><p>In Paimon 0.9, the clone table action is supported for data migration. Currently, it only clones the table files used\nby the latest snapshot. If the table you are cloning hasnâ€™t been modified during this period, it is recommended to\nsubmit a Flink batch job for better performance. However, if you wish to clone the table while writing, you should\nsubmit a Flink stream processing job for automatic fault recovery. This command can assist you with convenient data\nbackup and migration.</p>\n<h3 id=\"procedures\">Procedures</h3><p>Paimon 0.9 introduces a large number of procedures. Additionally, Flink procedures now support the latest version of\nnamed procedures, making your execution more convenient without the need to specify all parameters forcefully.</p>\n<h2 id=\"spark\">Spark</h2><h3 id=\"dynamic-options\">Dynamic Options</h3><p>Historically, Spark SQL lacked the capability for dynamic parameters, while Flink SQL offers dynamic options that are\nvery convenient. In version 0.9, Spark introduces this capability through the SET command. The SET command allows for\nspecific Paimon configurations by requiring the prefix spark.paimon. to be added.</p>\n<pre class=\"code\"><code><span class=\"token comment\">-- set paimon conf</span>\n<span class=\"token keyword\">SET</span> spark<span class=\"token punctuation\">.</span>paimon<span class=\"token punctuation\">.</span><span class=\"token keyword\">file</span><span class=\"token punctuation\">.</span>block<span class=\"token operator\">-</span>size<span class=\"token operator\">=</span><span class=\"token number\">512</span>M<span class=\"token punctuation\">;</span>\n\n<span class=\"token comment\">-- reset conf</span>\nRESET spark<span class=\"token punctuation\">.</span>paimon<span class=\"token punctuation\">.</span><span class=\"token keyword\">file</span><span class=\"token punctuation\">.</span>block<span class=\"token operator\">-</span>size<span class=\"token punctuation\">;</span></code></pre><h3 id=\"bucketed-join\">Bucketed Join</h3><p>Hive has a feature called Bucketed Join. When two tables are bucketed by the same column, a Bucketed Join can be\nperformed without the need for shuffling the data; the join can be executed directly on the buckets, making it very efficient.</p>\n<p>Buckets are one of the core concepts in Paimon. Previously, Paimon lacked integration with compute engines, preventing\nthe utilization of this feature for optimization.</p>\n<p>In version 0.9, Paimon achieves deep integration with Spark SQL, enabling this optimization:</p>\n<ul>\n<li>Optimized Execution: By leveraging the bucket structure during join operations, Paimon can significantly reduce \nthe overhead of data shuffling, leading to better performance on join queries.</li>\n<li>Seamless Configuration: Users can easily define buckets when creating tables, allowing for straightforward\nimplementation of bucketed joins in their Spark SQL queries.</li>\n</ul>\n<pre class=\"code\"><code><span class=\"token comment\">-- Enable bucketing optimization</span>\n<span class=\"token keyword\">SET</span> spark<span class=\"token punctuation\">.</span><span class=\"token keyword\">sql</span><span class=\"token punctuation\">.</span>sources<span class=\"token punctuation\">.</span>v2<span class=\"token punctuation\">.</span>bucketing<span class=\"token punctuation\">.</span>enabled<span class=\"token operator\">=</span><span class=\"token boolean\">true</span><span class=\"token punctuation\">;</span>\n\n<span class=\"token comment\">-- Bucketed Join</span>\n<span class=\"token keyword\">SELECT</span> <span class=\"token operator\">*</span> <span class=\"token keyword\">FROM</span> t1 <span class=\"token keyword\">JOIN</span> t2 <span class=\"token keyword\">on</span> t1<span class=\"token punctuation\">.</span>id <span class=\"token operator\">=</span> t2<span class=\"token punctuation\">.</span>id</code></pre><p>As long as both tables involved in the join are bucketed tables (whether they are primary key tables or non-primary key\ntables) and the bucket-key field corresponds to the join field, an efficient Bucketed Join will take place. This\noptimization minimizes data shuffling and enhances query performance, making it a valuable feature for handling large\ndatasets in Paimon.</p>\n<h3 id=\"writing-to-dynamic-bucket-tables\">Writing to Dynamic Bucket Tables</h3><ul>\n<li>Optimized Writing: Spark SQL can now write to dynamic bucket tables with optimizations that reduce data shuffling\nduring the first write operation.</li>\n<li>Cross-Partition Updates: Spark SQL now supports writing to tables with cross-partition updates, although the overall\nefficiency for these operations is still not optimal.</li>\n</ul>\n<h2 id=\"other-progress\">Other Progress</h2><ul>\n<li>Paimon Web UI: The Web UI is being released; feel free to try it out!</li>\n<li>Paimon Python: The release process for version 0.1 is expected to start soon.</li>\n<li>Paimon Rust: In development, with an expected release of a readable version in 0.1.</li>\n</ul>\n","toc":[{"depth":2,"text":"Version Overview","id":"version-overview"},{"depth":2,"text":"Compatibility Changes","id":"compatibility-changes"},{"depth":3,"text":"Bucketed Append Tables","id":"bucketed-append-tables"},{"depth":3,"text":"File Format and Compression","id":"file-format-and-compression"},{"depth":3,"text":"CDC Ingestion","id":"cdc-ingestion"},{"depth":2,"text":"Paimon Branch","id":"paimon-branch"},{"depth":2,"text":"Universal Format","id":"universal-format"},{"depth":2,"text":"Caching Catalog","id":"caching-catalog"},{"depth":2,"text":"Deletion Vectors","id":"deletion-vectors"},{"depth":2,"text":"Core","id":"core"},{"depth":3,"text":"New Aggregation Functions","id":"new-aggregation-functions"},{"depth":3,"text":"Universal File Index","id":"universal-file-index"},{"depth":3,"text":"Historical Partition Compact","id":"historical-partition-compact"},{"depth":2,"text":"Flink","id":"flink"},{"depth":3,"text":"Cluster","id":"cluster"},{"depth":3,"text":"Partition Mark Done","id":"partition-mark-done"},{"depth":3,"text":"Table Clone","id":"table-clone"},{"depth":3,"text":"Procedures","id":"procedures"},{"depth":2,"text":"Spark","id":"spark"},{"depth":3,"text":"Dynamic Options","id":"dynamic-options"},{"depth":3,"text":"Bucketed Join","id":"bucketed-join"},{"depth":3,"text":"Writing to Dynamic Bucket Tables","id":"writing-to-dynamic-bucket-tables"},{"depth":2,"text":"Other Progress","id":"other-progress"}],"alias":"release-0.9","version":"0.9.0","weight":90}