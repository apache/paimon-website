{"title":"Release 0.7","type":"release","content":"<p>February 29, 2024 - Paimon Community (<a href=\"mailto:dev@paimon.apache.org\" target=\"_blank\" title=\"undefined\" rel=\"noopener\">dev@paimon.apache.org</a>)</p>\n<p>Apache Paimon PPMC has officially released Apache Paimon 0.7.0-incubating version. A total of 34 people contributed to\nthis version and completed over 300 Commits. Thank you to all contributors for their support!</p>\n<p>In this version, we mainly focus on enhancement and optimization for the existing features. For more details, please \nrefer to the following content below.</p>\n<h2 id=\"flink\">Flink</h2><h3 id=\"lookup-join\">Lookup Join</h3><ol>\n<li>Fix bug that lookup join cannot handle sequence field of dim table.</li>\n<li>Introduced primary key partial lookup based on Paimon hash lookup for lookup join. </li>\n<li>Use parallel reading and bulk loading to speed up initial data loading of dim table.</li>\n</ol>\n<h3 id=\"paimon-cdc\">Paimon CDC</h3><p>In 0.7, we continued to improve the Paimon CDC:</p>\n<ol>\n<li>Support Postgres table synchronization.</li>\n<li>Paimon data commit relies on checkpoint but many users forget to enable it when submitting CDC job. In this case, the job \nwill set checkpoint interval to 180 seconds.</li>\n<li>Support UNAWARE Bucket mode table as CDC sink.</li>\n<li>Support extract time attribute from CDC input as watermark. Now, you can set <code>tag.automatic-creation</code> to <code>watermark</code> in CDC jobs.</li>\n</ol>\n<h2 id=\"spark\">Spark</h2><ol>\n<li>Merge-into supports WHEN NOT MATCHED BY SOURCE semantics.</li>\n<li>Sort compact supports Hilbert Curve sorter.</li>\n<li>Multiple optimization for improving query performance.</li>\n</ol>\n<h2 id=\"hive\">Hive</h2><p>In 0.7, we mainly focus on improving compatibility with Hive.</p>\n<ol>\n<li>Support timestamp with local time zone type.</li>\n<li>Support create database with location, comment and properties of HiveSQL.</li>\n<li>Support table comment.</li>\n</ol>\n<h2 id=\"tag-management\">Tag Management</h2><ol>\n<li>Support a new <code>tag.automatic-creation</code> mode <code>batch</code>. In this mode, a tag will be created after a batch job completed.</li>\n<li>The tag auto creation relies on commit, so if no commit when it is time to auto-create tag,  the tag won&#39;t be created. \nIn this case, we introduce an option <code>snapshot.watermark-idle-timeout</code>. If the flink source idles over the specified \ntimeout duration, the job will force to create a snapshot and thus trigger tag creation.</li>\n</ol>\n<h2 id=\"new-aggregation-functions\">New Aggregation Functions</h2><ol>\n<li>count: counts the values across multiple rows.</li>\n<li>product: computes product values across multiple rows.</li>\n<li>nested-update: collects multiple rows into one ARRAY<ROW> (so-called &#39;nested table&#39;). You can use <code>fields.&lt;field-name&gt;.nested-key=pk0,pk1,...</code> to \ndefine the primary keys of the nested table. If no keys defined, the rows will be appended to the array.</li>\n<li>collect: collects elements into an ARRAY. You can set <code>fields.&lt;field-name&gt;.distinct=true</code> to deduplicate elements.</li>\n<li>merge_map: merges input maps into single map.</li>\n</ol>\n<h2 id=\"new-metrics\">New Metrics</h2><ol>\n<li>Support Flink standard connector metric <code>currentEmitEventTimeLag</code>.</li>\n<li>Support <code>level0FileCount</code> to show the compaction progress.</li>\n</ol>\n<h2 id=\"other-improvements\">Other Improvements</h2><p>Besides above, there are some useful improvements for existed features:</p>\n<ol>\n<li>New time travel option <code>scan.file-creation-time-millis</code>: By specifying this option, only the data files created after \nthis time will be read. It is more convenient than <code>scan.timestamp-millis</code> and <code>scan.tag-name</code>, but is imprecise (depending \non whether compaction occurs).</li>\n<li>For primary key table, now the row kind can be determined by field which is specified by option <code>rowkind.field</code>.</li>\n<li>Support ignoring delete records in deduplicate mode by option <code>deduplicate.ignore-delete</code>.</li>\n<li>Support ignoring consumer id when starting streaming reading job by option <code>consumer.ignore-progress</code>.</li>\n<li>Support new procedure <code>expire_snapshots</code> to manually trigger snapshot expiration.</li>\n<li>Support new system table <code>aggregation_fields</code> to show the aggregation fields information for aggregate or partial-update table.</li>\n<li>Introduce bloom filter to speed up the local file lookup, which can benefit both lookup changelog-producer and flink lookup join.</li>\n</ol>\n","toc":[{"depth":2,"text":"Flink","id":"flink"},{"depth":3,"text":"Lookup Join","id":"lookup-join"},{"depth":3,"text":"Paimon CDC","id":"paimon-cdc"},{"depth":2,"text":"Spark","id":"spark"},{"depth":2,"text":"Hive","id":"hive"},{"depth":2,"text":"Tag Management","id":"tag-management"},{"depth":2,"text":"New Aggregation Functions","id":"new-aggregation-functions"},{"depth":2,"text":"New Metrics","id":"new-metrics"},{"depth":2,"text":"Other Improvements","id":"other-improvements"}],"alias":"release-0.7","version":"0.7.0","weight":70}