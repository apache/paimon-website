{"title":"Release 1.3","type":"release","content":"<p>NOV 27, 2025 - Jingsong Lee (<a href=\"mailto:jingsonglee0@gmail.com\" target=\"_blank\" title=\"undefined\" rel=\"noopener\">jingsonglee0@gmail.com</a>)</p>\n<p>Apache Paimon PMC has officially released version 1.3. This core version has undergone over 3 months of careful polishing, \nwith a total of more than 500 code commits completed. We would like to express our sincere gratitude to all the developers\nwho have participated in the contribution!</p>\n<h2 id=\"version-overview\">Version Overview</h2><p>Notable changes in this version are:</p>\n<ol>\n<li>PyPaimon: Refactored Python SDK, a pure Python implementation version without JVM, with performance surpassing Java SDK in some scenarios.</li>\n<li>Row Tracking: adds a global Row ID to the table; Data Evolution provides tables with the ability to quickly update column data, optimizing for large and wide tables.</li>\n<li>Incremental Clustering: Sort and cluster data in an incremental manner, optimizing data layout at a relatively low cost and providing fast query validation for the Append table.</li>\n<li>Rest Catalog: Virtual File System， Allow access to the file system managed by Rest Catalog in the form of database and table names, with unified directory names and unified permission management.</li>\n<li>Performance optimization: Support Spark TopN push down; Limit push down; Introduce a new high-performance Range bitmap.</li>\n<li>Performance optimization: Manifest Cache organizes its cache according to partitions and buckets, making it more efficient for OLAP engines to scan Manifests during queries.</li>\n<li>Commit Conflict: Resolve the potential risk of file storage errors caused by MERGE-INTO and COMPACT simultaneously.</li>\n</ol>\n<h2 id=\"multimodal-data-lake\">Multimodal data lake</h2><p>The direction of multimodal data lake, Apache Paimon focuses on the following directions:</p>\n<ol>\n<li>Support multimodal data storage such as text, images, audio and video, and also support unified storage of structured tags and vector data. The Paimon community is developing capabilities for Blob storage and vector storage.</li>\n<li>Provide efficient retrieval of multimodal data, including random retrieval and global indexing. The Paimon community is developing global indexing capabilities, providing bitmap, B-tree, vector indexing, and other capabilities.</li>\n<li>Deep AI integration and application collaboration, docking with AI related distributed engines and applications, Paimon requires a high-performance Python SDK to integrate with the JVM free AI Python ecosystem.</li>\n<li>The multimodal direction requires support for rapid column addition of tables to provide fast updates of tags corresponding to multimodal data, enabling engineers to quickly tagging and greatly improving the efficiency of AI processing.</li>\n</ol>\n<p>Paimon 1.3 has made significant progress in both @3 and @4, and is being designed and developed for @1 and @2. We hope to release it in the next version.</p>\n<h3 id=\"pypaimon\">PyPaimon</h3><p>We need a powerful PyPaimon SDK for the AI oriented Python ecosystem. PyPaimon had a version (0.2) last year that encapsulated Java code based on Py4j. Although it can meet all table schemas, it has the following serious issues:</p>\n<ol>\n<li>The performance is too poor. If Py4j has data transmission, the performance will regress significantly.</li>\n<li>JVM dependencies require the client&#39;s machine to install JVM related dependencies.</li>\n</ol>\n<p>To this end, Paimon 1.3 completely reshaped the PyPaimon code and integrated it into the Paimon main repository, completely re implementing Paimon&#39;s Python SDK from the Python ecosystem. We compared its related performance:</p>\n<img src=\"./img/1.3-pypaimon.png\" alt=\"pypaimon\" />\n\n<p>As can be seen, compared to the old version of Python SDK, the performance is significantly ahead; Compared to Java implementation, it is also faster in some scenarios, thanks to the performance optimization of Arrow&#39;s native read and write in the Python ecosystem.</p>\n<p>Note that currently PyPaimon can basically meet the requirements in the Append table, but only supports simple Deduplication capabilities for the primary key table, and does not currently support rich patterns. In future versions, the community will:</p>\n<ol>\n<li>Continue to improve PyPaimon and cover more modes.</li>\n<li>In the future, PyPaimon will be used to integrate with more ecosystems, such as Ray and Daft engines.</li>\n</ol>\n<h3 id=\"row-tracking\">Row Tracking</h3><p>Row Tracking allows Paimon to track row level changes in the Append table. Once enabled on the Paimon table, two additional hidden columns will be added to the table structure:</p>\n<ol>\n<li>_ROW_ID: BIGINT， This is the unique identifier for each row in the table. It is used to track updates of rows and can be used to identify rows when updating, merging, or deleting.</li>\n<li>_SEQUENCE_NUMBER: BIGINT， This is a field indicating the version of this record. It is actually the snapshot ID of the snapshot to which this row belongs. It is used to track updates to the row version.</li>\n</ol>\n<p>The biggest benefit of Row Tracking is the design of global IDs for tables, which lays the foundation for our subsequent Data Evolution and global indexing mechanisms.</p>\n<p>Although Paimon supports full Schema Evolution, allowing you to freely add, modify, or delete column schemas. But how to update column data, you can use the MERGE Into statement, but it will overwrite all the affected row data during execution, which has high storage and computational costs.</p>\n<p>Data Evolution is a new feature of the Append table that completely changes the way data evolution is handled, especially when adding new columns. This mode allows you to update some columns without rewriting the entire data file. On the contrary, it writes new column data into separate files and intelligently merges them with the original data during read operations.</p>\n<p>For example, SQL:</p>\n<pre class=\"code\"><code><span class=\"token keyword\">CREATE</span> <span class=\"token keyword\">TABLE</span> target_table <span class=\"token punctuation\">(</span>id <span class=\"token keyword\">INT</span><span class=\"token punctuation\">,</span> b <span class=\"token keyword\">INT</span><span class=\"token punctuation\">,</span> c <span class=\"token keyword\">INT</span><span class=\"token punctuation\">)</span> \nTBLPROPERTIES <span class=\"token punctuation\">(</span>\n  <span class=\"token string\">'row-tracking.enabled'</span> <span class=\"token operator\">=</span> <span class=\"token string\">'true'</span><span class=\"token punctuation\">,</span> \n  <span class=\"token string\">'data-evolution.enabled'</span> <span class=\"token operator\">=</span> <span class=\"token string\">'true'</span> \n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n<span class=\"token keyword\">INSERT</span> <span class=\"token keyword\">INTO</span> target_table <span class=\"token keyword\">VALUES</span> <span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n<span class=\"token keyword\">CREATE</span> <span class=\"token keyword\">TABLE</span> source_table <span class=\"token punctuation\">(</span>id <span class=\"token keyword\">INT</span><span class=\"token punctuation\">,</span> b <span class=\"token keyword\">INT</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n<span class=\"token keyword\">INSERT</span> <span class=\"token keyword\">INTO</span> source_table <span class=\"token keyword\">VALUES</span> <span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">11</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">22</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> <span class=\"token number\">33</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">MERGE</span> <span class=\"token keyword\">INTO</span> target_table <span class=\"token keyword\">AS</span> t <span class=\"token keyword\">USING</span> source_table <span class=\"token keyword\">AS</span>\n s <span class=\"token keyword\">ON</span> t<span class=\"token punctuation\">.</span>id <span class=\"token operator\">=</span> s<span class=\"token punctuation\">.</span>id <span class=\"token keyword\">WHEN</span> <span class=\"token keyword\">MATCHED</span> <span class=\"token keyword\">THEN</span> <span class=\"token keyword\">UPDATE</span> <span class=\"token keyword\">SET</span> t<span class=\"token punctuation\">.</span>b <span class=\"token operator\">=</span> s<span class=\"token punctuation\">.</span>b\n  <span class=\"token keyword\">WHEN</span> <span class=\"token operator\">NOT</span> <span class=\"token keyword\">MATCHED</span> <span class=\"token keyword\">THEN</span> <span class=\"token keyword\">INSERT</span> <span class=\"token punctuation\">(</span>id<span class=\"token punctuation\">,</span> b<span class=\"token punctuation\">,</span> c<span class=\"token punctuation\">)</span> \n  <span class=\"token keyword\">VALUES</span> <span class=\"token punctuation\">(</span>id<span class=\"token punctuation\">,</span> b<span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n<span class=\"token keyword\">SELECT</span> <span class=\"token operator\">*</span> <span class=\"token keyword\">FROM</span> target_table<span class=\"token punctuation\">;</span>\n\n<span class=\"token operator\">+</span><span class=\"token comment\">----+----+----+ </span>\n<span class=\"token operator\">|</span> id <span class=\"token operator\">|</span> b  <span class=\"token operator\">|</span> c  <span class=\"token operator\">|</span>\n<span class=\"token operator\">+</span><span class=\"token comment\">----+----+----+ </span>\n<span class=\"token operator\">|</span> <span class=\"token number\">1</span>  <span class=\"token operator\">|</span> <span class=\"token number\">11</span> <span class=\"token operator\">|</span> <span class=\"token number\">1</span>  <span class=\"token operator\">|</span> \n<span class=\"token operator\">|</span> <span class=\"token number\">2</span>  <span class=\"token operator\">|</span> <span class=\"token number\">22</span> <span class=\"token operator\">|</span> <span class=\"token number\">2</span>  <span class=\"token operator\">|</span> \n<span class=\"token operator\">|</span> <span class=\"token number\">3</span>  <span class=\"token operator\">|</span> <span class=\"token number\">33</span> <span class=\"token operator\">|</span> <span class=\"token number\">0</span>  <span class=\"token operator\">|</span></code></pre><p>This statement only updates column b in the target table target_table based on the matching records of the source table source_table, while keeping columns id and c unchanged, and inserts a new record with the specified value. The difference between this and tables without Data Evolution enabled is that only column b data is written to the new file, which is very lightweight.</p>\n<p>In the performance comparison of typical data testing, after Data Evolution, the performance of the original MERGE Into is compared:</p>\n<ol>\n<li>MERGE INTO has been optimized from 27 minutes to 17 minutes, significantly reducing execution time. If there are fewer updated data, the comparison becomes even stronger.</li>\n<li>MERGE INTO storage space has been reduced from 170 GB to 1 GB, significantly reducing storage consumption and lowering costs.</li>\n</ol>\n<p>Subsequent community plans:</p>\n<ol>\n<li>Develop global indexes, including scalar indexes and vector indexes, to accelerate data queries.</li>\n<li>Introducing Blob storage allows Paimon tables to easily store and analyze blob data ranging from KB to GB.</li>\n</ol>\n<h2 id=\"incremental-clustering\">Incremental Clustering</h2><p>In version 1.3, a new and flexible data management method called Incremental Clustering is provided for the Append table. It is not only responsible for merging small files,\nbut also sorts and clusters data incrementally, optimizing data layout at a relatively low cost and bringing a fast query experience to the Append table. At the same time, \nusers can flexibly adjust clustering keys without rewriting the data, and the data will dynamically evolve with the execution of incremental clustering, gradually achieving \noptimal results and significantly reducing the decision-making complexity related to user data layout.</p>\n<p>To balance the effects of write amplification and sorting, Paimon utilized the hierarchical concept of LSM Tree to layer data files and the idea of Universal Compaction to select files that need to be clustered.</p>\n<img src=\"./img/1.3-incremental-1.png\" alt=\"incremental-1\" />\n\n<p>Through multi-level design, the data volume of each cluster is controlled. The higher the level of data clustering, the more stable it is, and the lower the probability of rewriting, in order to slow down write amplification while ensuring good sorting performance.</p>\n<p>Compared to tables without Cluster, under the dual clustering key filtering condition, Incremental Cluster query efficiency can be improved by over 150x;</p>\n<img src=\"./img/1.3-incremental-2.png\" alt=\"incremental-2\" />\n\n<p>After enabling Incremental Cluster for the Append table, scheduling Incremental Cluster periodically can not only solve the problem of small files, but also maintain excellent query efficiency for the Append table. At the same time, you can change the clustering key at any time after changing the query mode.</p>\n<h2 id=\"virtual-file-system\">Virtual File System</h2><p>REST Catalog provides built-in storage, including Paimon Table, Format Table, and Object Table (also known as Fileset or Volume), and some scenarios require direct access to the file system. And our REST Catalog generates UUID paths for tables, which makes it difficult to directly access the file system.</p>\n<p>Therefore, PVFS (Paimon Virtual File System) allows users to access it through“ pvfs://catalog/database/table/ The path directly accesses all files of Catalog, including all internal tables. Another advantage is that all users access this file system through the permission system of Paimon REST Catalog, without the need to maintain another file system permission system.</p>\n<pre class=\"code\"><code>val spark <span class=\"token operator\">=</span> <span class=\"token class-name\">SparkSession</span><span class=\"token punctuation\">.</span><span class=\"token function\">builder</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span><span class=\"token function\">appName</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"PVFS CSV Analysis\"</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span><span class=\"token function\">config</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"spark.hadoop.fs.pvfs.impl\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"org.apache.paimon.vfs.hadoop.PaimonVirtualFileSystem\"</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span><span class=\"token function\">config</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"spark.hadoop.fs.pvfs.uri\"</span><span class=\"token punctuation\">,</span> \n      <span class=\"token string\">\"http://localhost:10000\"</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span><span class=\"token function\">config</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"spark.hadoop.fs.pvfs.token.provider\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"bear\"</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span><span class=\"token function\">config</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"spark.hadoop.fs.pvfs.token\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"token\"</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span><span class=\"token function\">getOrCreate</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n  \nspark<span class=\"token punctuation\">.</span><span class=\"token function\">sql</span><span class=\"token punctuation\">(</span> s<span class=\"token triple-quoted-string string\">\"\"\" \n      |CREATE TEMPORARY VIEW csv_table\n      |USING csv \n      |OPTIONS ( \n      |  path 'pvfs://catalog_name/database_name/my_format_table_name/a.csv', \n      |  header 'true', \n      |  inferSchema 'true' \n      |) \"\"\"</span><span class=\"token punctuation\">.</span>stripMargin <span class=\"token punctuation\">)</span></code></pre><h2 id=\"other-optimizations\">Other Optimizations</h2><p>The Apache Paimon community continues to improve storage and read-write links, continuously optimizing performance and usability:</p>\n<ol>\n<li>In terms of performance, it supports more push down options, such as Spark TopN push down; Limit push down; Introducing a new high-performance Range bitmap; In addition, for the performance of OLAP queries, improve the Manifest Cache to organize its cache according to partitions and buckets.</li>\n<li>In terms of usage, address the potential risk of file storage errors caused by the simultaneous use of MERGE-INTO and COMPACT, especially the conflict issue in Deletion Vectors mode.</li>\n</ol>\n","toc":[{"depth":2,"text":"Version Overview","id":"version-overview"},{"depth":2,"text":"Multimodal data lake","id":"multimodal-data-lake"},{"depth":3,"text":"PyPaimon","id":"pypaimon"},{"depth":3,"text":"Row Tracking","id":"row-tracking"},{"depth":2,"text":"Incremental Clustering","id":"incremental-clustering"},{"depth":2,"text":"Virtual File System","id":"virtual-file-system"},{"depth":2,"text":"Other Optimizations","id":"other-optimizations"}],"alias":"release-1.3","version":"1.3.1","weight":95}