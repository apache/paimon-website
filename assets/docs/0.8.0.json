{"title":"Release 0.8","type":"release","content":"<p>May 9, 2024 - Jingsong Lee (<a href=\"mailto:jingsonglee0@gmail.com\" target=\"_blank\" title=\"undefined\" rel=\"noopener\">jingsonglee0@gmail.com</a>)</p>\n<p>Apache Paimon PMC has officially released Apache Paimon 0.8.0 version. A total of 47 people contributed to\nthis version and completed over 350 Commits. Thank you to all contributors for their support!</p>\n<p>This version is the first release by Paimon after graduating and becoming a top-level project on Apache.\nIt includes a large number of new features and is also the longest release by Paimon.</p>\n<h2 id=\"version-overview\">Version Overview</h2><p>Paimon&#39;s long-term plan is to become a unified lake storage format that meets the main requirements for minute level\nbig data: offline batch computing, real-time stream computing, and OLAP computing.</p>\n<p>The notable changes in this version include:</p>\n<ol>\n<li>Add Deletion Vectors for near real-time updates and fast queries</li>\n<li>Adjust the default value of Bucket to -1 to improve usability for new learners</li>\n<li>Add a universal file indexing mechanism to improve OLAP query performance</li>\n<li>Optimize memory and performance for read and write processes, reduce IO access times</li>\n<li>A separate management mechanism for Changelog files to extend their lifecycle</li>\n<li>Add a file system based privilege system to manage read and write permissions</li>\n</ol>\n<h2 id=\"deletion-vectors\">Deletion Vectors</h2><p>Paimon&#39;s Deletion Vectors mode allows your primary key table (with &#39;delete-vectors.enabled&#39; set to &#39;true&#39;) to achieve\nsignificant read performance improvement without sacrificing too much write update performance, achieving near real-time\nupdates and fast queries.</p>\n<img src=\"./img/deletion-vectors.png\" alt=\"deletion vectors\" />\n\n<p>This mode will do more work to generate deletion files at checkpoint, so it is recommended that your Flink stream write\njobs have a larger &#39;execution.checkpointing.timeout&#39; value to avoid checkpoint timeouts.</p>\n<p>With the latest 0.8.0 version of paimon-flink, paimon-spark, paimon-hive, and paimon-trino, you can enjoy the optimized\nquery performance of this feature, and the Starrocks integration will be included in 3.2.8 &amp; 3.3.2 versions, Apache Doris\nintegration will be included in 2.0.10 &amp; 2.1.4 versions.</p>\n<p>It is recommended to enable this feature for most primary key tables.</p>\n<h2 id=\"bucket-default-value\">Bucket Default Value</h2><pre class=\"code\"><code><span class=\"token keyword\">CREATE</span> <span class=\"token keyword\">TABLE</span> T <span class=\"token punctuation\">(</span>\n  k <span class=\"token keyword\">INT</span> <span class=\"token keyword\">PRIMARY</span> <span class=\"token keyword\">KEY</span> <span class=\"token operator\">NOT</span> ENFORCED<span class=\"token punctuation\">,</span>\n  v0 <span class=\"token keyword\">INT</span><span class=\"token punctuation\">,</span>\n  v1 <span class=\"token keyword\">INT</span>\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre><p>For the above SQL CREATE TABLE, Paimon used a default value of bucket of 1 in the past version, which caused some new\nlearners to test Paimon with single parallelism, which would be a big bottleneck, this version adjusts the bucket to -1:</p>\n<ul>\n<li>For primary key tables: bucket to -1 uses dynamic bucket mode, which will consume more writes resources than the fixed\nbucket mode, but it brings distributed processing with easy configuration.</li>\n<li>For Append tables: a bucket of -1 is a scalable mode, which has better and more convenient distributed processing.</li>\n</ul>\n<p>This change will greatly improve the experience, and Paimon can cover most scenarios without having to configure any\nparameters. For compatibility issues, old tables will continue to use bucket 1 by default, only newly created tables\nwill be affected.</p>\n<h2 id=\"universal-file-indexing\">Universal File Indexing</h2><p>Prior to this release, you could use ORC&#39;s own indexing mechanism to speed up queries, but it only supported a few\nindexes such as Bloom filter, and you could only generate good corresponding indexes when writing to a file.</p>\n<p>To solve these problems, this release proposes Paimon&#39;s Universal file index (configure &#39;file-index.bloom-filter.columns&#39;),\nwhich will maintain the index file separately:</p>\n<ul>\n<li>Supports not only indexing of fields, but also construction of indexes on Map Keys.</li>\n<li>Plans to support building indexes on existing files at any time, which will prevent you from rewriting data files when\nadding new indexes.</li>\n<li>Plan to add indexes for Bitmap, N-Gram BloomFilter, inverted, and so on in subsequent releases.</li>\n</ul>\n<p>The current universal file index is only the basic framework, and only supports Append tables, which need to be\nimproved in subsequent releases.</p>\n<h2 id=\"read-and-write-performance-optimization\">Read and Write Performance Optimization</h2><p>In this release, the performance of critical codes for reading and writing has been optimized:</p>\n<ol>\n<li>Write performance optimization:\na. Optimized serialization performance on writes, with a 10-20% performance improvement on overall writes.\nb. Significantly improved the performance of Append table for multi-partition writes (more than 5 partitions).\nc. Increased the default value of &#39;num-sorted-run.stop-trigger&#39;, which slows down backpressure.\nd. Optimized startup performance for dynamic bucket writes.</li>\n<li>Commit performance optimization:\na. Dramatically reduce the memory usage of Commit node.\nb. Remove useless checks in Commit, write-only commits will be much faster.\nc. Partition Expire performance has been greatly improved.</li>\n<li>Query performance optimization:\na. Significantly reduce the memory usage of Plan generation.\nb. Reduced access to the file system NameNode in the plan and read phases, which is also beneficial to the OLAP performance of the object store.\nc. codegen supports cache, which will effectively improve the performance of short queries.\nd. Hive queries dramatically reduce the frequency of file system NameNode accesses by serializing Table objects.\ne. Dramatically improve the query performance of the first_row merge-engine.</li>\n</ol>\n<h2 id=\"changelog-lifecycle\">Changelog Lifecycle</h2><p>In the previous version, for primary key table, the default Snapshot retention time of Table is 1 hour, which means\nthe Snapshots before 1 hour will be expired, this will seriously affect the security of streaming read, the job\nstreaming read this table can not hang for more than 1 hour, otherwise it will consume the snapshot that have already\nbeen expired, and it won&#39;t be able to be recovered.</p>\n<p>The solution can be to configure <code>consumer-id</code> in the streaming job, the job that writes the table will check all the\nconsumers of the table in the filesystem when deciding whether the snapshot has expired or not, if there are still users\nrelying on the snapshot, the snapshot will not be deleted at the time of expiration. However, consumers require some\nmanagement operations and different jobs need to be configured with different consumer ids, which requires some\nmanagement costs.</p>\n<p>In this release, a new solution is proposed, which allow the Paimon table to act like a real queue and save Changelogs\nfor a longer period of time. Actually, the reason we can&#39;t save too many snapshots is that the Snapshot contains the\nresult file of multiple versions of Compaction, which is very large and takes up more space, while we only need the\nChangelog file for streaming read, so we can separate the lifecycle of the Changelog:</p>\n<img src=\"./img/changelog-lifecycle.png\" alt=\"changelog lifecycle\" />\n\n<p>When the Snapshot expires, we create the corresponding changelog reference, delete the multiple versions of the Compaction\nfile, and keep only the Changelog file. This way you can set up a changelog lifecycle of 1 day:</p>\n<pre class=\"code\"><code><span class=\"token keyword\">CREATE</span> <span class=\"token keyword\">TABLE</span> T <span class=\"token punctuation\">(</span>\n  k <span class=\"token keyword\">INT</span> <span class=\"token keyword\">PRIMARY</span> <span class=\"token keyword\">KEY</span> <span class=\"token operator\">NOT</span> ENFORCED<span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>\n<span class=\"token punctuation\">)</span> <span class=\"token keyword\">WITH</span> <span class=\"token punctuation\">(</span>\n  <span class=\"token string\">'changelog-producer'</span><span class=\"token operator\">=</span><span class=\"token string\">'input'</span><span class=\"token punctuation\">,</span>\n  <span class=\"token string\">'changelog.time-retained'</span> <span class=\"token operator\">=</span> <span class=\"token string\">'1 d'</span>\n<span class=\"token punctuation\">)</span></code></pre><p>The current version only supports Changelog files, so you need to configure <code>changelog-producer</code> for table to work.</p>\n<h2 id=\"privilege-management-system\">Privilege management system</h2><p>In this release, Paimon provides a file-based privilege system. Permissions determine which users can perform which\noperations on which objects, so you can manage table access in a fine-grained way. Currently, Paimon uses the\nIdentity-Based Access Control (IBAC) permission model, where permissions are assigned directly to users.</p>\n<pre class=\"code\"><code><span class=\"token keyword\">CREATE</span> CATALOG <span class=\"token identifier\"><span class=\"token punctuation\">`</span>my-catalog<span class=\"token punctuation\">`</span></span> <span class=\"token keyword\">WITH</span> <span class=\"token punctuation\">(</span>\n    <span class=\"token string\">'type'</span> <span class=\"token operator\">=</span> <span class=\"token string\">'paimon'</span><span class=\"token punctuation\">,</span>\n     <span class=\"token comment\">-- ...</span>\n    <span class=\"token string\">'user'</span> <span class=\"token operator\">=</span> <span class=\"token string\">'root'</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">'password'</span> <span class=\"token operator\">=</span> <span class=\"token string\">'mypassword'</span>\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n<span class=\"token comment\">-- create a user authenticated by the specified password</span>\n<span class=\"token comment\">-- change 'user' and 'password' to the username and password you want</span>\n<span class=\"token keyword\">CALL</span> sys<span class=\"token punctuation\">.</span>create_privileged_user<span class=\"token punctuation\">(</span><span class=\"token string\">'user'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'password'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n<span class=\"token comment\">-- you can change 'user' to the username you want, and 'SELECT' to other privilege you want</span>\n<span class=\"token comment\">-- grant 'user' with privilege 'SELECT' on the whole catalog</span>\n<span class=\"token keyword\">CALL</span> sys<span class=\"token punctuation\">.</span>grant_privilege_to_user<span class=\"token punctuation\">(</span><span class=\"token string\">'user'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'SELECT'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token comment\">-- grant 'user' with privilege 'SELECT' on database my_db</span>\n<span class=\"token keyword\">CALL</span> sys<span class=\"token punctuation\">.</span>grant_privilege_to_user<span class=\"token punctuation\">(</span><span class=\"token string\">'user'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'SELECT'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'my_db'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token comment\">-- grant 'user' with privilege 'SELECT' on table my_db.my_tbl</span>\n<span class=\"token keyword\">CALL</span> sys<span class=\"token punctuation\">.</span>grant_privilege_to_user<span class=\"token punctuation\">(</span><span class=\"token string\">'user'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'SELECT'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'my_db'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'my_tbl'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre><p>This privilege system does not prevent access to older versions. Please upgrade all engines to the new Paimon version\nfor the privilege system to take effect.</p>\n<h2 id=\"the-rest-of-the-core-features\">The rest of the core features</h2><ol>\n<li>support TTL specification when creating Tag, which allows you to create Tag more freely for safe batch reading.</li>\n<li>new record level TTL configuration (<code>record-level.expire-time</code>), the data will be expired at the time of Compaction,\nwhich can effectively reduce the pressure of Compaction by eliminating the expired data.</li>\n<li>aggregation functions <code>collect</code>, <code>merge_map</code>, <code>last_value</code>, <code>nested_update</code> support retraction\n(<code>DELETE</code> / <code>UPDATE_BEFORE</code>) message input, the specific use of the test with your scenario.</li>\n<li>Sequence Field is redesigned, when two data&#39;s Sequence Fields are equal, the order of entering Paimon will be used\nto decide the order.</li>\n<li>A new Time Travel method is added, which can specify the batch reading from snapshot watermark.</li>\n<li>Documentation: Flink and Spark have separate catalogs, including pages for reading, writing, table management, etc.\nHope you like them.</li>\n<li>system tables: greatly improve the query performance and stability of <code>files</code> &amp; <code>snapshots</code> &amp; <code>partitions</code> system table.</li>\n<li>ORC: greatly improve the write performance of orc complex types (array, map); support zstd compression, which is a\nhighly recommended algorithm for high compression.</li>\n</ol>\n<h2 id=\"flink\">Flink</h2><h3 id=\"datastream-api\">DataStream API</h3><p>The old version does not provide DataStream API, it is recommended to use Table API and DataStream conversion to write\ncode, however, it is difficult to solve some problems, for example, users want to write to Paimon while writing to\nother DataStream Sinks, Table conversion is difficult to solve this problem. So this version proposes a complete\nDataStream API:</p>\n<ol>\n<li><p><code>FlinkSinkBuilder</code>: build DataStream Sink. 2.</p>\n</li>\n<li><p><code>FlinkSourceBuilder</code>: build the DataStream Source.</p>\n</li>\n<li><p>We still don&#39;t recommend you to use DataStream API directly, we recommend you to prioritize using SQL to solve your business problems.</p>\n</li>\n</ol>\n<h3 id=\"lookup-join\">Lookup Join</h3><ol>\n<li>Flink Lookup Join uses Hash Lookup to fetch data in this version, which avoids the overhead of RocksDB&#39;s data\ninsertion.</li>\n<li>This version also continues to improve the Hash Lookup, supports compression, and defaults to lz4, the\n<code>changelog-producer</code> for lookup will also benefit from this.</li>\n<li>And Flink Lookup Join introduces max_pt mode, which is an interesting mode, it will only join the latest partition data, which is more suitable for the dimension table where each partition is full data.</li>\n</ol>\n<p>At present, Flink Lookup Join is still worth improving, for example, it does not Shuffle the data of the main table\n(or we can say fact table) according to the primary key of the dimension table, and the current Cache utilization is\nvery unfriendly, which leads to a large amount of IO, which is a problem that needs to be solved in the next version\nof Flink.</p>\n<h3 id=\"other-flink-changes\">Other Flink Changes</h3><ol>\n<li>Batch read partitioned table performance has been greatly improved. Previously, due to a design problem, each batch\nwould scan all partitions, which has now been removed.</li>\n<li>Metrics system has been redesigned, removing metrics at partition and bucket level, which would cause OOM when Flink\nJobManager runs for a long time.</li>\n<li>Introduced <code>commit.force-create-snapshot</code> to force snapshot generation, which allows certain operations to strongly\nrely on snapshot generation.</li>\n<li>Enhance Compact Sort: introduce Hilbert Sort, this kind of Sort still has some effect when there are more than 5\nfields, while z-order only recommends to sort within 5 fields; Sort new Range policy, which can avoid the skewed\nsorting problem due to the inconsistency of row sizes.</li>\n<li>CDC Ingestion&#39;s time function supports the handling of epoch time.</li>\n<li>Optimized the scalability of Flink&#39;s <code>consumer-id</code> streaming to support multi-partition streaming.</li>\n<li>Flink 1.19, COMPACT Procedure support named argument, and we regret to decide, due to the maintenance of more than 5\nversions, no longer support Flink 1.14, recommended to use Flink 1.17 + version!</li>\n</ol>\n<h2 id=\"spark\">Spark</h2><p>Spark continues to optimize query performance, supporting the generation and use of statistics at the table level.</p>\n<p>Spark uses COW technology to support DELETE and UPDATE for Append tables, and Spark DELETE supports primary key tables\nfor all MergeEngines. Spark DELETE and UPDATE also support subquery conditions. Spark COMPACT Procedure supports the\nwhere method.</p>\n<p>Other improvements:</p>\n<ol>\n<li>Spark Generic Catalog supports function methods.</li>\n<li>Delete Tag Procedure supports the ability to delete multiple Tags.</li>\n<li>Unfortunately, due to the maintenance of more than 5 versions, Spark 2 is no longer supported, recommended to use\nSpark 3.3+ version.</li>\n</ol>\n<h2 id=\"ecology-and-related-projects\">Ecology and Related Projects</h2><ol>\n<li>Hive Migration: Supports migration of entire Hive Database to Paimon Tables.</li>\n<li>Introducing Jdbc Catalog, which allows you to get rid of the Hive Metastore dependency.</li>\n<li>Hive Writer supports Tez-mr engine, we only recommend Hive Writer for small data volume.</li>\n<li>Paimon-Trino latest version only supports Trino 420+ version, but the performance of query orc has been greatly\nimproved.</li>\n<li>Paimon-Webui project development has made great progress and will be released soon.</li>\n</ol>\n","toc":[{"depth":2,"text":"Version Overview","id":"version-overview"},{"depth":2,"text":"Deletion Vectors","id":"deletion-vectors"},{"depth":2,"text":"Bucket Default Value","id":"bucket-default-value"},{"depth":2,"text":"Universal File Indexing","id":"universal-file-indexing"},{"depth":2,"text":"Read and Write Performance Optimization","id":"read-and-write-performance-optimization"},{"depth":2,"text":"Changelog Lifecycle","id":"changelog-lifecycle"},{"depth":2,"text":"Privilege management system","id":"privilege-management-system"},{"depth":2,"text":"The rest of the core features","id":"the-rest-of-the-core-features"},{"depth":2,"text":"Flink","id":"flink"},{"depth":3,"text":"DataStream API","id":"datastream-api"},{"depth":3,"text":"Lookup Join","id":"lookup-join"},{"depth":3,"text":"Other Flink Changes","id":"other-flink-changes"},{"depth":2,"text":"Spark","id":"spark"},{"depth":2,"text":"Ecology and Related Projects","id":"ecology-and-related-projects"}],"alias":"release-0.8","version":"0.8.0","weight":80}